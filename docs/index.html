<!doctype html>
<html lang="ja" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="UTF-8" />
    <title>Web Music Documentation</title>
    <meta name="description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta name="keywords" content="Web Music, Web Audio, Audio Signal Processing, Music" />
    <meta name="author" content="Korilakkuma (Tomohiro IKEDA)" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, viewport-fit=cover" />
    <meta name="format-detection" content="telephone=no" />
    <meta name="theme-color" content="#fafafa" />
    <meta property="og:description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta property="og:image" content="https://korilakkuma.github.io/Web-Music-Documentation/images/icon.png" />
    <meta property="og:site_name" content="Web Music Documentation" />
    <meta property="og:title" content="Web Music Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <meta name="twitter:card" content="summary" />
    <link rel="canonical" href="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <link rel="icon" href="images/icon.png" type="image/png" />
    <link rel="apple-touch-icon" href="images/icon.png" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-okaidia.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="docs.css" type="text/css" media="all" />
  </head>
  <body>
    <main id="document-top">
      <h1>Web Music ドキュメント</h1>
      <section id="section-about-web-music">
        <h2>Web Music</h2>
        <p>
          <strong>Web Music</strong> とは, Web (ブラウザ) をプラットフォームにした音楽アプリケーション, そして, そのような Web
          アプリケーションを実装するために必要となる, クライアントサイド JavaScript API (ブラウザ API) の総称です. これは, 一般的な技術用語ではなく,
          技術マーケティング的な造語です.
        </p>
        <p>具体的には, 以下のような, クライアントサイド JavaScript API の総称です.</p>
        <ul>
          <li><a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API</a></li>
          <li><a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement (HTMLAudioElement)</a></li>
          <li><a href="https://www.w3.org/TR/webrtc/" target="_blank" rel="noopener noreferrer">WebRTC</a></li>
          <li><a href="https://www.w3.org/TR/webmidi/" target="_blank" rel="noopener noreferrer">Web MIDI API</a></li>
          <li><a href="https://www.w3.org/TR/webcodecs/" target="_blank" rel="noopener noreferrer">WebCodecs API</a></li>
          <li><a href="https://www.w3.org/TR/mediasession/" target="_blank" rel="noopener noreferrer">Media Session</a></li>
        </ul>
        <p>
          本サイト制作開始時点の 2023 年時点で Web Audio API と WebRTC に関しては, <b>W3C recommendation</b>, HTMLMediaElement に関しては,
          <b>HTML Living Standard</b> (2019 年 6 月以降, HTML や DOM に関わる仕様策定は <abbr title="World Wide Web Consortium">W3C</abbr> ではなく
          <abbr title="Web Hypertext Application Technology Working Group">WHATWG</abbr> が仕様策定の主体になることが決定されたので, HTMLMediaElement は HTML
          Living Standard です) となっており, モダンブラウザであれば利用することが可能です (ただし, クライアントサイド JavaScript の宿命ではありますが, OS
          やブラウザによって挙動が異なることは少なからずあるので, 移植性まで考慮すると, そのためのクロスブラウザ対応の問題は必要となります).
          これらのクライアントサイド JavaScript API は 2010 年代前半ごろは, <b>HTML5</b> というバズワード化したカテゴリに分類される API でした. 現在は, HTML5
          という仕様, あるいは, 用語が定着したからか, HTML5 というワードが使われることはほぼなくなりました. したがって, Web Music に関係する API も,
          膨大なクライアントサイド JavaScript API のうちのいくつかです (という認識が一般的と言えます).
        </p>
        <article id="section-web-browser-javascript">
          <h3>クライアントサイド JavaScript とは ?</h3>
          <p>
            クライアントサイド JavaScript とは, JavaScript の仕様の標準である ECMAScript (JavaScript の実行コンテキストに依存しない言語仕様. これに準拠している
            JavaScript のコードであれば, Web ブラウザでも, Node.js でも, ブラウザ拡張でも使うことができます) と, 実行コンテキスト
            (広義な意味でのプラットフォーム) である Web ブラウザで実行する場合にのみアクセス可能な API です (例えば, Web Music の API は Node.js
            で使うことはできません. また, Web ブラウザでも Web Workers が生成したスレッドでは, メインスレッド (UI スレッド)
            と実行コンテキストが異なるので使うことができません).
          </p>
        </article>
      </section>
      <section id="section-web-audio-api-overview">
        <h2>Web Audio API</h2>
        <p>
          Web Music のなかで, もっともコアな API が <strong>Web Audio API</strong> です. 言い換えると, Web
          をプラットフォームとした音楽アプリケーションを制作するほとんどの場合で必要になる API ということです. なぜなら,
          <code>HTMLAudioElement</code> はオーディオファイルを再生するための API で, 高度なオーディオ処理をすることはできず (<a
            href="https://egonelbre.com/project/jsfx/"
            target="_blank"
            rel="noopener noreferrer"
            >jsfx</a>
          のようにハッキーな実装をすることでエフェクトをかけるぐらいは可能ですが, 仕様のユースケースとして想定されている使い方ではありません),
          リアルタイム性やインタラクティブ性も考慮された API ではないからです (厳密には, 考慮された経緯もあって,
          <code>Audio</code> コンストラクタが定義されています). また, Web Music として, Web MIDI API や WebRTC を使う場合, 実際のオーディオ処理は Web Audio API
          が実行することになります.
        </p>
        <section id="section-web-music-history">
          <h3>Web Music の歴史</h3>
          <p>
            古くは, Internet Explorer が独自に
            <a href="https://www.tohoho-web.com/html/bgsound.htm" target="_blank" rel="noopener noreferrer"><code>bgsound</code></a> という HTML
            タグを実装しており, (Internet Explorer のみではありますが) ブラウザでオーディオをファイルを再生することが可能でした (現在の
            <code>HTMLAudioElement</code> に相当する HTML タグと言えます). そのあと, Java アプレットや ActionScript (Flash) によって, Web Audio API
            で実現できているような高度なオーディオ処理が可能となりました.
          </p>
          <p>
            しかし, これらは特定のベンダーに依存していたので, Flash や Silverlight などブラウザの拡張機能 (プラグイン) という位置づけでした. Web 2.0
            (もっと言えば, Ajax) を機にブラウザでも, ネイティブアプリケーションのような Web アプリケーションが実装されてくるようになると, これまで拡張機能
            (オーディオ処理だけでなく, グラフィックス, ストレージ, ローカルファイルへのアクセス, ソケットなど) に依存していたような機能をブラウザ標準で
            (クライアントサイド JavaScript API で) 実現できる流れが 2010 年ごろから活発になりました (このころ, HTML5 という位置づけで仕様策定され,
            モダンブラウザで実装されるようになりました).
          </p>
          <p>
            ドキュメントプラットフォームとしての Web に, アプリケーションプラットフォームが追加されていく転換期に, Web Audio API
            も仕様策定されて現在に至っています.
          </p>
          <ol>
            <li>
              <a href="https://www.w3.org/TR/2011/WD-webaudio-20111215/" target="_blank" rel="noopener noreferrer">草案 (Working Draft)</a> (2011 年 12 月 15
              日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio-1.0/" target="_blank" rel="noopener noreferrer">Web Audio API 1.0 勧告 (W3C recommendation)</a> (2021 年 6
              月 17 日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API 1.1 (最新の W3C Working Draft)</a> (2024 年 11
              月 5 日に公開)
            </li>
          </ol>
        </section>
        <article id="section-about-audio-data-api">
          <h3>Audio Data API</h3>
          <p>
            厳密な歴史を記載すると, Web Audio API よりわずかに先行して Firefox で
            <a href="https://wiki.mozilla.org/Audio_Data_API" target="_blank" rel="noopener noreferrer">Audio Data API</a> というブラウザオーディオ API
            が実装されていました. <code>HTMLAudioElement</code> の拡張という位置づけで, 出力するオーディオデータを直接演算する API がメインでした (Web Audio API
            の <code>ScriptProcessorNode</code> に相当する API). 間もなくして, Web Audio API に統一される方針となり, Firefox も Web Audio API
            のサポートを開始したので現在は削除されています.
          </p>
        </article>
      </section>
      <section id="section-about-document">
        <h2>このサイトに関して</h2>
        <p>
          このサイト (ドキュメント) の目的は, Web Music, その中核となる Web Audio API について解説しますが, W3C
          が公開している仕様のすべてを解説するわけではありません. また, JavaScript の言語仕様の解説は, サイトの目的ではないこともご了承ください (ただし, Web
          Audio API を使う上で, 必要となってくるクライアントサイド JavaScript API に関しては必要に応じて解説をします (例. <code>File API</code>,
          <code>Fetch API</code> など).
        </p>
        <p>このサイトは W3C が公開している仕様にとって代わるものではなく, Web Audio API の仕様の理解を補助するリファレンスサイトと位置づけてください.</p>
        <p>
          デスクトップブラウザでは少なくなりましたが, モバイルブラウザでは仕様とブラウザの実装に差異があり,
          仕様では定義されているのに動作しないということもあります. その場合には, 開発者ツールなどを活用して,
          実装されているプロパティやメソッドを確認してみてください.
        </p>
        <section id="section-about-sample-code">
          <h3>解説の JavaScript コードに関して</h3>
          <p>
            <b>ECMAScript 2015</b> 以降の仕様に準拠したコードで記載します. また, ビルドツールなどを必要としないように, TypeScript
            での記述やモジュール分割などもしません (端的には, コピペすればブラウザコンソールなどで実行できるようなサンプルコード, あるいは,
            コード片を記載します). 具体的には, 以下のような構文を使います.
          </p>
          <ul>
            <li><code>const</code>, <code>let</code> による変数宣言</li>
            <li>Template Strings</li>
            <li>アロー関数</li>
            <li>クラス</li>
            <li><code>Promise</code>, または, <code>async</code>/<code>await</code></li>
          </ul>
          <p>
            Web Audio API のコードも仕様で推奨されているコードを基本的に記載します (例えば, <code>AudioNode</code> インスタンスを生成する場合,
            コンストラクタ形式が推奨されているので, そちらを使います). ただし, 現時点であまりにも実装の乖離が大きい場合は, フォールバック的な解説として,
            実装として動作するコードを記載します.
          </p>
        </section>
        <section id="section-recommendation-browsers">
          <h3>推奨ブラウザ</h3>
          <p>
            閲覧自体は, モダンブラウザであれば特に問題ありませんが, 実際のサンプルコードを動作させることを考慮すると, デスクトップブラウザ, 特に, Web Audio API
            の仕様に準拠している <a href="https://www.google.com/chrome/" target="_blank" rel="noopener noreferrer">Google Chrome</a> もしくは
            <a href="https://www.mozilla.org/ja/firefox/" target="_blank" rel="noopener noreferrer">Mozilla Firefox</a> (いずれも最新バージョン) を推奨します
            (Google Chrome の場合, より高度な
            <a href="https://web.dev/articles/profiling-web-audio-apps-in-chrome" target="_blank" rel="noopener noreferrer">Web Audio API 専用のプロファイラ</a>があるのでおすすめです).
          </p>
        </section>
        <section id="section-prerequisite">
          <h3>前提知識と経験</h3>
          <p>
            前提知識としては, ECMAScript 2015 以降の JavaScript の言語仕様を理解していることと, Web ブラウザを実行環境にした JavaScript による Web
            アプリケーションを実装した経験ぐらいです. Web Audio API は, ユースケースにおいて想定されるオーディオ信号処理を抽象化しているので,
            オーディオ信号処理に対する理解がなくても, それなりのアプリケーションは制作できます (アプリケーションの仕様しだいでは不要になるぐらいです). もちろん,
            オーディオ信号処理の理解や Web 以外のプラットフォームでのオーディオプログラミングの経験 (特に, GUI
            で必要なリアルタイム性のオーディオプログラミングの経験) があれば, それは Web Audio API を理解するうえで活きます. Web Audio API
            が標準でサポートしないようなオーディオ処理を実現したいケースではむしろ必要になります.
          </p>
          <p>
            また, 音楽理論に対する知識も不要です. Web Audio API はユースケースとして, 音楽用途に限定していないからです. したがって, このサイトでは,
            アプリケーションによっては必要になるドメイン知識として位置づけます (もちろん, ユースケースとして, 音楽用途も想定されているので, Web
            をプラットフォームにした音楽アプリケーションを制作する場合には必要となるケースが多いでしょう).
          </p>
          <p>
            このサイトでは, オーディオ信号処理や音楽理論など必要に応じて解説します. Web Audio API が解説の中心ではありますが, Web Music
            アプリケーションを制作するための標準ドキュメントとなることを目指すからです (オーディオ信号処理や音楽理論を深入りする場合は,
            それぞれ最適なドキュメントや書籍がたくさんあるのでそちらを参考にしてください).
          </p>
        </section>
        <article id="section-skeptical">
          <h4>Web Audio API に対する懐疑的な意見</h4>
          <p>
            Web Audio API は, 他のプラットフォームのオーディオ API と比較すると, やや奇怪な API 設計であったり, 仕様策定されたころの JavaScript の事情と, 現代の
            JavaScript の事情が様変わりしたりしたことから, 懐疑的な意見もあります (参考
            <a href="https://zenn.dev/okuoku/articles/13c39882596c92" target="_blank" rel="noopener noreferrer">WebAudioは何故あんな事になっているのか</a>).
            しかしながら, この記事でも述べられているように,
            <q>実はWebAudioはオーディオAPIのオープンスタンダードとしては唯一生き残っている存在と言える。</q> これはたしかで, その点において学ぶ意義はあります.
            音楽アプリケーションとして Web をプラットフォームにする場合は必須となるでしょう.
          </p>
        </article>
        <section id="section-contribution-to-document">
          <h3>Issue と Pull Requests</h3>
          <p>
            プロローグの最後に, このサイト (ドキュメント) はオープンソースとして
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation" target="_blank" rel="noopener noreferrer">GitHub</a> に公開しています.
            このサイトのオーナーも完璧に理解しているわけではないので, 間違いもあるかと思います. その場合には, GitHub に
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/issues" target="_blank" rel="noopener noreferrer">issue</a> を作成したり,
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/pulls" target="_blank" rel="noopener noreferrer">Pull Requests</a>
            を送っていただいたりすると大変ありがたいです.
          </p>
          <p>それでは, Web Music の未来を一緒に開拓していきましょう !</p>
        </section>
      </section>
      <section id="section-getting-started">
        <h2>Getting Started</h2>
        <section id="section-audio-context">
          <h3>AudioContext</h3>
          <p>
            Web Audio API を使うためには, <code>AudioContext</code> クラスのコンストラクタを呼び出して,
            <code>AudioContext</code> インスタンスを生成する必要があります. <code>AudioContext</code> インスタンスが Web Audio API
            で可能なオーディオ処理の起点になるからです. <code>AudioContext</code> インスタンスを生成することで, Web Audio API
            が定義するプロパティやメソッドにアクセス可能になるわけです.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();</code></pre>
          <p>
            何らかの理由で, レガシーブラウザ (特に, モバイルブラウザ) もサポートしなければならない場合, ベンダープレフィックスつきの
            <code>webkitAudioContext</code> もフォールバックとして設定しておくとよいでしょう (少なくとも, デスクトップブラウザでは不要な処理で,
            これから将来においては確実に不要になる処理ではありますが).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">window.AudioContext = window.AudioContext || window.webkitAudioContext;

const context = new AudioContext();</code></pre>
          <p><code>AudioContext</code> インスタンスをコンソールにダンプしてみます.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

console.dir(context);</code></pre>
          <p>
            <code>AudioContext</code> インスタンスに様々なプロパティやメソッドが実装されていることがわかるかと思います. このドキュメントではこれらを
            (すべてではありませんが) メインに解説していくことになります. また, このように実装を把握することで, 仕様と実装の乖離を調査することにも役立ちます.
          </p>
          <img src="images/audio-context.png" alt="AudioContext" width="1232" height="770" loading="lazy" />
          <p>
            Web Audio API でオーディオ処理を実装するうえで意識することはほとんどありませんが, <code>AudioContext</code> は <code>BaseAudioContext</code> を拡張
            (継承) したクラスであることもわかります.
          </p>
          <img src="images/base-audio-context.png" alt="BaseAudioContext" width="1232" height="770" loading="lazy" />
          <section id="section-autoplay-policy">
            <h4>Autoplay Policy 対策</h4>
            <p>
              Web Audio API に限ったことではないですが, ページが開いたときに, ユーザーが意図しない音を聞かせるのはよくないという観点から (つまり, UX
              上好ましくないという観点から), ブラウザでオーディオを再生する場合,
              <a href="https://developer.chrome.com/blog/autoplay#web_audio" target="_blank" rel="noopener noreferrer">Autoplay Policy</a>
              という制限がかかります. これを解除するためには, <b>ユーザーインタラクティブなイベント</b> 発火後に
              <code>AudioContext</code> インスタンスを生成するか, もしくは, <code>AudioContext</code> インスタンスの <code>resume</code> メソッドを実行して
              <code>AudioContextState</code> を <code>&apos;running&apos;</code> に変更する必要があります. これをしないと, オーディオを鳴らすことができません.
              また, <code>decodeAudioData</code> など一部のメソッドが Autoplay Policy 解除まで実行されなくなります. ユーザーインタラクティブなイベントとは,
              <code>click</code>, <code>mousedown</code> や <code>touchstart</code> などユーザーが明示的に操作することによって発火するイベントのことです.
              したがって, <code>load</code> イベントや <code>mousemove</code> など, 多くのケースにおいてユーザが明示的に操作するわけではないようなイベントでは
              Autoplay Policy の制限を解除することはできません.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">document.addEventListener(&apos;click&apos;, () =&gt; {
  const context = new AudioContext();
});</code></pre>
            <p>
              <code>resume</code> メソッドで解除する場合 (この場合, コンソールには警告メッセージが表示されますが, Autoplay Policy
              は解除できるので無視して問題ありません).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

document.addEventListener(&apos;click&apos;, async () =&gt; {
  await context.resume();
});</code></pre>
            <p>
              <b>これ以降のセクションでは, 本質的なコードを表記したいので, Autoplay Policy は解除されている状態を前提とします.</b>
            </p>
          </section>
        </section>
        <section id="section-audio-node">
          <h3>AudioNode</h3>
          <p>
            Web Audio API におけるオーディオ処理の基本は, <code>AudioNode</code> クラスのインスタンス生成と <code>AudioNode</code> がもつ
            <code>connect</code> メソッドで <code>AudioNode</code> インスタンスを接続していくことです. <code>AudioNode</code> クラスは,
            それ自身のインスタンスを生成することはできず, <code>AudioNode</code> を拡張 (継承) したサブクラスのインスタンスを生成して, オーディオ処理に使います.
            <code>AudioNode</code> はその役割を大きく 3 つに分類することができます.
          </p>
          <ul>
            <li>サウンドの入力点となる <code>AudioNode</code> のサブクラス (<code>OscillatorNode</code>, <code>AudioBufferSourceNode</code> など)</li>
            <li>サウンドの出力点となる <code>AudioNode</code> のサブクラス (<code>AudioDestinationNode</code>)</li>
            <li>
              音響特徴量を変化させる <code>AudioNode</code> のサブクラス (<code>GainNode</code>, <code>DelayNode</code>, <code>BiquadFilterNode</code> など)
            </li>
          </ul>
          <p>
            現実世界のオーディオ機器に例えると, サウンドの入力点に相当する <code>AudioNode</code> のサブクラスが, マイクロフォンや楽器, 楽曲データなどに相当,
            サウンドの出力点に相当する <code>AudioNode</code> のサブクラスが. スピーカーやイヤホンなどに相当, そして, 音響特徴量を変化させる
            <code>AudioNode</code> のサブクラスがエフェクターやボイスチェンジャーなどが相当します.
          </p>
          <p>
            これらの, <code>AudioNode</code> のサブクラスを使うためには, <b>コンストラクタ呼び出し</b>, または,
            <b><code>AudioContext</code> インスタンスに実装されているファクトリメソッド</b> 呼び出す必要があります (ただし, サウンドの出力点となる
            <code>AudioDestinationNode</code> は <code>AudioContext</code> インスタンスの <code>destination</code> プロパティでインスタンスとして使えるので,
            コンストラクタ呼び出しやファクトリメソッドは定義されていません).
          </p>
          <p>例えば, 入力として, オシレーター (<code>OscillatorNode</code>) を使う場合, コンストラクタ呼び出しの実装だと以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);</code></pre>
          <p>
            インスタンス生成時には, その <code>AudioNode</code> のサブクラスに定義されているパラメータ (<code>OscillatorNode</code> の場合,
            <code>OscillatorOptions</code>) を指定することも可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 880 });</code></pre>
          <p>ファクトリメソッドでインスタンス生成する場合, 以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = context.createOscillator();</code></pre>
          <p>
            コンストラクタ呼び出しによる, <code>AudioNode</code> のサブクラスのインスタンス生成は, Web Audio API の初期には仕様策定されておらず,
            <code>AudioContext</code> インスタンスに実装されているファクトリメソッド呼び出す実装のみでした. インスタンス生成時に,
            パラメータを変更可能なことから, どちらかと言えば, コンストラクタ呼び出しによるインスタンス生成が推奨されているぐらいですが,
            ファクトリメソッドが将来非推奨になることはなく, また, 初期の仕様には仕様策定されていなかったことから,
            <b>レガシーブラウザの場合, コンストラクタ呼び出しが実装されていない場合もあります</b>. したがって, サポートするブラウザが多い場合は,
            ファクトリメソッドを, サポートするブラウザが限定的であれば, コンストラクタ呼び出しを使うのが現実解と言えるでしょう.
          </p>
          <section id="section-connect-audio-node">
            <h4>connect メソッド (AudioNode の接続)</h4>
            <p>
              現実世界の音響機器では, 入力と出力, あるいは, 音響変化も接続することで, その機能を果たします. 例えば, エレキギターであれば,
              サウンド入力を担うギターとサウンド出力を担うアンプ (厳密にはスピーカー) は, 単体ではその機能を果たしません.
              シールド線などで接続することによって機能します.
            </p>
            <p>
              このことは, Web Audio API の世界も同じです. (<code>AudioContext</code> インスタンスを生成して,) サウンド入力点となる
              <code>AudioNode</code> のサブクラスのインスタンス (先ほどのコード例だと, <code>OscillatorNode</code> インスタンス) と, サウンド出力点となる
              <code>AudioDestinationNode</code> インスタンスを生成しただけではその機能を果たしません. 少なくとも,
              サウンド入力点と出力点を接続する処理が必要となります (さらに, Web Audio API が定義する様々なノードと接続することで, 高度なオーディオ処理を実現する
              API として真価を発揮します).
            </p>
            <p>
              Web Audio API のアーキテクチャは, 現実世界における音響機器のアーキテクチャと似ています. このことは, Web Audio API
              の理解を進めていくとなんとなく実感できるようになると思います.
            </p>
            <p>
              Web Audio APIにおいて「接続」の役割を担うのが, <code>AudioNode</code> がもつ <b><code>connect</code> メソッド</b>です. 実装としては,
              <code>AudioNode</code> サブクラスのインスタンスの, <code>connect</code> メソッドを呼び出します. このメソッドの第 1 引数には, 接続先となる
              <code>AudioNode</code> のサブクラスのインスタンスを指定します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);</code></pre>
            <p>
              サウンドの入力点と出力点を接続し, 最小の構成を実装できました. しかし, まだ音は出せません. なぜなら,
              サウンドを開始するための音源スイッチをオンにしていないからです. 現実世界の音響機器も同じです. 現実世界がそうであるように, Web Audio API
              においても, 音源のスイッチをオン, オフする必要があります. そのためには, <code>OscillatorNode</code> クラスがもつ
              <b><code>start</code> メソッド</b>, <b><code>stop</code> メソッド</b> を呼び出します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);</code></pre>
            <p>
              <code>start</code> メソッドの引数に <code>0</code> を指定していますが, これはメソッドが呼ばれたら, 即時にサウンドを開始します.
              <code>stop</code> メソッドの引数には, <code>AudioContext</code> インスタンスの <b><code>currentTime</code></b> プロパティに
              <code>2.5</code> を加算した値を指定していますが, これは, <code>stop</code> メソッドを実行してから, 2.5
              秒後に停止することをスケジューリングしています (詳細は, のちほどのセクションで Web Audio API におけるスケジューリングとして解説しますが,
              <code>AudioContext</code> インスタンスの <code>currentTime</code> は,
              <b><code>AudioContext</code> インスタンスが生成されてからの経過時間を秒単位で計測した値</b>が格納されています. より厳密には,
              <b><code>AudioContextState</code> が <code>&apos;running&apos;</code> である状態での経過秒数</b>となります). <code>stop</code> メソッドの引数も
              <code>0</code> を指定すれば即時にサウンドを停止します. ちなみに, <code>start</code> メソッド, <code>stop</code> メソッドもデフォルト値は
              <code>0</code> なので, 引数を省略して呼び出した場合, 即時にサウンドを開始, 停止します.
            </p>
            <p>これで, とりあえず, ブラウザ (Web) で音を鳴らすことができました !</p>
          </section>
        </section>
        <section id="section-audio-param">
          <h3>AudioParam</h3>
          <p>
            サウンドの入力点と出力点を生成して, それらを接続するだけでは, 元の入力音をそのまま出力するだけなので高度なオーディオ処理はできません. むしろ, Web
            Audio API において重要なのは, この入力と出力の間に, 音響変化をさせる <code>AudioNode</code> を接続することです. 音響変化をさせるためには,
            音響変化のためのパラメータを取得・設定したり, 周期的に変化させたり (LFO) できる必要があります. Web Audio API において, その役割を担うのが
            <b><code>AudioParam</code></b> クラスです. <code>AudioNode</code> が現実世界の音響機器と例えをしましたが, それに従うと,
            <code>AudioParam</code> クラスはノブやスライダーなど音響機器のパラメータを設定するコントローラーのようなものです.
          </p>
          <p>
            <code>AudioParam</code> クラスは直接インスタンス化することはありません. <code>AudioNode</code> のプロパティとして,
            <code>AudioNode</code> のサブクラスのインスタンスを生成した時点でインスタンス化されているのでプロパティアクセスで参照することが可能です.
          </p>
          <p>
            <code>AudioParam</code> では, 単純なパラメータの取得や設定だけでなく, そのパラメータを周期的に変化させたり (LFO), スケジューリングによって変化させる
            (エンベロープジェネレーターなど) ことが可能です (ここはオーナーの経験からですが, Web Audio API で高度なオーディオ処理を実装するためには,
            <code>AudioParam</code> を理解して音響パラメータを制御できるようになるかが非常に重要になっていると思います).
          </p>
        </section>
        <section id="section-gain-node">
          <h3>GainNode (AudioNode インタンスの生成と接続, AudioParam の取得と設定)</h3>
          <p>
            <code>AudioNode</code> と <code>AudioParam</code> の具体的な利用例として, このセクションでは, <b><code>GainNode</code></b> を利用して,
            パラメータの取得・設定を実装します. <code>GainNode</code> はその命名のとおり, <b>ゲイン</b> (<b>増幅率</b>), つまり, 入力に対する出力の比率 (入力を
            <code>1</code> としたときに出力の値) を制御するための <code>AudioNode</code> で, Web Audio API におけるオーディオ処理で頻繁に使うことになります.
            このセクションでは, 単純に, <code>GainNode</code> の <b><code>gain</code></b> プロパティ (<code>AudioParam</code> インスタンス) を参照して,
            そのパラメータを取得・設定してみます (この実装例では, 音量の制御と考えても問題ありません).
          </p>
          <p>
            <code>GainNode</code> も <code>AudioNode</code> のサブクラスなので, コンストラクタ呼び出し, または, ファクトリメソッドで
            <code>GainNode</code> インスタンスを生成できます.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context);</code></pre>
          <p>
            コンストラクタ呼び出しで生成する場合, 初期パラメータ (<b><code>GainOptions</code></b> 型) を指定することも可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context, { gain: 0.5 });</code></pre>
          <p>ファクトリメソッドで生成する場合.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = context.createGain();</code></pre>
          <p><code>GainNode</code> インスタンスを生成したら, <code>OscillatorNode</code> と <code>AudioDestinationNode</code> の間に接続します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context, { gain: 0.5 });

// OscillatorNode (Input) -&gt; GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);</code></pre>
          <p>これで実際にサウンドを発生させると, 音の大きさが小さく聴こえるはずです.</p>
          <p>
            このコードだと, 初期値を変更しているだけなので, 例えば, ユーザー操作によって変更するといったことができないので,
            インスタンス生成時以外でパラメータを設定したり, 取得したりする場合は, <code>GainNode</code> の <code>gain</code> プロパティを参照します. これは,
            先ほども記載したように, <code>AudioParam</code> インスタンスです. パラメータの取得や設定をするには, その
            <b><code>value</code></b> プロパティにアクセスします.
          </p>
          <p>簡単な UI として, 以下の HTML があるとします.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;label for=&quot;range-gain&quot;&gt;gain&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-gain&quot; value=&quot;1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-gain-value&quot;&gt;1&lt;/span&gt;</code></pre>
          <p>
            この <code>input[type=&quot;range&QUOT;]</code> のイベントリスナーで, <code>input[type=&quot;range&QUOT;]</code> で入力された値 (JavaScript の
            <code>number</code> 型) を <code>gain</code> (<code>AudioParam</code> インスタンス) の <code>value</code> プロパティに設定し, また,
            その値を取得して, HTML に動的に表示します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context);

// OscillatorNode (Input) -&gt; GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

const spanElement = document.getElementById(&apos;print-gain-value&apos;);

document.getElementById(&apos;range-gain&apos;).addEventListener(&apos;input&apos;, (event) =&gt; {
  gain.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = gain.value;
});</code></pre>
          <p>
            <code>AudioParam</code> のパラメータの取得や設定は, このように, JavaScript のオブジェクトに対するプロパティの getter や setter と同じなので,
            特に違和感なく理解できるのではないでしょうか (設定に関しては,
            あとのセクションで解説するパラメータのオートメーションメソッドを利用する方法もあります).
          </p>
        </section>
        <p>
          <code>AudioNode</code> や <code>AudioParam</code> の接続によって構成されるオーディオデータのルーティングを, Web Audio API の仕様では,
          <b>オーディオグラフ</b> (<b>Audio Graph</b>) と用語定義しています.
        </p>
        <p>
          このセクションでは, Web Audio API の設計の基本となる (Web Audio API のアーキテクチャを決定づけている), <code>AudioContext</code>,
          <code>AudioNode</code>, <code>AudioParam</code> の関係性とそのパラメータの取得・設定の実装を解説しました. 以降のセクションでは, ユースケースに応じて,
          これら 3 つのクラスの詳細についても解説を追加していきます.
        </p>
      </section>
      <section id="section-about-sound">
        <h2>「音」とは ?</h2>
        <p>
          このセクションでは, そもそも「音」とはなにか ? からスタートして, 音の特性について簡単に解説します (いわゆる,
          <b>音響学</b>の基本のほんの一部分を解説します). 網羅的な解説はしないので, Web Audio API を理解するうえで, 最低限の解説をできるだけ簡単に解説します.
          また, そのために, 厳密さは犠牲にしている解説も多くあると思います. 音のスペシャリストの方からすると, ちょっと違う ...
          という部分はたくさんあるかと思いますがご了承ください (ただし, あきらかに間違った解説や誤解を招く可能性のある解説については遠慮なく Issue を作成したり,
          Pull Requests を送ったりしていただければと思います).
        </p>
        <p>
          Web Audio API について解説するセクションではないので, 音の特性 (音響学) に関して学んだことあれば,
          このセクションはスキップしていただくのがよいでしょう.
        </p>
        <section id="section-what-is-sound">
          <h3>音の実体</h3>
          <p>
            そもそも, 「音」って何なのでしょうか？ 結論としては, 音とは媒体の振動が聴覚に伝わったものと定義することができます.
            「媒体」というものが抽象的でよくわからないかもしれませんが, 具体的には, 空気や水です. 日常の多くの音は空気を媒体として,
            空気の振動が聴覚に伝わることで音として知覚するわけですが, 同じことは水中でも起きます. また,
            普段聴いている自分の声は骨を媒体にして伝わっている音です.
          </p>
        </section>
        <section id="section-modeling-sound">
          <h3>音のモデリング</h3>
          <p>
            音をコンピュータで表現するためには, 媒体の振動を数式で表現して, その数式によって導出される数値を 2 進数で表現できる必要があります.
            音の実体は媒体の振動というのを説明しましたが, この振動を表現するのに適した数学的な関数が, <b>sin 関数</b>です (cos 関数は sin
            関数の位相の違いでしかないので本質的に同じと考えてもよいでしょう. また, tan 関数は含まれません. その理由は,
            <span class="math-inline">$\frac{\pi}{2}$</span> や <span class="math-inline">$-\frac{\pi}{2}$</span> で
            <span class="math-inline">$\infty$</span> や
            <span class="math-inline">$-\infty$</span> になるので振動を表現するには都合が悪いからと考えてよいでしょう).
          </p>
          <p>
            Web Audio APIでも, <code>OscillatorNode</code> の <code>type</code> プロパティがとりうる値 (<code>OscillatorType</code>) の 1 つとして
            <code>&#039;sine&#039;</code> が定義されています.
          </p>
          <p>
            音を扱う学問や工学では, この sin 関数が, 音の波 (<b>音波</b>) をモデリングしていることから, <b>正弦波</b> (<b>sin 波</b>) と呼ぶことが多いです.
            とちらであっても, 実体は同じなのですが, このドキュメントではこれ以降, 慣習にしたがって, 正弦波 (sin 波) と記述することにします.
          </p>
          <section id="section-sine-wave">
            <h3>正弦波 (sin 波)</h3>
            <p>ここからは少し数学・物理的な話になってきます. 正弦波 (sin 関数) ってどんな形か覚えてらっしゃいますか？</p>
            <figure>
              <svg id="svg-figure-sin-function" width="720" height="405" data-a="1.0" data-f="1" />
              <figcaption>正弦波 (sin 関数)</figcaption>
            </figure>
            <p>具体的に解説するためにパラメータを設定します.</p>
            <figure>
              <svg
                id="svg-figure-sin-function-with-parameters-1-1Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="1"
                data-f="1"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>パラメータつき正弦波 (sin 関数)</figcaption>
            </figure>
            <section id="section-amplitude-and-frequency">
              <h4>振幅と周波数 (周期)</h4>
              <p>
                まず, 縦軸に着目してみます. 縦軸のパラメータは, <b>振幅</b>と呼ばれ, 単位はありません. ちなみに, 振幅 <code>1</code> の正弦波と表現した場合,
                上記のように振幅の最大値が <code>1</code>, 最小値が <code>-1</code>の 正弦波のことを意味しています. 次に, 横軸に着目してみます.
                横軸のパラメータは, <b>時間</b>を表しています. 縦軸との関係で表現すると, ある時刻における正弦波の振幅値を表した図 (グラフ) と言えます. ここで,
                パラメータつきの正弦波を見てみます. すると, 山 1 つと谷 1 つを最小の構成として, それが繰り返されている, すなわち,
                <b>周期性</b>をもつことがわかります. 数学的には, すべての時間
                <span class="math-inline">$t \left(0 \leqq {t} &lt; \infty \right)$</span> に対して,
                <span class="math-inline">$f\left(t + L\right) = f\left(t\right)$</span> となる定数が存在するとき,
                <span class="math-inline">$f\left(t\right)$</span> は周期 <span class="math-inline">$L$</span> の<b>周期関数</b>と定義されます. そして, sin
                関数は, 周期 <span class="math-inline">$L$</span> としたとき
                <span class="math-inline">$\sin\left(t + L\right) = \sin\left(t\right)$</span> が成立するので, <b>正弦波 (sin 関数) は周期関数</b>です.
              </p>
              <p>
                この波の最小の構成が発生するために要する時間を<b>周期</b>と呼びます. 例として, 上記の正弦波で考えると, 最小の構成の発生までに
                <code>1 sec</code> の時間を要しているので, 周期は <code>1 sec</code> となります. この真逆の概念を表す用語が<b>周波数</b>です. すなわち,
                <code>1 sec</code> の間に, 波の最小の構成が何回発生するか ? ということを表し, 単位は <b>Hz</b> (ヘルツ) です. Hz (ヘルツ) という名前ですが,
                日本語に翻訳すれば, 何回の「回」に相当するでしょう. 上記の正弦波で考えると. この正弦波は, <code>1 sec</code> の間に最小の構成が
                <code>1</code> 回発生しているので, 周波数は, <code>1 Hz</code> ということになります.
              </p>
              <p>
                周期と周波数は互いに真逆の概念ですが, これは数学的には, 互いに<b>逆数</b>の関係にあります. すなわち,
                <b>周期の逆数は周波数を表し, 周波数の逆数は周期を表します</b>. 互いに関係のある値なので, 周期の話をすれば周波数の話も同時にしていることであり,
                周波数の話をすれば周期の話も同時にしていることになります. ただ, 周波数という用語のほうがよく使われる傾向にあると思うので, このドキュメントでは,
                周波数の用語を優先的に利用することにします.
              </p>
              <p>少し慣れるために, パラメータ (振幅や周波数) を変えた正弦波 (sin 波) を見てましょう.</p>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-2Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="0.5"
                  data-f="1"
                  data-t="0.0,0.5,1.0"
                />
                <figcaption>振幅 <code>0.5</code>, 周波数 <code>1 Hz</code> (周期 <code>1 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-0.5-1Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="2"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>2 Hz</code> (周期 <code>0.5 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-0.5Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="1"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>0.5 Hz</code> (周期 <code>2 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
            </section>
            <p>いかがでしたか ? 振幅と周波数は Web Audio API の解説においても頻出する用語なので, ある程度理解しておくと, Web Audio API の理解も進むでしょう.</p>
          </section>
          <section id="section-synthesizer-waveforms">
            <h3>基本波形</h3>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値は, 正弦波を生成する文字列
              <code>&apos;sine&apos;</code> 以外にも, 矩形波を生成する <code>&apos;square&apos;</code> やノコギリ波を生成する <code>&apos;sawtooth&apos;</code>,
              三角波を生成する <code>&apos;triangle&apos;</code> があります. 正弦波の形はわかりましたが, それ以外はどのような形をしているのか見てみましょう.
            </p>
            <figure>
              <svg
                id="svg-figure-square-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の矩形波 (<code>&apos;square&apos;</code>)</figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-sawtooth-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>
                振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) のノコギリ波 (<code>&apos;sawtooth&apos;</code>)
              </figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-triangle-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の三角波 (<code>&apos;triangle&apos;</code>)</figcaption>
            </figure>
            <p>
              矩形波・ノコギリ波・三角波のいずれも正弦波と同じように, 周期性をもつ波 (関数) であるということです. 周期性をもつので,
              周波数の概念を適用することができます. そして, 最も重要な点ですが, 周期性をもつ波は周波数の異なる正弦波を合成してできるということです.
              矩形波・ノコギリ波・三角波はいずれも周期性をもちます. 周期性をもつので,
              矩形波・ノコギリ波・三角波はいずれも周波数の異なる正弦波を合成して生成することができます. シンセサイザーでも,
              正弦波・矩形波・ノコギリ波・三角波は基本波形として, サウンド生成のベースとなる波形です. そして, Web Audio API においても, 基本波形はサウンド生成
              (<code>OscillatorNode</code>) のベースになる波形です.
            </p>
          </section>
        </section>
        <section id="section-the-three-components-of-sound">
          <h3>音の 3 要素</h3>
          <p>ここまで, 数学・物理的な話が続いたので, 少し気分を変えて, 感覚視点 (知覚) から音を考えてみましょう.</p>
          <p>
            日常でも, 「音が大きい・小さい」, 音楽を聴いていて「音が高い・低い」,
            楽器を演奏していて「この楽器の音色が好き」などと表現することがあるかと思います. これらは, 音を感覚視点, すなわち, <b>音を知覚するときの視点</b>で,
            どんな音か ? を表現しています. これらの表現にある, <b>音の大きさ</b>・<b>音の高さ</b>・<b>音色</b>を<b>音の 3 要素</b>と呼びます.
          </p>
          <p>音の 3 要素と, 先に解説した振幅・周波数・波形と大きな関わりがあります.</p>
          <dl>
            <dt>音の大きさ (Loudness)</dt>
            <dd>振幅が大きく影響する</dd>
            <dt>音の高さ (Pitch)</dt>
            <dd>周波数が大きく影響する</dd>
            <dt>音色 (Timbre)</dt>
            <dd>波形 (エンベロープ) が大きく影響する</dd>
          </dl>
          <p>
            <b>大きく影響する</b>という表現に注意してください. 例えば, 音の大きさは振幅のみで決定されるわけではないということです. 知覚は主観的な指標であり,
            振幅・周波数・波形は物理量だからです. 物理現象である音と知覚を関連づける指標として, <b>音響特徴量</b> (等ラウドネス曲線や基本周波数, セントロイド,
            ケプストラムなど) が知られています.
          </p>
          <p>簡単な解説を記載しますが, これらの音響特徴量に関する詳細な解説は, 最適なドキュメントや書籍が豊富にあるので, そちらを参考にしてください.</p>
          <dl>
            <dt>等ラウドネス曲線</dt>
            <dd>
              周波数を横軸に, 音圧レベルを縦軸にして, 同じ音の大きさに聴こえる (知覚できる) 点をプロットして曲線で結んだグラフです, 等ラウドネス曲線から,
              音圧レベル (振幅) が同じでも周波数によって音の大きさが異なることが確認できます
            </dd>
            <dt>基本周波数</dt>
            <dd>音信号に含まれる最も低い周波数成分で, 特に, 音楽においては, 音高を決める重要な音響特徴量となります</dd>
            <dt>セントロイド</dt>
            <dd>
              他の工学分野でも利用される用語で「重心」という意味ですが, オーディオ信号処理においては, スペクトルの重心 (偏り) を表す音響特徴量で,
              音楽制作ではミキシング・マスタリングのプロセスで, セントロイドを調整をすることがあります
            </dd>
            <dt>ケプストラム</dt>
            <dd>
              音声分析で利用されることが多く, パワースペクトル (振幅を 2 乗したスペクトル) 対数をとって, 逆フーリエ変換した, 時間領域の信号 (音響特徴量) です.
              ケプストラムによって, 音声を声帯振動と声道フィルタの成分に分離して分析することが可能になります
            </dd>
          </dl>
        </section>
        <section id="section-web-audio-api-relation-to-sound">
          <h3>Web Audio API と音の関係</h3>
          <p></p>
          <section id="section-gain-relation-to-sound">
            <h4>GainNode の gain プロパティと音の大きさ</h4>
            <p>
              <code>GainNode</code>の <code>gain</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の大きさを変えることができます.
              物理的な視点で見ると, 振幅を操作することによって, 音の大きさを変えています.
            </p>
            <img src="images/gain-gain.png" alt="GainNode gain" width="1232" height="770" loading="lazy" />
          </section>
          <section id="section-frequency-relation-to-sound">
            <h4>OscillatorNode の frequency プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>frequency</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の高さを変えることができます.
              物理的な視点で見ると, 周波数を操作することによって, 音の高さを変更しています.
            </p>
            <img src="images/oscillator-frequency.png" alt="OscillatorNode frequency" width="1232" height="770" loading="lazy" />
            <p>
              仕様では, <code>frequency</code> プロパティのとりうる値の範囲は, 負のナイキスト周波数からナイキスト周波数までですが (ナイキスト周波数は,
              <a href="#section-analog-to-digital-conversion-sampling">サンプリング</a>のセクションで解説しています. ナイキスト周波数について理解がなければ,
              おおよそ, <code>-20 kHz</code> ~ <code>20 kHz</code> と大雑把に把握していただいて問題ないです),
              音楽アプリケーションなどで出力する音としてはそこまで設定できてもあまり意味はないでしょう. その理由は,
              <b>人間が聴きとることが可能な音の周波数の範囲は <code>20 Hz</code> ~ <code>20000 Hz</code> (<code>20 kHz</code>) 程度だからです</b>.
            </p>
            <p>
              さらに, <b>音程</b> (音の高さの差) として知覚可能な周波数の上限, 言い換えると, 音楽として有効な音の周波数はもっと低くなります (ピアノ 88
              鍵の音域を参照してください).
            </p>
            <figure>
              <svg id="svg-figure-frequency-and-piano-frequency" width="1196" height="282" data-highlights="0,87" />
              <figcaption>ピアノ 88 鍵と周波数</figcaption>
            </figure>
          </section>
          <section id="section-detune-relation-to-sound">
            <h4>OscillatorNode の detune プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>detune</code> プロパティ (<code>AudioParam</code>) を利用することでも, 音の高さを変えることができます.
              物理的な視点も <code>frequency</code> プロパティと同じです. ただし, <code>detune</code> プロパティは, 音楽的な視点で音の高さを変更します.
              <code>detune</code> プロパティの用途は, (音楽で言う) 半音よりも小さい範囲で音の高さを調整したり,
              オクターブ違いの音を生成・合成したりするために利用します. この機能によって, きめ細かいサウンド生成が可能になったり,
              サウンドを合成する場合において厚みをもたせることが可能になったりします. シンセサイザーのファインチューン機能や, エフェクターの 1
              種であるオクターバーを実現するためにあると言えるでしょう.
            </p>
            <img src="images/oscillator-detune.png" alt="OscillatorNode detune" width="1196" height="770" loading="lazy" />
            <p>
              <code>frequency</code> プロパティの単位は Hz (ヘルツ) で, 波が 1 sec の間に何回発生するのかを意味していました. 一方で,
              <code>detune</code> プロパティの単位は <b>cent</b> (セント) です. これは, 音楽の視点から音の高さをとらえた単位で,
              <b>1 オクターブの音程を 1200 で等分した値</b>です.
            </p>
            <p>
              1 つ高いラとか, 1 つ低いラのことを, 1 オクターブ高いラ, 1 オクターブ低いラと表現することがあります.
              音楽的な視点でのオクターブはまさにそういう意味です.
            </p>
            <p>
              オクターブを物理的な視点でみると, <b>周波数比が 1 : 2 の関係にある音程</b>を意味しています. 具体的に説明すると, いわゆる普通のラ (A) (ギターの第 5
              弦の開放弦) の周波数は <code>440 Hz</code> です (キャリブレーションチューニングなどしている場合は別ですが ...). この音を基準に考えると, 1
              オクターブ高いラの周波数は <code>880 Hz</code> です. 周波数比が, 440 : 880 = 1 : 2 になります.
            </p>
            <p>
              話を cent に戻すと, この 1 : 2 の音程を 1200 で割った値が <code>1 cent</code> というわけです. なぜ, 1200 ?
              と疑問に思う方もいらっしゃると思いますが, ピアノをされる方は直感で理解できると思います. ピアノをされない方のために, 1
              オクターブの音程間にピアノの鍵盤がいくつあるか数えてみましょう. 1 オクターブ間であればいいので, 好きな音から始めてください.
            </p>
            <figure>
              <svg id="svg-figure-12-equal-temperament" width="1196" height="162" data-highlights="39,40,41,42,43,44,45,46,47,48,49,50" />
              <figcaption>1 オクターブの鍵盤数</figcaption>
            </figure>
            <p>
              数えてみると, <b>12</b> 個の鍵盤があります. 1 オクターブ間の音程を 1200 で割った (1200 分割した) 値が <code>1 cent</code> でしたので, 1
              オクターブ間の音程を 12 分割すると, <code>100 cent</code> ということになります. つまり, <code>100 cent</code> 値が高くなると,
              右隣の鍵盤の音の高さに変わるということです.
            </p>
            <p>
              例として, <code>440 Hz</code> のラ (A) の音を <code>100 cent</code> 高くすると, 右隣の鍵盤の ラ# (A#) に, さらに <code>100 cent</code> 高くすると,
              シ (B) になります. このように, <code>-100 cent</code> ~ <code>100 cent</code> の間の値を設定することによって,
              半音以下の音の高さの調整が可能になるわけです. また, <code>1200 cent</code>, あるいは, <code>-1200 cent</code> と <code>1200 cent</code>
              ごとに値を設定することにより, オクターブ単位で調整することも可能です.
            </p>
            <p>
              音楽では, 1 オクターブの音程を 12 等分した周波数比の関係を 12 平均音律と呼びます. 12 平均音律においては, 隣り合う音, つまり, 半音の周波数比は,
              およそ, 1 : 1.059463 (正確には, 1 : <span class="math-inline">$2^{\left(1 / 12\right)}$</span>) で, これが <code>100 cent</code> となるわけです.
            </p>
          </section>
          <section id="section-type-relation-to-sound">
            <h4>OscillatorNode の type プロパティと音色</h4>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値を利用することで, 正弦波だけでなく,
              矩形波やノコギリ波, 三角波を生成することができます. それによって, 音色を変化させることが可能です. ちなみに,
              波形の概形は<b>エンベロープ</b>と呼ばれます. <code>OscillatorNode</code> のみで制御可能な範囲では, この
              <code>type</code> プロパティに応じたエンベロープが音色に大きく影響しています.
            </p>
          </section>
          <p>
            このセクションのまとめとして, 基本波形, 振幅, 周波数を変化させたときの波形を視覚化するデモとなります. 波形の変化とともに, 知覚する音 (音の 3 要素)
            の変化を体感してみてください.
          </p>
          <div class="app-container">
            <svg id="svg-oscillator" class="svg-oscillator" width="720" height="240"></svg>
            <div>
              <button type="button" id="button-oscillator" class="button-oscillator">start</button>
              <form id="form-oscillator-type" class="form-oscillator-type">
                <label><span>sine</span><input type="radio" name="radio-oscillator-type" value="sine" checked /></label>
                <label><span>square</span><input type="radio" name="radio-oscillator-type" value="square" /></label>
                <label><span>sawtooth</span><input type="radio" name="radio-oscillator-type" value="sawtooth" /></label>
                <label><span>triangle</span><input type="radio" name="radio-oscillator-type" value="triangle" /></label>
              </form>
              <div class="ranges-oscillator">
                <label><span>gain</span><input type="range" id="range-gain" value="1" min="0" max="1" step="0.05" /></label>
                <label><span>frequency</span><input type="range" id="range-frequency" value="440" min="27.5" max="8000" step="0.5" /></label>
                <label><span>detune</span><input type="range" id="range-detune" value="0" min="-600" max="600" step="1" /></label>
              </div>
            </div>
          </div>
        </section>
      </section>
      <section id="section-oscillator-node">
        <h2>OscillatorNode</h2>
        <p>
          Web Audio API のアーキテクチャを解説するうえで, <code>OscillatorNode</code> は少し説明しましたが, このセクションでは, Web Audio API
          におけるサウンド生成・合成のベースとなる, <code>OscillatorNode</code> についてその詳細を解説します.
        </p>
        <p>
          シンセサイザーの基本波形の生成・合成, モジュレーション系エフェクターで必須となる LFO (Low-Frequency Oscillator) など, Web Audio API
          において用途の広い, コアとなる <code>AudioNode</code> です. LFO に関しては, エフェクターのセクションで解説するので,
          このセクションでは基本波形の生成・合成に関して解説します.
        </p>
        <section id="section-oscillator-node-type">
          <h3>type プロパティ (OscillatorOptions)</h3>
          <p>
            ただし, <code>&apos;custom&apos;</code> のみは特殊で, 直接値を設定するとエラーが発生します. これは, <code>OscillatorNode</code> の
            <code>setPeriodicWave</code> メソッドによって, 自動的に <code>&apos;custom&apos;</code> に設定されます. また, その引数として,
            <code>AudioContext</code> の <code>createPeriodicWave</code> メソッドで波形テーブルを生成する必要があります. 波形テーブルの生成は,
            スペクトルや倍音などオーディオ信号処理の知識が必要になるので, 別のセクションで解説します.
          </p>
        </section>
        <section id="section-oscillator-node-frequency-and-detune">
          <h3>frequency プロパティ (AudioParam) / detune プロパティ (AudioParam)</h3>
          <p>
            周波数を制御して音の高さを変更します. <code>frequency プロパティ</code> と <code>detune</code> プロパティを合わせて算出される周波数 (<span
              class="math-inline"
              >$f_{\mathrm{computed}}$</span>) は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$f_{\mathrm{computed}} = \mathrm{frequency} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
          <p>
            この数式は, <code>frequency</code> は物理的な視点 (Hz) で周波数を制御, <code>detune</code> は音楽的な視点 (cent)
            で周波数を制御することを意味しています.
          </p>
        </section>
        <section id="section-oscillator-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>OscillatorNode</code> のプロパティを設定して音の高さや音色を制御することはそれほど難しくないかと思います. また, 発音し続けるか, 1 度だけ発音
            (<code>start</code> メソッド)・停止 (<code>stop</code> メソッド) する場合も直感的に実装可能です. おそらく, 多くの場合, ハマってしまうのが,
            <code>OscillatorNode</code> の発音と停止を繰り返す場合です.
          </p>
          <p>
            <code>OscillatorNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>OscillatorNode</code> インスタンスは再度, 発音 (停止)
            することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合, <code>OscillatorNode</code> インスタンスを再生成して,
            再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code>stop</code> メソッド) を実行する必要があります.
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 発音・停止することを期待していますが, 2 回目のクリック以降は, 発音されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();
const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  // Start immediately
  // But, cannot start since the second times ...
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  // Stop immediately
  oscillator.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>
            期待する動作, つまり, 発音・停止を繰り返すするには, 一度 <code>start</code>・<code>stop</code>した
            <code>OscillatorNode</code> インスタンスは破棄して, 再度 <code>OscillatorNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // Start immediately
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (oscillator === null) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);

  // GC (Garbage Collection)
  oscillator = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>このような仕様なので, <code>start</code> メソッドを続けて呼んだり, <code>stop</code> メソッドを続けて呼んだりしても, エラーが発生します.</p>
          <p>
            <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は, さまざまなプラットフォームのオーディオ API のなかでも Web Audio
            API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web ではないプラットフォームのオーディオ API はここまで抽象化されている API
            すら少ないと思います).
          </p>
        </section>
        <section id="section-oscillator-node-synthesize">
          <h3>基本波形の合成</h3>
          <p>
            基本波形の合成, すなわち, Web Audio API における <code>OscillatorNode</code> の合成は直感的で, 必要なだけ
            <code>OscillatorNode</code> インスタンスを生成して, (最後の) 接続先として <code>AudioDestinationNode</code> を指定するだけです.
          </p>
          <p>
            ただし, そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています
            (逆に, この音割れ (クリッピング) をエフェクトとして使うのが歪み系エフェクターです). もしくは,
            <code>DynamicsCompressorNode</code> を接続して振幅を制御して, 意図しない音割れを防ぐこともできます (ただし, 厳密には,
            コンプレッサーは振幅の小さい音も相対的に大きくするので, 物理的にはまったく同じではありません).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let oscillatorC = null;
let oscillatorE = null;
let oscillatorG = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillatorC !== null) || (oscillatorE !== null) || (oscillatorG !== null)) {
    return;
  }

  oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
  oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
  oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

  const gain = new GainNode(context, { gain: 0.25 });

  // OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  oscillatorC.connect(gain);
  oscillatorE.connect(gain);
  oscillatorG.connect(gain);
  gain.connect(context.destination);

  // Start immediately
  oscillatorC.start(0);
  oscillatorE.start(0);
  oscillatorG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillatorC === null) || (oscillatorE === null) || (oscillatorG === null)) {
    return;
  }

  // Stop immediately
  oscillatorC.stop(0);
  oscillatorE.stop(0);
  oscillatorG.stop(0);

  // GC (Garbage Collection)
  oscillatorC = null;
  oscillatorE = null;
  oscillatorG = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
        </section>
      </section>
      <section id="section-audio-buffer-source-node">
        <h2>AudioBufferSourceNode</h2>
        <p>
          <b><code>AudioBufferSourceNode</code></b> は, <b>ワンショットオーディオの再生</b>を目的に利用します. ワンショットオーディオとは,
          ピアノやギターなど実際の楽器の音源を収録した WAVE ファイルや MP3 ファイルのことです. Web Audio API の仕様では, ユースケースとして,
          楽曲データに関しては, <b><code>MediaElementAudioSourceNode</code></b> を利用することを想定しているので, この点は注意が必要です. ただし,
          <code>AudioBufferSourceNode</code> を楽曲データの再生に使うこともできます. 現実解としてユースケースに反した利用をすることも多いです (これは,
          <code>AudioBufferSourceNode</code> がオーディオデータの実体である <code>AudioBuffer</code> インスタンスをもつので,
          オーディオ信号処理が適用しやすいことが理由として考えられます).
        </p>
        <p>このセクションでは, 仕様上のユースケースであるワンショットオーディオの再生を目的に, <code>AudioBufferSourceNode</code> を解説します.</p>
        <p>
          ところで, ワンショットオーディオの再生であれば, 同じことは <code>HTMLAudioElement</code> (<code>audio</code> タグ) でも可能な場合もあります. 事実, Web
          Audio API が仕様策定される以前は, そのようなユースケースも想定して, <code>Audio</code> コンストラクタが定義されています. しかしながら,
          <code>HTMLAudioElement</code> (<code>Audio</code> コンストラクタ) によるワンショットオーディオの再生は以下のような問題があります.
        </p>
        <ul>
          <li>JavaScript のタイマー (<code>setInterval</code> や <code>setTimeout</code>) では, 正確なスケジュールングが難しい</li>
          <li><code>HTMLAudioElement</code> のイベントハンドラでも精度が粗く, 正確なスケジュールングが難しい</li>
          <li>同時発音数の制限</li>
          <li>ワンショットオーディオに対して, さらにオーディオ処理を付加したいユースケース</li>
        </ul>
        <p>
          これらの問題を, ある程度容易に解決してくれるのが <code>AudioBufferSourceNode</code> です (もっとも, <code>AudioBufferSourceNode</code> を利用しても,
          コンピュータのリソースは有限なので, 計算量が多い場合や他のプロセスがリソースを多く消費している場合などは,
          少なからずスケジューリングも正確でなくなります).
        </p>
        <img src="images/audio-buffer-source-node.png" alt="AudioBufferSourceNode" width="1232" height="770" loading="lazy" />
        <section id="section-audio-buffer-source-node-buffer">
          <h3>buffer プロパティ</h3>
          <p>
            <code>AudioBufferSourceNode</code> において, 最も重要と言えるのが, <b><code>buffer</code></b> プロパティであり, これは,
            <b><code>AudioBuffer</code></b> インスタンスを参照します. <code>AudioBuffer</code> とは, オーディオデータの実体 (を抽象化するクラス) です.
          </p>
          <img src="images/audio-buffer.png" alt="AudioBuffer" width="1232" height="770" loading="lazy" />
          <section id="section-audio-buffer">
            <h4>AudioBuffer</h4>
            <p>
              <code>AudioBuffer</code> クラスは, オーディオデータの実体ですが, 直接的にアクセスすることはできません. そのためのメソッドや,
              デジタル化されたオーディオデータに必要なパラメータ (サンプリングレートやチャンネル数, オーディオデータ全体のサイズなど) を定義しています.
            </p>
            <section id="section-audio-buffer-sample-rate">
              <h5>sampleRate プロパティ</h5>
              <p>
                オーディオデータのサンプリング周波数です. これは, <code>createBuffer</code> メソッドで利用して
                <code>AudioBuffer</code> インスタンスを生成する場合, 実質的に意味のあるプロパティとなります. <code>decodeAudioData</code> メソッドで取得した
                <code>AudioBuffer</code> インスタンスは, <code>AudioContext</code> インスタンスの
                <code>sampleRate</code> プロパティの値にリサンプリングされるからです (つまり, その場合, <code>AudioContext</code> インスタンスの
                <code>sampleRate</code> プロパティを参照しても同じ値なので).
              </p>
            </section>
            <section id="section-audio-buffer-length">
              <h5>length プロパティ</h5>
              <p>
                1 チャネルにおける, オーディオデータのサイズです. つまり, <code>sampleRate</code> プロパティの逆数である<code>サンプリング周期</code>と
                <code>length</code> プロパティを乗算した値が, オーディオデータの再生時間となります (次に解説する,
                <code>duration</code> プロパティの値と同じになります).
              </p>
              <div class="math-block">$\mathrm{duration} = \frac{\mathrm{length}}{\mathrm{sampleRate}}$</div>
            </section>
            <section id="section-audio-buffer-duration">
              <h5>duration プロパティ</h5>
              <p>
                オーディオデータの再生時間 (単位は <code>sec</code>) です. 先ほど解説したように, <code>sampleRate</code> プロパティと
                <code>length</code> プロパティと関連している値となります.
              </p>
            </section>
            <section id="section-audio-buffer-number-of-channels">
              <h5>numberOfChannels プロパティ</h5>
              <p>
                オーディオデータのチャンネル数です. 例えば, モノラルであれば <code>1</code>, ステレオであれば <code>2</code>, 5.1 チャンネルであれば
                <code>6</code> になります. 次に解説する, <code>getChannelData</code> メソッドの引数の上限を決めている値になっています.
              </p>
            </section>
            <section id="section-audio-buffer-get-channel-data">
              <h5>getChannelData メソッド</h5>
              <p>
                <code>getChannelData</code> メソッドで引数で指定したチャンネルのオーディオデータを <code>Float32Array</code> として取得することが可能です.
                引数となるチャンネルの指定は <code>0</code> から <code>numberOfChannels - 1</code> までです. 例えば, ステレオ (<code>numberOfChannels</code> が
                <code>2</code>)であれば, <code>getChannelData(0)</code> で左チャンネルのオーディオをデータを <code>Float32Array</code> で取得し,
                <code>getChannelData(1)</code> で右チャンネルのオーディオデータを<code>Float32Array</code> で取得することができます.
              </p>
            </section>
            <section id="section-audio-buffer-copy">
              <h5>copyFromChannel メソッド / copyToChannel メソッド</h5>
              <p>
                他に, <code>AudioBuffer</code> をコピーするためのメソッドがあります. ワンショットオーディオの再生においてはおそらく使うことはないので,
                必要であれば, 仕様や MDN などを参考にしてください.
              </p>
            </section>
          </section>
          <section id="section-create-audio-buffer">
            <h4>AudioBuffer の生成</h4>
            <p>
              <code>AudioBuffer</code> クラスに関して簡単に解説しましたが, 肝心なのは
              <code>AudioBuffer</code> インスタンスをどうやって生成するのかということだと思います. Web Audio API では,
              <b><code>decodeAudioData</code></b> メソッドを利用するか, <b><code>createBuffer</code></b> メソッドを利用することによって,
              <code>AudioBuffer</code> インスタンスを生成可能です.
            </p>
            <p>
              もっとも, ワンショットオーディオ再生目的であれば, <code>createBuffer</code> メソッドを利用することはおそらくなく,
              <code>ArrayBuffer</code> インスタンスから <code>AudioBuffer</code> インスタンスを生成する
              <code>decodeAudioData</code> メソッドを利用することになると思います. したがって, まずは,
              <code>ArrayBuffer</code> インスタンスの取得に関して解説します (これは Web Audio API の解説というよりは, JavaScript で
              <code>ArrayBuffer</code> インスタンスを取得する方法なので, すでにご存知の場合はスキップして問題ないです).
            </p>
            <section id="section-array-buffer-and-decode-audio-data">
              <h5>ArrayBuffer の取得と decodeAudioData メソッド</h5>
              <p>
                クライアントサイド JavaScript で <code>ArrayBuffer</code> を取得するには, Web にあるリソースであれば, <code>Fetch API</code> (もしくは,
                <code>XMLHttpRequest</code>), ユーザーのファイルシステムから選択するのであれば <code>File API</code> と
                <code>FileReader API</code> を使うことになります.
              </p>
              <p>
                ワンショットオーディオ再生の場合, アプリケーション側であらかじめオーディオデータを Web にアップロードしているケースがほとんどなので,
                このセクションでは, <code>Fetch API</code> で <code>ArrayBuffer</code> を取得する実装を解説します.
              </p>
              <p>
                <code>Fetch API</code> は, <code>fetch</code> 関数, <code>Headers</code> オブジェクト, <code>Request</code> オブジェクト,
                <code>Response</code> オブジェクトの総称ですが, ほとんどのケースで明示的に利用するのは, <code>fetch</code> 関数の呼び出しです.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    // TODO: Create instance of `ArrayBuffer` by calling `decodeAudioData`
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                <code>fetch</code> 関数のデフォルトの HTTP メソッドは GET なので, ワンショットオーディオの取得であれば, そのリソースの URL
                を指定すればよいでしょう. あとは, 取得した <code>Response</code> オブジェクトの <code>arrayBuffer</code> メソッドを呼び出して,
                <code>ArrayBuffer</code> インスタンスを取得するだけです. いずれの関数・メソッドも, <code>Promise</code> を返します. 可読性重視などであれば,
                <code>async</code>/<code>await</code> で実装してもよいでしょう.
              </p>
              <p>
                <code>ArrayBuffer</code> インスタンスが取得できたら, <code>AudioContext</code> インスタンスの <b><code>decodeAudioData</code></b> メソッドの第 1
                引数に, <code>ArrayBuffer</code> インスタンスを指定して, 第 2 引数に, 成功時のコールバック関数を指定します. このコールバック関数の引数に,
                <code>AudioBuffer</code> インスタンスが渡されます. 失敗した場合, 第 3 引数のコールバック関数が実行されます. このコールバック関数の引数には,
                <code>DOMException</code> インスタンスが渡されます.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      // Create instance of `AudioBufferSourceNode`
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                初期の頃は上記のような仕様でしたが, 最新の仕様では, 成功時は <code>Promise&lt;AudioBuffer&gt;</code> を返すので, 戻り値から
                <code>AudioBuffer</code> インスタンスを取得することも可能です.
              </p>
              <p>
                <code>decodeAudioData</code> メソッドの実行で 1 つ注意しなければならないのは, <code>decodeAudioData</code> メソッドも
                <b><a href="#section-autoplay-policy">Autoplay Policy</a></b> の影響を受けるということです. したがって,
                ユーザーインタラクティブなイベント発生後に実行する必要があります.
              </p>
            </section>
            <section id="section-create-buffer">
              <h5>createBuffer メソッド</h5>
              <p>
                <code>AudioBuffer</code> インスタンスを生成するには, <code>AudioContext</code> インスタンスの
                <b><code>createBuffer</code></b> メソッドを利用することでも可能です. 引数は, 第 1 引数にチャンネル数, 第 2 引数に 1
                チャンネルのオーディオデータのサイズ, 第 3 引数にサンプリング周波数を指定します. しかしながら, インスタンスは生成できるものの,
                オーディオデータをもっているわけではないので, ワンショットオーディオの再生において利用することはないでしょう. ユースケースとしては,
                オーディオデータから生成した <code>AudioBuffer</code> インスタンスからコピー (<code>copyFromChannel</code> メソッドや
                <code>copyToChannel</code> メソッドが必要なケース) が考えられます.
              </p>
            </section>
            <p>
              これで, ワンショットオーディオを再生する最低限の処理ができているので, あとは <code>AudioBufferSourceNode</code> のインスタンスを生成します
              (ファクトリメソッドで生成する場合, <code>createBufferSource</code> メソッドを利用します).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

      // If use `createBufferSource`
      // const source = context.createBufferSource();
      //
      // source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          </section>
        </section>
        <section id="section-audio-buffer-source-node-playback-rate-and-detune">
          <h3>playbackRate プロパティ / detune プロパティ</h3>
          <p>
            音楽用途でワンショットオーディオを使う場合, 対応するピッチの数だけ, ワンショットオーディオデータを作成するのは大変です. また, HTTP
            リクエストの送受信や <code>decodeAudioData</code> メソッドの実行も多くなってしまうのでパフォーマンス的にもよくありません. それを解決するのが,
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティです. これらは, 音の物理的な性質, つまり,
            <b>再生速度を変化させるとピッチも変化する</b>という性質を利用して, ピッチ (と再生時間) を変更します. 例えば, <code>playbackRate</code> を
            <code>2</code> に設定すれば, ピッチも 2 倍, つまり, 1 オクターブ高いピッチのオーディオデータの再生を同一の
            <code>AudioBuffer</code> インスタンスから可能です. <code>detune</code> は, cent 単位でピッチを変更します. ピッチを変更すると,
            再生時間も変わりますが, ワンショットオーディオは再生時間が短時間なので, この点が問題になることはほとんどないでしょう. いずれも,
            <code>AudioParam</code> インスタンスなので, 値を取得したり, 設定する場合は, <code>value</code> プロパティにアクセスします.
          </p>
          <p>
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティを考慮した, 実際の再生速度
            <span class="math-inline">$p_{\mathrm{computed}}$</span> は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$p_{\mathrm{computed}} = \mathrm{playbackRate} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
        </section>
        <section id="section-audio-buffer-source-node-loop">
          <h3>loop プロパティ / loopStart プロパティ / loopEnd プロパティ</h3>
          <p>
            ワンショットオーディオをループ再生させたい場合, <code>loop</code> プロパティを <code>true</code> に設定します. また, <code>loop</code> プロパティを
            <code>true</code> に設定することで, <code>loopStart</code> プロパティと <code>loopEnd</code> プロパティが有効になります. これらのプロパティは,
            ループ再生するオーディオデータの開始位置, 終了位置を秒単位で指定します.
          </p>
        </section>
        <section id="section-audio-buffer-source-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>AudioBufferSourceNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>AudioBufferSourceNode</code> インスタンスは再度,
            発音 (停止) することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合,
            <code>AudioBufferSourceNode</code> インスタンスを再生成して, 再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code
              >stop</code>
            メソッド) を実行する必要があります. この仕様は, <code>OscillatorNode</code> とまったく同じです (ただし,
            <code>AudioBuffer</code> インスタンスは使い回すことが可能です).
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 再生・停止することを期待していますが, 2 回目のクリック以降は, 再生されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const source = new AudioBufferSourceNode(context);

// AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Start immediately
  // But, cannot start since the second times ...
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            期待する動作, つまり, 再生・停止を繰り返すには, 一度 <code>start</code>・<code>stop</code> した (あるいは, <code>duration</code> まで再生した)
            <code>AudioBufferSourceNode</code> インスタンスは破棄して, 再度 <code>AudioBufferSourceNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let source = null;
let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  source = new AudioBufferSourceNode(context, { buffer });

  // AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);

  // Start immediately
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (source === null)) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            ワンショットオーディオも, 複数の <code>AudioBufferSourceNode</code> インスタンスを <code>AudioDestinationNode</code> に接続することで合成が可能です
            (そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています).
          </p>
          <p>
            また, 3 つの <code>AudioBufferSourceNode</code> インスタンスで, それぞれ <code>detune</code> プロパティの値を調整して, C
            メジャーコードを再生しています.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let sourceC = null;
let sourceE = null;
let sourceG = null;

let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  sourceC = new AudioBufferSourceNode(context, { buffer });
  sourceE = new AudioBufferSourceNode(context, { buffer });
  sourceG = new AudioBufferSourceNode(context, { buffer });

  sourceC.detune.value = 0;
  sourceE.detune.value = 400;
  sourceG.detune.value = 700;

  const gain = new GainNode(context, { gain: 0.25 });

  // AudioBufferSourceNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  sourceC.connect(gain);
  sourceE.connect(gain);
  sourceG.connect(gain);

  gain.connect(context.destination);

  // Start immediately
  sourceC.start(0);
  sourceE.start(0);
  sourceG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (sourceC === null) || (sourceE === null) || (sourceG === null)) {
    return;
  }

  // Stop immediately
  sourceC.stop(0);
  sourceE.stop(0);
  sourceG.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            <code>AudioBufferSourceNode</code> でも, <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は,
            さまざまなプラットフォームのオーディオ API のなかでも Web Audio API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web
            ではないプラットフォームのオーディオ API はここまで抽象化されている API すら少ないと思います).
          </p>
        </section>
      </section>
      <section id="section-media-element-audio-source-node">
        <h2>MediaElementAudioSourceNode</h2>
        <p>
          Web Audio API において, 楽曲データに対してなんらかのオーディオ信号処理を適用したい場合に利用するのが
          <b><code>MediaElementAudioSourceNode</code></b> です. 具体的には, <code>HTMLMediaElement</code> (<code>HTMLAudioElement</code> や
          <code>HTMLVideoElement</code>) のオーディオデータに対するオーディオ信号処理を適用する場合に利用します.
        </p>
        <img src="images/media-element-audio-source-node.png" alt="MediaElementAudioSourceNode" width="1232" height="770" loading="lazy" />
        <p>
          <code>HTMLMediaElement</code> を音源にするので, <code>MediaElementAudioSourceNode</code> コンストラクタの第 2 引数 (<b
            ><code>MediaElementAudioSourceOptions</code></b>
          型) の <b><code>mediaElement</code></b> プロパティ (もしくは, ファクトリメソッドの <code>createMediaElementSource</code> の引数) に,
          <code>HTMLMediaElement</code> を指定します.
        </p>
        <p>
          また, コンストラクタやファクトリメソッドに指定する <code>HTMLMediaElement</code> が HTML パース時点で,
          <code>src</code> 属性に指定しているメディアファイルが同一オリジンでない場合, クロスオリジン制限にかかってしまうので,
          <b><code>crossorigin</code> 属性に <code>&apos;anonymous&apos;</code> を設定</b>しておく必要あります. この属性と値の設定によって,
          <b>オリジン間リソース共有</b> (<b>CORS</b>: <b>Cross-Origin Resources Sharing</b>) が可能となります (<b
            ><code>HTMLMediaElement</code> のみで再生する場合は不要です</b>).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;!-- シューベルト 交響曲 第8番 ロ短調 D759 「未完成」 第1楽章 (余談ですが, X JAPAN の「ART OF LIFE」のモチーフになっている楽曲です) --&gt;
&lt;audio src=&quot;https://korilakkuma.github.io/Web-Music-Documentation/assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&quot; crossorigin="anonymous" controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const audioElement = document.querySelector(&apos;audio&apos;);

const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

// If use `createMediaElementSource`
// const source = context.createMediaElementSource(audioElement);

// MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);</code></pre>
        <p>
          <code>MediaElementAudioSourceNode</code> インスタンス生成には 2 点注意すべき点があります. 上記のサンプルコードのように,
          <code>HTMLMediaElement</code> に HTML パース時点で, <code>src</code> 属性にメディアファイルが指定されている場合は, 特に問題ありませんが,
          インタラクティブに, 例えば, ユーザーのファイルシステムからメディアファイルを選択するような場合,
          <b><code>HTMLMediaElement</code> の <code>loadstart</code> イベント発火以降にインスタンスを生成する必要があります</b> (逆に, HTML パース時点で
          <code>src</code> 属性にメディアファイルを指定している場合, <code>loadstart</code> イベントは発火しないので注意が必要です).
          <code>loadstart</code> イベント以降に発火するイベントであればよいので, <code>canplaythrough</code> イベントハンドラなどで
          <code>MediaElementAudioSourceNode</code> インスタンスを生成してもよいでしょう.
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <p>
          もう 1 点は, 1 つの <code>HTMLMediaElement</code> に対して 1 つの <code>MediaElementAudioSourceNode</code> インスタンスが対応しているという点です.
          例えば, <code>HTMLMediaElement</code> の <code>src</code> 属性のみを変更する場合,
          <code>MediaElementAudioSourceNode</code> インスタンスを再度生成するとエラーが発生します (逆に, 別のオブジェクトとなる
          <code>HTMLMediaElement</code> を指定する場合, <code>MediaElementAudioSourceNode</code> インスタンスを生成する必要があります).
        </p>
        <p>
          したがって, 先ほどのサンプルコードだと, 2 回以上, ファイルを選択してしまうと, 同じ <code>HTMLAudioElement</code> に対して, 複数回
          <code>MediaElementAudioSourceNode</code> インスタンスが生成されてエラーが発生してしまうので, 以下のように変更します.
        </p>
        <p>
          また, <code>File API</code> から選択した楽曲データを, <code>HTMLMediaElement</code> の <code>src</code> 属性に指定する場合, Object URL を利用します
          (<code>FileReader API</code> を使って Data URL を利用しても可能ですが, 実装が増えるだけなので, なんらかの理由がなければ
          <code>createObjectURL</code> を利用して Object URL を設定するのがよいでしょう).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

let source = null;

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <section id="section-media-element-audio-source-node-start-and-stop">
          <h3>再生と停止</h3>
          <p>
            <code>MediaElementAudioSourceNode</code> に楽曲データを再生・停止するためのメソッドはありません. 再生や一時停止は, コンストラクタの引数に指定した
            <code>HTMLMediaElement</code> の <code>play</code> / <code>pause</code> メソッドを実行します. したがって, <code>OscillatorNode</code> や
            <code>AudioBufferSourceNode</code> のように使い捨てのノードではない, つまり, インスタンスを再度生成して
            <code>AudioDestinationNode</code> に再度接続する必要もないので, この点は直感的な仕様と言えます.
          </p>
          <p>
            あとは, <code>AudioDestinationNode</code> に接続すれば, 再生・停止することは簡単ですが, これでは
            <code>HTMLMediaElement</code> をそのまま利用するほうが合理的なので, 簡易例として, オーディオ信号処理を適用していることがわかるように,
            <code>BiquadFilterNode</code> を利用して Low-Pass Filter (低域通過フィルタ) を使ったサンプルコードです. カットオフ周波数を変更すると,
            音の輪郭が変わることを確認してみてください (<code>BiquadFilterNode</code> に関しては,
            <a href="#section-effectors-filter-biquad-filter-node">フィルタのセクション</a>で詳細を解説します).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

const inputCutoffElement = document.getElementById(&apos;range-cutoff&apos;);
const spanElement        = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = `${lowpass.frequency.value} Hz`;
});

// UI (by `controls` attribute) plays and pauses media
audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});</code></pre>
        </section>
        <section id="section-html-media-element-and-media-element-audio-source-node">
          <h3>HTMLMediaElement と MediaElementAudioSourceNode</h3>
          <p>
            すでにサンプルコードを実行して, お気づきになったかもしれませんが,
            <code>HTMLMediaElement</code> のプロパティやイベントハンドラはすべて利用することが可能です. <code>volume</code> や <code>muted</code>,
            <code>playbackRate</code> は再生する楽曲データそのものに影響します. <code>autoplay</code> や <code>loop</code> は再生における UX に影響します. また,
            実際のプロダクトでは, <code>loadedmetadata</code> イベント, <code>canplaythrough</code>イベント, <code>timeupdate</code> イベント,
            <code>ended</code> イベントなどで, UI を更新するイベントハンドラを実行することも多いでしょう. このドキュメントですべてを解説することはできないので,
            <a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement</a>
            の仕様などを参考にしてください.
          </p>
          <p>
            よくある実装として, <code>loadedmetadata</code> イベントで <code>duration</code> プロパティ (トータルの再生時間秒数) を取得,
            <code>timeupdate</code> イベントで <code>currentTime</code> プロパティ (現在の再生位置) を更新,
            <code>ended</code> イベントで初期表示に戻すというのは Web Audio API に直接関係はありませんが, メディアデータをあつかう Web
            アプリケーションでは必須になるような実装なので理解しておいて損はないでしょう. また, <code>MediaElementAudioSourceNode</code> の解説に着目するために
            <code>HTMLMediaElement</code> の <code>controls</code> 属性での UI で再生・一時停止を実装していましたが,
            再生・停止ボタンも実装したサンプルコードです. コードをご覧になると理解できるかもしれませんが, Web Audio API
            のコードは変更されていないことにも着目してみてください.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;play&lt;/button&gt;
&lt;span id=&quot;print-current-time&quot;&gt;00 : 00&lt;/span&gt; / &lt;span id=&quot;print-duration&quot;&gt;00 : 00&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;audio /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const inputElement  = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement  = document.querySelector(&apos;audio&apos;);

const spanCurrentTimeElement = document.getElementById(&apos;print-current-time&apos;);
const spanDurationElement    = document.getElementById(&apos;print-duration&apos;);
const inputCutoffElement     = document.getElementById(&apos;range-cutoff&apos;);
const spanCutoffElement      = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanCutoffElement.textContent = `${lowpass.frequency.value} Hz`;
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});

audioElement.addEventListener(&apos;loadedmetadata&apos;, () =&gt; {
  spanDurationElement.textContent = `${Math.trunc(audioElement.duration / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.duration) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;timeupdate&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = `${Math.trunc(audioElement.currentTime / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.currentTime) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;ended&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = &apos;00 : 00&apos;;
});

buttonElement.addEventListener(&apos;click&apos;, async () =&gt; {
  if (audioElement.paused) {
    await audioElement.play();

    buttonElement.textContent = &apos;pause&apos;;
  } else {
    audioElement.pause();

    buttonElement.textContent = &apos;play&apos;;
  }
});</code></pre>
        </section>
      </section>
      <section id="section-media-stream-audio-source-node">
        <h2>MediaStreamAudioSourceNode</h2>
        <p>
          Web Audio API において, マイクロフォンやオーディオインターフェースに入力されたサウンドデータに対して,
          なんらかのオーディオ信号処理を適用したい場合に利用するのが <b><code>MediaStreamAudioSourceNode</code></b> です. もっと言ってしまえば,
          <b>WebRTC</b> (<b><code>MediaDevices</code></b> の <b><code>getUserMedia</code></b> メソッドで取得できる <b><code>MediaStream</code></b> インスタンス)
          で取得したサウンドデータに対するオーディオ信号処理を適用する場合に利用します.
        </p>
        <img src="images/media-stream-audio-source-node.png" alt="MediaStreamAudioSourceNode" width="1232" height="770" loading="lazy" />
        <p>
          WebRTC の仕様は Web Audio API と同等かそれ以上に膨大ですが, Web Audio API との関係で言えば, <code>MediaDevices</code> の
          <code>getUserMedia</code> メソッドを理解すれば問題ないでしょう.
        </p>
        <p>
          <code>getUserMedia</code> メソッドの引数には
          <a href="https://www.w3.org/TR/mediacapture-streams/#dom-mediastreamconstraints" target="_blank" rel="noopener noreferrer">MediaStreamConstraints</a>
          を指定します (少なくとも, Web Audio API で利用することを想定するので, <code>audio</code> は <code>true</code> にしておきます). 初回実行時は,
          マイクロフォン (もしくは, 選択したオーディオインターフェース) に対するアクセス許可を求めるダイアログが表示されます. アクセスを許可して問題なければ,
          戻り値の <code>Promise</code> が <code>fulfilled</code> 状態になります. 成功時の <code>Promise</code> のコールバック関数の引数に
          <code>MediaStream</code> インスタンスが渡されるので, そのインスタンスを <code>MediaElementAudioSourceNode</code> コンストラクタ
          (もしくはファクトリメソッドの <code>createMediaStreamSource</code> の引数) に指定します
        </p>
        <p>
          あとは, <code>MediaStreamAudioSourceNode</code> インスタンスを <code>AudioDestinationNode</code> に接続すれば, WebRTC
          からのサウンドデータを出力することが可能です.
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    // MediaStreamAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
    source.connect(context.destination);

  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        <p>
          もちろん, オーディオ信号処理を適用しないのであれば, WebRTC だけを利用するほうが合理的なので, 簡易例として,
          オーディオ信号処理を適用していることがわかるように, <code>BiquadFilterNode</code> を利用して Low-Pass Filter (低域通過フィルタ)
          を使ったサンプルコードです. カットオフ周波数を変更すると, 音の輪郭が変わることを確認してみてください (<code>BiquadFilterNode</code> に関しては,
          別のセクションで詳細を解説します).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        <section id="section-media-stream-audio-source-node-enumerate-devices">
          <h3>デバイス情報の列挙</h3>
          <p>
            <code>MediaStreamAudioSourceNode</code> で, デフォルトの入力オーディオデバイス (多くの場合, 内臓のマイクロフォン)
            を使う場合には特に必要ありませんが, 指定のデバイスを入力オーディオデバイスとしたり, Web Audio API 1.1 以降では,
            <code>AudioContext</code> インスタンスの <b><code>setSinkId</code></b> メソッドで出力オーディオデバイスを指定できたりします.
          </p>
          <p>
            そのために, 利用可能なデバイス情報を取得する必要があります. <b><code>MediaDeviceInfo</code></b> はデバイスの情報を定義するクラス (<code>kind</code>
            プロパティでデバイスの種類 (<code>MediaDeviceKind</code> 列挙型) や, <code>label</code> プロパティでデバイス名,
            <code>deviceId</code> プロパティでデバイスの識別子などが定義されています) で, <code>MediaDevices</code> の
            <b><code>enumerateDevices</code></b> メソッドを呼び出すと, 成功時の <code>Promise</code> のコールバック関数の引数に,
            <b><code>MediaDeviceInfo</code></b> インスタンスの配列が渡されます. この配列にアクセスすることで,
            利用可能な入出力オーディオデバイスや映像デバイスの情報を取得できます.
          </p>
          <p>
            <b><code>MediaDeviceKind</code></b> 列挙型は, <b><code>&apos;audioinput&apos;</code></b>, <b><code>&apos;audiooutput&apos;</code></b>, <b><code>&apos;videoinput&apos;</code></b> のいずれかの文字列になります. したがって, 例えば, 入力オーディオデバイスの情報のみ必要であれば,
            <code>MediaDeviceInfo</code> の <code>kind</code> プロパティが, <code>audioinput</code> のインスタンスだけフィルタリングすればよいでしょう.
          </p>
          <p>
            注意点として, <code>MediaDevices</code> の <code>enumerateDevices</code> メソッドで利用可能なデバイスを取得するためには,
            デバイスへのアクセスを許可している必要があるので, 先に <code>getUserMedia</code> メソッドを実行しておきます.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;select id=&quot;select-input-devices&quot;&gt;&lt;/select&gt;
&lt;select id=&quot;select-output-devices&quot;&gt;&lt;/select&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then(() =&gt; {
    navigator.mediaDevices.enumerateDevices()
      .then((deviceInfos) =&gt; {
        // `deviceInfos` is `MediaDeviceInfo` list (`MediaDeviceInfo[]`)

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as input audio devices
        const inputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audioinput&apos;;
        });

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as output audio devices
        const outputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audiooutput&apos;;
        });

        const fragmentInputDevices  = document.createDocumentFragment();
        const fragmentOutputDevices = document.createDocumentFragment();

        inputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentInputDevices.appendChild(optionElement);
        });

        outputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentOutputDevices.appendChild(optionElement);
        });

        const selectInputDevicesElement  = document.getElementById(&apos;select-input-devices&apos;);
        const selectOutputDevicesElement = document.getElementById(&apos;select-output-devices&apos;);

        selectInputDevicesElement.appendChild(fragmentInputDevices);
        selectOutputDevicesElement.appendChild(fragmentOutputDevices);
      })
      .catch((error) =&gt; {
        // error handling
      });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-media-stream-audio-source-node-designate-input-audio-device">
          <h3>入力オーディオデバイスの指定</h3>
          <p>
            入力オーディオデバイスを指定するには, <code>MediaDevices</code> の <code>enumerateDevices</code> メソッドで取得した,
            <code>MediaDeviceInfo</code> インスタンスのうち, <code>kind</code> プロパティが <code>&apos;audioinput&apos;</code> のインスタンスの,
            <b
              ><code>deviceId</code> プロパティを <code>MediaStreamConstraints</code> の <code>audio</code> プロパティの
              <code>deviceId</code> プロパティに指定します</b>. そして, <code>MediaStreamConstraints</code> が更新されるので, 再度, <code>MediaDeviceInfo</code> の
            <code>getUserMedia</code> メソッドを呼び出すことで, 指定した入力オーディオデバイスの <code>MediaStream</code> インスタンスを取得することができます.
            1 つ注意点としては, オーディオデバイスを切り替えるたびに, 次のセクションで解説する<b>デバイスの破棄を実行する必要があります</b> (これを実行しないと,
            デバイスが接続されたままになるので, 多くの場合, それは意図する動作ではないと考えられるので).
          </p>
          <img src="images/media-device-info.png" alt="MediaDeviceInfo" width="1232" height="770" loading="lazy" />
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;select id=&quot;select-input-devices&quot;&gt;&lt;/select&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

let mediaStream = null;

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    mediaStream = stream;

    const source = new MediaStreamAudioSourceNode(context, { mediaStream });

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);

    navigator.mediaDevices.enumerateDevices()
      .then((deviceInfos) =&gt; {
        // `deviceInfos` is `MediaDeviceInfo` list (`MediaDeviceInfo[]`)

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as audio inputs
        const inputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audioinput&apos;;
        });

        const fragmentInputDevices = document.createDocumentFragment();

        inputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentInputDevices.appendChild(optionElement);
        });

        const selectInputDevicesElement = document.getElementById(&apos;select-input-devices&apos;);

        selectInputDevicesElement.appendChild(fragmentInputDevices);

        selectInputDevicesElement.addEventListener(&apos;change&apos;, async (event) =&gt; {
          // Stop previous selected input audio device
          const audioTracks = mediaStream.getAudioTracks();

          for (const audioTrack of audioTracks) {
            audioTrack.stop();
          }

          const deviceId = event.currentTarget.value;

          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: { deviceId } });

          const source = new MediaStreamAudioSourceNode(context, { mediaStream });

          // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
          source.connect(lowpass);
          lowpass.connect(context.destination);
        });
      })
      .catch((error) =&gt; {
        // error handling
      });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-media-stream-audio-source-node-media-stream-track-stop">
          <h3>デバイスの破棄</h3>
          <p>
            <code>MediaStreamAudioSourceNode</code> には, オーディオデバイスがらの入力を停止するためのメソッドはありません. デバイスを破棄するには,
            <code>MediaStream</code> インスタンスの <b><code>getAudioTracks</code></b> メソッドで, オーディオデバイスの実体である
            <b><code>MediaStreamTrack</code></b> インスタンスの配列を取得します. <code>MediaStreamTrack</code> には, デバイスを破棄するための
            <b><code>stop</code></b> メソッドが実装されているので, 対象の <code>MediaStreamTrack</code> インスタンスで <code>stop</code> メソッドを実行します
            (同様に, ビデオデバイスを停止するには <b><code>getVideoTracks</code></b> メソッドで <code>MediaStreamTrack</code> インスタンスの配列を取得します).
          </p>
          <img src="images/media-stream.png" alt="MediaStream" width="1232" height="770" loading="lazy" />
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);

    window.setTimeout(() =&gt; {
      const audioTracks = stream.getAudioTracks();

      for (const audioTrack of audioTracks) {
        audioTrack.stop();
      }
    }, 10000);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
      </section>
      <section id="section-audio-worklet">
        <h2>AudioWorklet</h2>
        <p>
          Web Audio API には, 基本波形のサウンド生成やエフェクター, サウンドの視覚化など高度なサウンド処理をより簡単に実装するために, 様々な
          <code>AudioNode</code> が定義されています. これらの <code>AudioNode</code> があるおかげで, 内部で実行されているオーディオ信号処理の詳細を知らなくても,
          高度なサウンド機能の実装が簡単にできるわけです. 例えば, 正弦波の数式を知らなくても,
          <code>OscillatorNode</code> によって正弦波を生成することができました.
        </p>
        <p>
          Web Audio API が定義する多くの <code>AudioNode</code> は, サウンドデータの実体にアクセスする機能をもちません. なぜなら, <code>AudioNode</code> (と,
          <code>AudioNode</code> がもつ <code>AudioParam</code>) は, サウンド処理を抽象化する, つまり, 抽象度の高い API として定義されているからです.
          しかしながら, その代償として, <code>AudioNode</code> の接続と <code>AudioParam</code> の制御では不可能なオーディオ処理も存在してしまいます. 具体的に,
          現状の仕様では, ノイズ生成, ノイズサプレッサー, ボーカルキャンセラ, ピッチシフターなどは <code>AudioNode</code> の接続と
          <code>AudioParam</code> の制御のみでは実装できないので, 直接サウンドデータにアクセスできる必要があります.
        </p>
        <p>
          直接サウンドデータにアクセスすることを可能にするのが, (広義の) <b><code>AudioWorklet</code></b> です (狭義には
          <code>AudioWorklet</code> クラスを意味するので). <code>AudioWorklet</code> は複数の API で構成されており, メインスレッドで
          <code>AudioNode</code> を継承する <b><code>AudioWorkletNode</code></b>, オーディオスレッド (<b><code>AudioWorkletGlobalScope</code></b>) で直接サウンドデータにアクセスすることを可能にする <b><code>AudioWorkletProcessor</code></b>, メインスレッドからオーディオスレッドのファイルをロードする <code>AudioContext</code> インスタンスがもつ
          <b><code>AudioWorklet</code></b> インスタンスです.
        </p>
        <section id="section-audio-worklet-node">
          <h3>AudioWorkletNode</h3>
          <p>
            メインスレッドで定義されていて, <code>AudioNode</code> クラスを継承しています. <code>AudioWorkletNode</code> を
            <code>AudioNode</code> に接続することで, <code>AudioWorkletProcessor</code> (の <code>process</code> メソッド)
            で実装したオーディオ信号処理が適用されて, 次に接続している <code>AudioNode</code> への入力として出力されます.また, <code>AudioNode</code> を
            <code>AudioWorkletNode</code> に接続することで, <code>AudioWorkletProcessor</code> に入力サウンドデータとして渡して,
            オーディオ信号処理を適用することも可能です.
          </p>
          <p>
            <code>AudioWorkletNode</code> コンストラクタの第 1 引数には, <code>AudioContext</code> インスタンスを指定し, 第 2 引数には,
            <code>AudioWorkletGlobalScope</code> (オーディオスレッド) で <code>registerProcessor</code> メソッドで指定した文字列を指定します.
            <code>AudioWorkletNode</code> インスタンスを生成するのは, <code>AudioWorklet</code> インスタンスの
            <b><code>addModule</code> メソッド成功後に実行する必要があります</b> (また, 後発な API であるので,
            ファクトリメソッドによるインスタンス生成も仕様定義されていないことに注意してください).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/processor.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/processor.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-audio-worklet-global-scope">
          <h3>AudioWorkletGlobalScope</h3>
          <p>
            AudioWorklet はその API の仕様設計上, メインスレッドとは別のオーディオスレッドを専用に生成することになります.
            このオーディオスレッドのグローバルスコープが <b><code>AudioWorkletGlobalScope</code></b> です. つまり, メインスレッドにおける
            <code>Window</code> に相当します. メインスレッドとは別の世界なので, 直接 DOM にはアクセスできなかったり,
            メインスレッドで使えるようなクライアントサイド JavaScript API が利用できなかったりします. <code>AudioWorkletGlobalScope</code> には,
            <code>sampleRate</code> プロパティや <code>currentTime</code> プロパティが定義されていますが, これはメインスレッドの
            <code>AudioContext</code> インスタンスと同値です.
          </p>
          <section id="section-audio-worklet-global-scope-register-processor">
            <h4>registerProcessor メソッド</h4>
            <p>
              <code>AudioWorkletGlobalScope</code> で定義されている, 最も重要なメソッドが <b>registerProcessor</b> メソッドです. メインスレッドの
              <code>AudioWorkletNode</code> と, オーディオスレッドの <code>AudioWorkletProcessor</code> を継承したクラスを関連づける役割をもっているからです. 第
              1 引数に, <code>AudioWorkletNode</code> のコンストラクタに関連づける文字列を, 第 2 引数に,
              <code>AudioWorkletProcessor</code> を継承したクラスを指定します (インスタンスではないので注意してください).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    // TODO: Audio Signal Processing

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
          </section>
        </section>
        <section id="section-audio-worklet-processor">
          <h3>AudioWorkletProcessor</h3>
          <section id="section-audio-worklet-processor-subclass">
            <h4>AudioWorkletProcessor の継承クラス</h4>
            <p>
              <code>AudioWorklet</code> を構成する API で, 実際にオーディオ信号処理を実行するのが,
              <b><code>AudioWorkletProcessor</code></b> クラスを継承したサブクラスです (<code>AudioWorkletProcessor</code> を継承したサブクラスを).
              <code>AudioWorkletProcessor</code> クラスで最も重要な API が <b><code>process</code></b> メソッドです.
              <code>AudioWorkletProcessor</code> を継承するサブクラスは <b><code>process</code> メソッドを必ずオーバライドする必要があります</b>.
            </p>
            <p>
              また, 実用的なことを考慮すると, <b><code>MessagePort</code></b> インスタンスである<b><code>port</code></b> プロパティも事実上, 必須と言えます.
              <code>AudioWorkletGlobalScope</code> に定義されている <code>AudioWorkletProcessor</code> (を継承したサブクラス) は, メインスレッドで設定された値
              (例えば, <code>input[type=&quot;range&quot;]</code> で設定された値) を直接的に取得することができません. そこで,
              <code>MessagePort</code> インススタンスの <b><code>messssage</code></b> イベントハンドラや <b><code>postMessage</code></b> メソッドを利用して,
              いわゆる <b>メッセージパッシング</b> (<b>Message Passing</b>) でメインスレッドとデータを送受信する必要があります.
            </p>
            <section id="section-audio-worklet-processor-process">
              <h5>process メソッド (AudioWorkletProcessCallback)</h5>
              <p>
                <code>process</code> メソッドの 第 1 引数には入力サウンドデータとなる <code>Float32Array</code>, 第 2 引数には出力サウンドデータとなる
                <code>Float32Array</code>, 第 3 引数には, 独自に <code>AudioParam</code> を定義する場合にパラメータとなる
                <code>Float32Array</code> がそれぞれ渡されます. これらの <code>Float32Array</code> のサイズは, すべて <b><code>128</code></b> (サンプル) です
                (しかしながら,
                <a href="#section-audio-worklet-processor-render-quantum-size"
                  ><b>Web Audio API 1.1 以降では必ずしも <code>128</code> サンプルではなくなる可能性があります</b></a>). また, 第 1 引数と第 2 引数 (入力サウンドデータと出力サウンドデータ) は, 実際には,
                <code>FrozenArray&lt;FrozenArray&lt;Float32Array&gt;&gt;</code> と配列の入れ子になっています (仕様上の定義として
                <code>FrozenArray</code> ですが, 実装上は <code>Array</code> です. 1 つ内側の <code>Array</code> がチャンネルごとの
                <code>Float32Array</code> を格納するためです).
              </p>
              <p>
                <code>AudioWorkletNode</code> に接続している <code>AudioNode</code> がなければ第 1 引数は不要です. よって, 必須となるのは,
                出力サウンドデータである, <code>128</code> サンプルの <code>Float32Array</code> にオーディオ信号処理を適用した値を格納していくことです.
                そのミニマムな実装例として, ホワイトノイズ (白色雑音) を生成する <code>process</code> メソッドのサンプルコードを記載します.
              </p>
              <p>
                ちなみに, 2 つの <code>Array</code> (<code>FrozenArray</code>) の入れ子になっているので, 形式的に, 入力サウンドデータ, 出力サウンドデータともに,
                0 番目の <code>Array&lt;Float32Array&gt;</code> (<code>FrozenArray&lt;Float32Array&gt;</code>) を取得すると理解していただいて問題ないでしょう
                (なぜこのような仕様になっているのかは, オーナーは理解できていません).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = (2 * Math.random()) - 1;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                ちなみに, <code>process</code> メソッドの戻り値は <code>boolean</code> ですが, ここは形式的に,
                <b><code>true</code> を返す</b>と理解していただいて問題ないでしょう.
                <b><code>true</code> を返すことで, <code>128</code> サンプルごとに <code>process</code> メソッドが繰り返し実行されるからです</b>. 1 度でも
                <code>false</code> を返した <code>AudioWorkletProcessor</code> は破棄されるような仕様になっているので, 戻り値を切り替える (<code>true</code> or
                <code>false</code>) ユースケースがほとんどないからです.
              </p>
            </section>
            <article id="section-audio-worklet-processor-render-quantum-size">
              <h5>Web Audio API 1.1 以降における render quantum size</h5>
              <p>
                Web Audio API 1.0 まで render quantum size は <code>128</code> サンプルの固定値でしたが, Web Audio API 1.1 の仕様では,
                <a href="https://www.w3.org/TR/webaudio/#enumdef-audiocontextrendersizecategory" target="_blank" rel="noopener noreferrer"
                  ><b><code>AudioContextRenderSizeCategory</code></b> 列挙型の値</a>
                (<code>AudioContextOptions</code> の <b><code>renderSizeHint</code></b> オプションの値) を,
                <b><code>&apos;hardware&apos;</code></b> に設定した場合, ユーザーの環境によって最適な render quantum size が選択されて, 必ずしも
                <code>128</code> サンプルでなくなる可能性があります. したがって, 実装においても,
                <code>128</code> サンプルをマジックナンバーとして定義するよりは, 上記のコード例のように, チャンネルごとの <code>Float32Array</code> の
                <code>length</code> プロパティから render quantum size を取得するほうが, 今後の仕様に合わせてコードを変更する必要がなくなります.
              </p>
              <p>
                もっとも, <code>AudioContextRenderSizeCategory</code> を <b><code>&apos;default&apos;</code></b> に設定している場合 (デフォルト値も
                <code>&apos;default&apos;</code> なので, <code>renderSizeHint</code> オプションの値を変更しなければ,
                <code>128</code> サンプルと決められているので, 制作する Web Music アプリケーションに応じて, マジックナンバーとして定義しても問題ないでしょう
                (ライブラリやフレームワークのような汎用的なコードの場合は, render quantum size は, チャンネルごとの <code>Float32Array</code> の
                <code>length</code> プロパティから render quantum size を取得するか, <code>AudioContext</code> インスタンスの
                <a href="https://www.w3.org/TR/webaudio/#dom-baseaudiocontext-renderquantumsize" target="_blank" rel="noopener noreferrer"
                  ><b><code>renderQuantumSize</code></b></a>
                (ただし, <b>Web Audio API 1.1 公開日時点で実装されているブラウザはありません</b>) を参照するのが安全と言えます).
              </p>
              <p>このあたりは, 制作するプロダクトの仕様に応じた<b>判断の問題</b>と言えそうです.</p>
            </article>
            <section id="section-audio-worklet-processor-message-port">
              <h5>port プロパティ (MessagePort インスタンス)</h5>
              <p>
                <code>MessagePort</code> の仕様は Web Audio API の仕様とは別にある (Web Audio API に依存している API ではない) のですが,
                実用上必須となるので簡単に解説しておきます (理解されている場合はスキップしてください).
              </p>
              <p>
                メインスレッドから <code>postMessage</code> されたデータを受信するには <code>messssage</code> イベントハンドラの
                <code>MessageEvent</code> イベントオブジェクトにアクセスする必要がありますが, <code>AudioWorkletProcessor</code> (を継承したサブクラス)
                で呼び出されるのは, コンストラクタと <code>process</code> メソッドのみです. イベントハンドラは, 一度設定してしまえばいいので,
                コンストラクタで設定するという実装が定石となります.
              </p>
              <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;select&gt;
  &lt;option value=&quot;whitenoise&quot; selected&gt;White Noise&lt;/option&gt;
  &lt;option value=&quot;pinknoise&quot;&gt;Pink Noise&lt;/option&gt;
  &lt;option value=&quot;browniannoise&quot;&gt;Brownian Noise&lt;/option&gt;
&lt;/select&gt;
</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/processor.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/processor.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.type = &apos;whitenoise&apos;;

    this.b0 = 0;
    this.b1 = 0;
    this.b2 = 0;
    this.b3 = 0;
    this.b4 = 0;
    this.b5 = 0;
    this.b6 = 0;

    this.lastOut = 0;

    this.port.onmessage = (event) =&gt; {
      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    switch (this.type) {
      case &apos;whitenoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            output[channelNumber][n] = (2 * Math.random()) - 1;
          }
        }

        break;
      }

      case &apos;pinknoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            this.b0 = (0.99886 * this.b0) + (white * 0.0555179);
            this.b1 = (0.99332 * this.b1) + (white * 0.0750759);
            this.b2 = (0.96900 * this.b2) + (white * 0.1538520);
            this.b3 = (0.86650 * this.b3) + (white * 0.3104856);
            this.b4 = (0.55000 * this.b4) + (white * 0.5329522);
            this.b5 = (-0.7616 * this.b5) - (white * 0.0168980);

            output[channelNumber][n] = this.b0 + this.b1 + this.b2 + this.b3 + this.b4 + this.b5 + this.b6 + (white * 0.5362);
            output[channelNumber][n] *= 0.11;

            this.b6 = white * 0.115926;
          }
        }

        break;
      }

      case &apos;browniannoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            output[channelNumber][n] = (this.lastOut + (0.02 * white)) / 1.02;

            this.lastOut = output[channelNumber][n];

            output[channelNumber][n] *= 3.5;
          }
        }

        break;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                また, <code>MessagePort</code> は相互に送受信することができるので, オーディオスレッド (<code>AudioWorkletGlobalScope</code>)
                からメインスレッドにデータを送信することも可能です. その場合, <code>AudioWorkletNode</code> の <code>port</code> プロパティが
                <code>MessagePort</code> インスタンスとなるので, 同様に <code>messssage</code> イベントハンドラをメインスレッドで実装すれば,
                <code>MessageEvent</code> イベントオブジェクトから, <code>postMessage</code> されたデータを受信することが可能になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/message.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/message.js&apos;)
  .then(() =&gt; {
    const oscillator = new OscillatorNode(context);
    const processor  = new AudioWorkletNode(context, &apos;MessageProcessor&apos;);

    // OscillatorNode (Input) -&gt; AudioWorkletNode (Bypass) -&gt; AudioDestinationNode (Output)
    oscillator.connect(processor);
    processor.connect(context.destination);

    oscillator.start(0);

    processor.port.onmessage = (event) =&gt; {
      if (event.data) {
        console.log(event.data);
      }
    };
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/message.js&apos;

class MessageProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = input.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      output[channelNumber].set(input[channelNumber]);
    }

    this.port.postMessage({ messaage: &apos;Bypass samples&apos; });

    return true;
  }
}

registerProcessor(&apos;MessageProcessor&apos;, MessageProcessor);</code></pre>
            </section>
            <section id="section-audio-worklet-processor-parameter-descriptors">
              <h5>parameterDescriptors メソッド</h5>
              <p>
                <b><code>parameterDescriptors</code></b> メソッドは, <code>AudioWorkletProcessor</code> で独自の
                <code>AudioParam</code> を実装したい場合に使います. <code>process</code> メソッドや <code>port</code> プロパティのように (事実上)
                必須の実装というわけではないので, ユースケースとして不要であればスキップしてください.
              </p>
              <p>
                <code>parameterDescriptors</code> メソッドは Getter です. <b><code>AudioParamDescriptor</code></b> の配列を返すように実装します.
                <code>AudioParamDescriptor</code> で定義できるプロパティは, <code>name</code>, <code>defaultValue</code>, <code>minValue</code>,
                <code>maxValue</code>, <code>automationRate</code> の 5 つで, このなかで, <b><code>name</code> プロパティのみは必須で定義する必要があります</b>.
                これは, メインスレッドで <b><code>AudioParamMap</code></b> インスタンスである <code>AudioWorkletNode</code> の
                <b><code>parameters</code></b> プロパティから, キーとして対象の <code>AudioParam</code> (<code>AudioParamDescriptor</code>)
                を取得するのに必要となるからです. それ以外のプロパティについてはデフォルト値が設定されています (<code>defaultValue</code> は <code>0</code>,
                <code>minValue</code> と <code>maxValue</code> がそれぞれ, <code>-3.4028235e38</code> と <code>3.4028235e38</code>,
                <code>automationRate</code> は <code>&apos;a-rate&apos;</code> です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/a-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;a-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = (2 * Math.random()) - 1;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                <code>parameterDescriptors</code> メソッドを実装すると, <code>process</code> メソッドの第 3 引数が渡されるようになります.
                <code>name</code> で指定した文字列をプロパティにして, パラメータの <code>Float32Array</code> を取得します. <code>automation</code> が
                <code>&apos;a-rate&apos;</code> の場合, <code>128</code> サンプル (render quantum size) ごとに異なる値が格納されているので,
                インデックスを走査します. <code>&apos;k-rate&apos;</code> の場合, <code>128</code> サンプル (render quantum size) ごとに固定値なので,
                <code>Float32Array</code> のインデックス <code>0</code> の値を利用します.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/a-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;a-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    const gain = parameters.noiseGain;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = ((gain.length &gt; 1) ? gain[n] : gain[0]) * ((2 * Math.random()) - 1);
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p><code>automationRate</code> が &apos;k-rate&apos; の場合,</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/k-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;k-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    const gain = parameters.noiseGain;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = gain[0] * ((2 * Math.random()) - 1);
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                メインスレッドで, 定義した <code>AudioParamDescriptor</code> を <code>AudioParam</code> インスタンスとして取得するには,
                <code>parameters</code> プロパティで <code>AudioParamMap</code> を取得して, <code>name</code> プロパティをキーにして取得します. これは
                <code>AudioParam</code> インスタンスなので,
                <a href="#section-audio-param-scheduling">オートメーションメソッド</a>を利用することでパラメータをスケジューリングすることが可能になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/a-rate.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/a-rate.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    const audioParamMap = processor.parameters;
    const noiseGain = audioParamMap.get(&apos;noiseGain&apos;);

    // do something for scheduling parameter ...
    const t0 = context.currentTime;
    const t1 = t0 + 2.5;

    noiseGain.setValueAtTime(0, t0);
    noiseGain.linearRampToValueAtTime(1, t1);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            </section>
          </section>
        </section>
        <section id="section-audio-worklet-add-module">
          <h3>AudioWorklet</h3>
          <p>
            すでに, サンプルコードでは利用していますが, <code>AudioContext</code> には <b><code>audioWorklet</code></b> プロパティが定義されており, これは
            (狭義の) <b><code>AudioWorklet</code></b> インスタンスです. 責務としては, <b><code>addModule</code></b> メソッドを呼び出して,
            <code>AudioWorkletProcessor</code> のサブクラスを定義したスクリプト (Worklet ファイル) をロードすることです.
          </p>
          <p>
            <code>addModule</code> メソッドは, Web Audio API に依存した仕様ではなく, JavaScript で使える Worklet (例えば, <code>CSS.paintWorklet</code>) が,
            指定したスクリプト (Worklet ファイル) をロードするために定義されています. <code>addModule</code> メソッドの第 1 引数は スクリプト (Worklet ファイル)
            の URL 文字列で必須です. また, 第 2 引数は任意で, <code>Fetch API</code> の
            <a href="https://fetch.spec.whatwg.org/#requests" target="_blank" rel="noopener noreferrer">Request</a> の
            <a href="https://fetch.spec.whatwg.org/#concept-request-credentials-mode" target="_blank" rel="noopener noreferrer">credentials mode</a>
            のオブジェクトを指定できます. 戻り値は <code>Promise</code> です. 成功時のコールバック関数の引数はありません.
          </p>
        </section>
        <section id="section-audio-worklet-examples">
          <h3>AudioWorklet によるオーディオ信号処理</h3>
          <p>
            <code>AudioWorklet</code> は複数の API で構成されているので, 抽象的な解説だけだと理解が難しいかもしれません. このセクションでは,
            <code>AudioWorklet</code> のユースケースとして想定されるオーディオ信号処理の実装例を紹介して理解のサポートになることを目指します.
          </p>
          <section id="section-audio-worklet-examples-noise-generator">
            <h4>ノイズ生成</h4>
            <p>
              すでに, 解説のサンプルコードとして記載していますが, Web Audio API でホワイトノイズやピンクノイズを生成する場合,
              <code>AudioWorklet</code> を利用する必要があります.
            </p>
            <p>
              ノイズの生成は, 乱数生成がベースになっています. 特に, ホワイトノイズは乱数そのままであり (振幅の調整だけしている),
              その振幅スペクトルはすべての周波数成分を一様に含んでいます (ノイズ生成の詳細に関しては,
              <a href="https://noisehack.com/generate-noise-web-audio-api/" target="_blank" rel="noopener noreferrer"
                >How to Generate Noise with the Web Audio API</a>
              を参考にしてください).
            </p>
            <p>
              実際に, Web アプリケーションとして実装する場合, ユーザーインタラクティブな操作によって, 発音・停止をさせるケースが多くなるでしょう. そのために,
              <code>AudioWorkletProcessor</code> を継承したサブクラスで, 発音・停止のフラグを実装しておき, メインスレッドからの
              <code>postMessage</code> で切り替えるようにします. すでに解説しましたが, <code>process</code> メソッドで, <code>false</code> を返してしまうと,
              その <code>AudioWorkletProcessor</code> は破棄されるような仕様になっているので, 常に <code>true</code> を返し, 停止中の場合, 出力となる
              <code>Float32Array</code> にデータを格納しないという実装で停止を実装しています.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;select&gt;
  &lt;option value=&quot;whitenoise&quot; selected&gt;White Noise&lt;/option&gt;
  &lt;option value=&quot;pinknoise&quot;&gt;Pink Noise&lt;/option&gt;
  &lt;option value=&quot;browniannoise&quot;&gt;Brownian Noise&lt;/option&gt;
&lt;/select&gt;
</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/noise-generator.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/noise-generator.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mousedown&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: true });

      event.currentTarget.textContent = &apos;stop&apos;;
    });

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mouseup&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: false });

      event.currentTarget.textContent = &apos;start&apos;;
    });

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/noise-generator.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.processing = false;

    this.type = &apos;whitenoise&apos;;

    this.b0 = 0;
    this.b1 = 0;
    this.b2 = 0;
    this.b3 = 0;
    this.b4 = 0;
    this.b5 = 0;
    this.b6 = 0;

    this.lastOut = 0;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.processing === &apos;boolean&apos;) {
        this.processing = event.data.processing;
      }

      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    if (!this.processing) {
      return true;
    }

    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    switch (this.type) {
      case &apos;whitenoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            output[channelNumber][n] = (2 * Math.random()) - 1;
          }
        }

        break;
      }

      case &apos;pinknoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            this.b0 = (0.99886 * this.b0) + (white * 0.0555179);
            this.b1 = (0.99332 * this.b1) + (white * 0.0750759);
            this.b2 = (0.96900 * this.b2) + (white * 0.1538520);
            this.b3 = (0.86650 * this.b3) + (white * 0.3104856);
            this.b4 = (0.55000 * this.b4) + (white * 0.5329522);
            this.b5 = (-0.7616 * this.b5) - (white * 0.0168980);

            output[channelNumber][n] = this.b0 + this.b1 + this.b2 + this.b3 + this.b4 + this.b5 + this.b6 + (white * 0.5362);
            output[channelNumber][n] *= 0.11;

            this.b6 = white * 0.115926;
          }
        }

        break;
      }

      case &apos;browniannoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            output[channelNumber][n] = (this.lastOut + (0.02 * white)) / 1.02;

            this.lastOut = output[channelNumber][n];

            output[channelNumber][n] *= 3.5;
          }
        }

        break;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-oscillator">
            <h4>基本波形生成</h4>
            <p>
              基本波形の生成は, <code>OscillatorNode</code> を利用すれば可能ですが, <code>OscillatorNode</code> による波形生成は, いわゆる,
              <b>フーリエ級数展開</b>にもとづいた波形生成, つまり, 周波数の異なる sin 波の合成によって, 矩形波やノコギリ波, 三角波が生成されています
              (その証拠として, <a href="#svg-oscillator">波形</a>を確認すると <b>Gibbs の現象</b>が発生していることが確認できます).
            </p>
            <p>実は, 基本波形は, 直線を組み合わせることによって (近似した波形を) 生成することも可能です (その場合, Gibbs の現象は発生しなくなります).</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;select&gt;
  &lt;option value=&quot;sine&quot; selected&gt;Sine&lt;/option&gt;
  &lt;option value=&quot;square&quot;&gt;Square&lt;/option&gt;
  &lt;option value=&quot;sawtooth&quot;&gt;Sawtooth&lt;/option&gt;
  &lt;option value=&quot;triangle&quot;&gt;Triangle&lt;/option&gt;
&lt;/select&gt;
&lt;label for=&quot;range-frequency&quot;&gt;frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-frequency&quot; value=&quot;440&quot; min=&quot;20&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-frequency-value&quot;&gt;440 Hz&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/oscillator.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/oscillator.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;OscillatorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mousedown&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: true });

      event.currentTarget.textContent = &apos;stop&apos;;
    });

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mouseup&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: false });

      event.currentTarget.textContent = &apos;start&apos;;
    });

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });

    document.querySelector(&apos;input[type=&quot;range&quot;]&apos;).addEventListener(&apos;input&apos;, (event) =&gt; {
      processor.port.postMessage({ frequency: event.currentTarget.valueAsNumber });

      document.getElementById(&apos;print-frequency-value&apos;).textContent = `${event.currentTarget.value} Hz`;
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/oscillator.js&apos;

class OscillatorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.processing = false;

    this.numberOfProcessedSamples = 0;

    this.type      = &apos;sine&apos;;
    this.frequency = 440;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.processing === &apos;boolean&apos;) {
        this.processing = event.data.processing;
      }

      if ((typeof event.data.frequency === &apos;number&apos;) &amp;&amp; (event.data.frequency &gt; 0)) {
        this.frequency = event.data.frequency;
      }

      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    if (!this.processing) {
      return true;
    }

    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    const numberOfSamplesPer1Hz = sampleRate / this.frequency;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        switch (this.type) {
          case &apos;sine&apos;: {
            output[channelNumber][n] = Math.sin((2 * Math.PI * this.frequency * this.numberOfProcessedSamples) / sampleRate);
            break;
          }

          case &apos;square&apos;: {
            output[channelNumber][n] = (this.numberOfProcessedSamples &lt; (numberOfSamplesPer1Hz / 2)) ? 1 : -1;
            break;
          }

          case &apos;sawtooth&apos;: {
            const a = (2 * this.numberOfProcessedSamples) / numberOfSamplesPer1Hz;

            output[channelNumber][n] = a - 1;
            break;
          }

          case &apos;triangle&apos;: {
            const a = (4 * this.numberOfProcessedSamples) / numberOfSamplesPer1Hz;

            output[channelNumber][n] = (this.numberOfProcessedSamples &lt; (numberOfSamplesPer1Hz / 2)) ? (-1 + a) : (3 - a);
            break;
          }
        }

        if (++this.numberOfProcessedSamples &gt;= numberOfSamplesPer1Hz) {
          this.numberOfProcessedSamples = 0;
        }
      }
    }

    return true;
  }
}

registerProcessor(&apos;OscillatorProcessor&apos;, OscillatorProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-reverse-channel">
            <h4>リバースチャンネル</h4>
            <p>
              左チャンネルからのサウンド出力と右チャンネルからのサウンド出力を反転する単純なエフェクターです (ただし, その原理から,
              左右チャンネルのサウンドデータが異なる場合にしか効果がありません). 一般的には, オーディオデータに対して適用するので, オーディオデータの準備として
              <code>AudioBufferSourceNode</code> か <code>MediaElementAudioSourceNode</code> を入力ノードとして,
              <code>AudioWorkletNode</code> に接続しておきます.
            </p>
            <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;div&gt;
  &lt;input type=&quot;file&quot; /&gt;
  &lt;label&gt;Reverse &lt;input type=&quot;checkbox&quot; /&gt;&lt;/label&gt;
&lt;/div&gt;
&lt;audio controls /&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/channel-reverser.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/channel-reverser.js&apos;)
  .then(() =&gt; {
    const reverser = new AudioWorkletNode(context, &apos;ChannelReverserProcessor&apos;);

    const inputElement    = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
    const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);
    const audioElement    = document.querySelector(&apos;audio&apos;);

    let source = null;

    inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
      const file = event.currentTarget.files[0];

      audioElement.src = window.URL.createObjectURL(file);
    });

    checkboxElement.addEventListener(&apos;click&apos;, (event) =&gt; {
      reverser.port.postMessage({ reversing: event.currentTarget.checked });
    });

    audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
      if (source === null) {
        source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
      }

      // MediaElementAudioSourceNode (Input) -&gt; AudioWorkletNode (Channel Reverser) -&gt; AudioDestinationNode (Output)
      source.connect(reverser);
      reverser.connect(context.destination);
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/channel-reverser.js&apos;

class ChannelReverserProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.reversing = false;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.reversing === &apos;boolean&apos;) {
        this.reversing = event.data.reversing;
      }
    };
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    if ((input.length === 0) || (output.length === 0)) {
      return true;
    }

    if ((input.length !== 2) || (output.length !== 2)) {
      output[0].set(input[0]);
      return true;
    }

    if (this.reversing) {
      output[0].set(input[1]);
      output[1].set(input[0]);
    } else {
      output[0].set(input[0]);
      output[1].set(input[1]);
    }

    return true;
  }
}

registerProcessor(&apos;ChannelReverserProcessor&apos;, ChannelReverserProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-vocal-canceler">
            <h4>ボーカルキャンセラ</h4>
            <p>
              一般的に, 音楽のオーディオデータはステレオで, 臨場感あるサウンドになるように, 左チャンネルと右チャンネルの音の大きさを調整しています.
              ボーカルの聴こえる位置は中央になるように左右のチャンネルを同じ音の大きさで録音し, ギターなどのボーカル以外の楽器音は,
              左右どちらかの音の大きさを大きくして, 聴こえる位置が左右のどちらかになるように録音されています. このように録音されたオーディオデータであれば,
              左右のチャンネルのサウンドデータの差分をとることにより, 左右均等に録音されているボーカルの音を取り除くことが可能になります. これが,
              ボーカルキャンセラです (ただし, その原理から, ドラムなど中央に位置する楽器音も取り除かれてしまいます).
            </p>
            <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;div&gt;
  &lt;input type=&quot;file&quot; /&gt;
  &lt;label&gt;Depth &lt;input type=&quot;range&quot; id=&quot;range-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;&lt;label&gt;
&lt;/div&gt;
&lt;audio controls /&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/vocal-canceler.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/vocal-canceler.js&apos;)
  .then(() =&gt; {
    const canceler = new AudioWorkletNode(context, &apos;VocalCancelerProcessor&apos;);

    const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
    const rangeElement = document.querySelector(&apos;input[type=&quot;range&quot;]&apos;);
    const audioElement = document.querySelector(&apos;audio&apos;);

    let source = null;

    inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
      const file = event.currentTarget.files[0];

      audioElement.src = window.URL.createObjectURL(file);
    });

    rangeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      canceler.port.postMessage({ depth: event.currentTarget.valueAsNumber });
    });

    audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
      if (source === null) {
        source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
      }

      // MediaElementAudioSourceNode (Input) -&gt; AudioWorkletNode (Vocal Canceler) -&gt; AudioDestinationNode (Output)
      source.connect(canceler);
      canceler.connect(context.destination);
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/vocal-canceler.js&apos;

class VocalCancelerProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.depth = 0;

    this.port.onmessage = (event) =&gt; {
      if ((typeof event.data.depth === &apos;number&apos;) &amp;&amp; (event.data.depth &gt;= 0)) {
        this.depth = event.data.depth;
      }
    };
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    if ((input.length === 0) || (output.length === 0)) {
      return true;
    }

    if ((input.length !== 2) || (output.length !== 2)) {
      output[0].set(input[0]);

      return true;
    }

    const bufferSize = input[0].length;

    for (let n = 0; n &lt; bufferSize; n++) {
      output[0][n] = input[0][n] - (this.depth * input[1][n]);
      output[1][n] = input[1][n] - (this.depth * input[0][n]);
    }

    return true;
  }
}

registerProcessor(&apos;VocalCancelerProcessor&apos;, VocalCancelerProcessor);</code></pre>
          </section>
        </section>
        <article id="section-script-processor-node">
          <h3>ScriptProcessorNode</h3>
          <p>
            <code>AudioWorklet</code> は初期の Web Audio API の仕様定義にはなく, <code>ScriptProcessorNode</code> が仕様定義されていました.
            <b>メインスレッド</b>で実行される<b>イベントハンドラ</b>によって, サウンド入出力する以外は <code>AudioWorklet</code> と考え方は同じです. しかし,
            メインスレッドで実行されるので, 不自然な音切れ (いわゆる, プチプチ音) が発生する<b>グリッチ</b> (<b>glitch</b>) や UI
            がスムーズに動作しなくなる現象である<b>ジャンク</b> (<b>jank</b>), イベントハンドラで実行されることによる<b>遅延</b> (<b>latency</b>) など API
            自体の根本的な問題がありました.
          </p>
          <p>
            Web Audio API の歴史的には, <code>ScriptProcessorNode</code> の問題を API 設計から解決するために, 後発的に仕様策定されて実装されているのが
            <code>AudioWorklet</code> です. <code>ScriptProcessorNode</code> は将来的に仕様からも削除される予定なので, 新規に実装するのであればわざわざ
            <code>ScriptProcessorNode</code> を利用する必要はありませんが, 既存のコードを読む必要があるかもしれないので,
            正弦波とホワイトノイズをミックスするサンプルコードを記載します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const processor  = context.createScriptProcessor(0, 2, 2);

oscillator.connect(processor);
processor.connect(context.destination);

oscillator.start(0);

const bufferSize = processor.bufferSize;

processor.onaudioprocess = (event) =&gt; {
  const inputLs  = event.inputBuffer.getChannelData(0);
  const inputRs  = event.inputBuffer.getChannelData(1);
  const outputLs = event.outputBuffer.getChannelData(0);
  const outputRs = event.outputBuffer.getChannelData(1);

  for (let n = 0; n &lt; bufferSize; n++) {
    outputLs[n] = (0.5 * inputLs[n]) + (0.25 * ((2 * Math.random()) - 1));
    outputRs[n] = (0.5 * inputRs[n]) + (0.25 * ((2 * Math.random()) - 1));
  }
};
</code></pre>
        </article>
      </section>
      <section id="section-scheduling">
        <h2>スケジューリング</h2>
        <section id="section-audio-context-current-time">
          <h3>AudioContext の currentTime プロパティ</h3>
          <p>
            Web Audio API におけるスケジューリング (<code>AudioScheduledSourceNode</code> の <code>start</code> / <code>stop</code> メソッドのスケジューリングや
            <code>AudioParam</code> のスケジューリング) において, 基本となる時間が
            <b><code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ</b>です. <code>currentTime</code> プロパティには,
            <b><code>AudioContextState</code> が <code>&apos;running&apos;</code> である状態での経過時間が秒単位で格納されています</b> (参照するだけの
            <code>readonly</code> プロパティです). <code>OscillatorNode</code> や <code>AudioBufferSourceNode</code> を即時に発音・停止するために,
            <code>start</code> / <code>stop</code> メソッドの第 1 引数に <code>0</code> を指定していましたが, 即時に発音・停止するであれば
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティを指定することでも可能です
            (即時に発音・停止するのは頻繁にあることなので, デフォルト値 <code>0</code> で即時に発音・停止するように仕様定義されています).
          </p>
          <p>
            つまり, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティを基準に, 未来の時間を指定すれば (加算すれば),
            スケジューリングが可能になるということです.
          </p>
          <p>
            仕様上の詳細を解説をすると, <code>OscillatorNode</code> や <code>AudioBufferSourceNode</code> は <code>AudioNode</code> クラスを継承した
            <code>AudioScheduledSourceNode</code> クラスを継承しており, <code>start</code> / <code>stop</code> メソッドはこのクラスに定義されているメソッドです
          </p>
          <article id="section-audio-context-current-time-definition">
            <h4>currentTime プロパティの時間の定義</h4>
            <p>
              Web Audio API が仕様策定された初期のころは, Autoplay Policy の制約もなく, <code>AudioContextState</code> 型もなかったので,
              <code>currentTime</code> プロパティには, <code>AudioContext</code> インスタンスが生成されてからの経過時間が秒単位で格納されていました. そのあと
              (2017 年ごろから), モダンブラウザ全般で, Autoplay Policy の導入が始まったことにより, <code>AudioContextState</code> 型も仕様に追加されて,
              <code>currentTime</code> プロパティには, <code>AudioContextState</code> が
              <code>&apos;running&apos;</code> である状態での経過時間が秒単位で格納されるように仕様も変わりました.
            </p>
          </article>
          <article id="section-javascript-time">
            <h4>JavaScript における時間</h4>
            <dl>
              <dt><code>Date.now</code></dt>
              <dd>
                UNIX 時間 (タイムスタンプとも呼ばれます). 1970 年 1 月 1 日 00 : 00 からの経過時間をミリ秒単位で表します. 音楽のような,
                時間管理の精度が高く要求されるようなユースケースでは不向きと言えます.
              </dd>
              <dt><code>performance.now</code></dt>
              <dd>
                <code>DOMHighResTimeStamp</code> 型の時間 (<code>64 bit</code> 浮動小数点数なので, 実体は <code>number</code> 型です) を表現します.
                詳細な仕様は他のドキュメントを参考にするほうがよいですが (実行コンテキストによって計測時刻が異なるので), ざっくり言うと, 対象の Web
                ページにアクセスしてからの経過時間をミリ秒単位で表します. hls.js など動画ストリーミングライブラリでは使われているなど (例
                <a href="https://github.com/video-dev/hls.js/blob/master/src/controller/abr-controller.ts#L179" target="_blank" rel="noopener noreferrer"
                  >ABR: Adaptive BitRate streaming</a>), <code>Date.now</code> と比較すると, 精度の高い時間と言えます.
              </dd>
            </dl>
          </article>
        </section>
        <section id="section-oscillator-node-scheduling">
          <h3>OscillatorNode のスケジューリング</h3>
          <p>
            <code>OscillatorNode</code> のスケジューリングを設定することで, 和音をアルペジオ (分散和音) のように発音・停止するサンプルコードにしてみました.
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティの値を基準に, スケジュールしたい時間を加算した値を引数に渡すことで,
            <code>AudioContext</code> インスタンスが生成されてからの経過時間が指定した時間に達すると, 発音・停止します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let oscillatorC = null;
let oscillatorE = null;
let oscillatorG = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillatorC !== null) || (oscillatorE !== null) || (oscillatorG !== null)) {
    return;
  }

  oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
  oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
  oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

  const gain = new GainNode(context, { gain: 0.25 });

  // OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  oscillatorC.connect(gain);
  oscillatorE.connect(gain);
  oscillatorG.connect(gain);
  gain.connect(context.destination);

  // Schedule oscillator start
  oscillatorC.start(context.currentTime + 0.0);
  oscillatorE.start(context.currentTime + 0.1);
  oscillatorG.start(context.currentTime + 0.2);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillatorC === null) || (oscillatorE === null) || (oscillatorG === null)) {
    return;
  }

  // Schedule oscillator stop
  oscillatorC.stop(context.currentTime + 0.0);
  oscillatorE.stop(context.currentTime + 0.1);
  oscillatorG.stop(context.currentTime + 0.2);

  // GC (Garbage Collection)
  oscillatorC = null;
  oscillatorE = null;
  oscillatorG = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
        </section>
        <section id="section-audio-buffer-source-node-scheduling">
          <h3>AudioBufferSourceNode のスケジューリング</h3>
          <p>
            <code>OscillatorNode</code> の場合と同様に, スケジューリングを設定することで, 和音をアルペジオのように発音・停止するサンプルコードにしてみました.
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティの値を基準に, スケジュールしたい時間を加算した値を引数に渡すことで,
            <code>AudioContext</code> インスタンスが生成されてからの経過時間が指定した時間に達すると, 発音・停止します. 1 点異なるのは,
            <b><code>AudioBufferSourceNode</code> の <code>start</code> メソッドはオーバライドされています</b>. もし,
            オーディオデータの再生開始位置を指定したい場合は, 第 2 引数に再生開始位置を秒単位で指定, 再生時間を指定する場合は, 第 3 引数に秒単位で指定します (<b
              ><a href="#section-audio-buffer-source-node-playback-rate-and-detune">再生速度</a>を変更している場合でも影響は受けない</b>ので, 第 2 引数, 第 3 引数は再生速度を <code>1</code> とした場合の値を指定します). どちらも, 任意の引数なので不要であれば省略可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let sourceC = null;
let sourceE = null;
let sourceG = null;

let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  sourceC = new AudioBufferSourceNode(context, { buffer });
  sourceE = new AudioBufferSourceNode(context, { buffer });
  sourceG = new AudioBufferSourceNode(context, { buffer });

  sourceC.detune.value = 0;
  sourceE.detune.value = 400;
  sourceG.detune.value = 700;

  const gain = new GainNode(context, { gain: 0.25 });

  // AudioBufferSourceNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  sourceC.connect(gain);
  sourceE.connect(gain);
  sourceG.connect(gain);

  gain.connect(context.destination);

  // Schedule one-shot audio start
  sourceC.start((context.currentTime + 0.0), 0, sourceC.buffer.duration);
  sourceE.start((context.currentTime + 0.1), 0, sourceE.buffer.duration);
  sourceG.start((context.currentTime + 0.2), 0, sourceG.buffer.duration);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (sourceC === null) || (sourceE === null) || (sourceG === null)) {
    return;
  }

  // Schedule one-shot audio stop
  sourceC.stop(context.currentTime + 0.0);
  sourceE.stop(context.currentTime + 0.1);
  sourceG.stop(context.currentTime + 0.2);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-audio-param-scheduling">
          <h3>AudioParam のスケジューリング (パラメータのオートメーション)</h3>
          <p>
            <code>AudioParam</code> には, パラメータをスケジュールするための, <b>パラメータのオートメーションメソッド</b>が仕様定義されています.
            このセクションでは, その最適なユースケースと言える, エンペロープジェネレーターを例にそれらを解説します.
          </p>
          <p><code>AudioParam</code> で定義されている, パラメータのオートメーションメソッドを以下のリストに記載します.</p>
          <section id="section-audio-param-set-value-at-time">
            <h4>setValueAtTime(value, startTime)</h4>
            <p><code>startTime</code> にパラメータを <code>value</code> の値に設定する</p>
          </section>
          <section id="section-audio-param-linear-ramp-to-value-at-time">
            <h4>linearRampToValueAtTime(value, endTime)</h4>
            <p><code>endTime</code> にパラメータが <code>value</code> の値になるように線形的に (直線的に) 変化させる</p>
          </section>
          <section id="section-audio-param-exponential-ramp-to-value-at-time">
            <h4>exponentialRampToValueAtTime(value, endTime)</h4>
            <p><code>endTime</code> にパラメータが <code>value</code> の値になるように指数関数的に変化させる</p>
          </section>
          <section id="section-audio-param-set-target-at-time">
            <h4>setTargetAtTime(target, startTime, timeConstant)</h4>
            <p>
              <code>startTime</code> になったら, パラメータを <code>target</code> の値に向けて, <code>timeConstant</code> の時間をかけて変化させる
              (より正確には, パラメータの現在の値と <code>target</code> の値の差分の約 <code>63.2%</code> (<span class="math-inline"
                >$1 - \frac{1}{\mathrm{exp}} = 0.632120 \cdots$</span>) まで変化するのに, <code>timeConstant</code>の時間を要する)
            </p>
          </section>
          <section id="section-audio-param-set-value-curve-at-time">
            <h4>setValueCurveAtTime(values, startTime, duration)</h4>
            <p>
              <code>startTime</code> になったら, <code>Float32Array</code> の <code>values</code> の値にしたがって,
              <code>duration</code> の時間をかけて変化させる
            </p>
          </section>
          <section id="section-audio-param-cancel-scheduled-values">
            <h4>cancelScheduledValues(cancelTime)</h4>
            <p><code>cancelTime</code> 以降のスケジューリングを解除する</p>
          </section>
          <section id="section-audio-param-cancel-and-hold-at-time">
            <h4>cancelAndHoldAtTime(cancelTime)</h4>
            <p><code>cancelTime</code> 以降のスケジューリングを解除する (<code>cancelTime</code> 時点の値を保持する)</p>
          </section>
        </section>
        <section id="section-envelope-generator">
          <h3>エンベロープジェネレーター</h3>
          <section id="section-envelope">
            <h4>エンベロープとは ?</h4>
            <p>
              <b>エンベロープ</b>とは, 波形の概形のことです. テキストによる解説よりは, イラストによる解説が一目瞭然なので, 例として以下のような波形で説明します.
            </p>
            <figure>
              <svg id="svg-figure-career" width="720" height="405" data-parameters="false" data-a="1" data-f="1" />
              <figcaption>時間領域の波形</figcaption>
            </figure>
            <figure>
              <svg id="svg-figure-envelope" width="720" height="405" data-parameters="false" data-a="1" data-f="1" />
              <figcaption>振幅エンベロープ</figcaption>
            </figure>
            <p>
              上記のエンベロープは, 振幅に対する波形の概形なので, <b>振幅エンベロープ</b>と呼びます. 振幅エンベロープを,
              時間的に制御するオーディオ処理を<b>エンベロープジェネレーター</b>と呼びます.
            </p>
            <p>
              エンベロープジェネレーターの実装において, パラメータのスケジュールの対象になるの <code>GainNode</code> インスタンスの
              <code>gain</code> プロパティです. <code>gain</code> プロパティは, <code>AudioParam</code> インスタンスであり,
              <code>AudioParam</code> で仕様定義されているパラメータのオートメーションメソッドを利用することで, パラメータをスケジュールすることが可能です
              (同様に, <code>OscillatorNode</code> インスタンスの <code>frequency</code> プロパティや, <code>DelayNode</code> インスタンスの
              <code>delayTime</code> プロパティ, <code>BiquadFilterNode</code> の <code>frequency</code> プロパティ ... など,
              <code>AudioParam</code> インスタンスであれば利用可能です. したがって, パラメータのオートメーションメソッドを理解することで,
              さまざまなエフェクトが試行錯誤可能になります).
            </p>
          </section>
          <section id="section-envelope-generator-initialization">
            <h4>gain プロパティの初期化</h4>
            <p>
              まず, 初期化処理として, <a href="#section-audio-param-set-value-at-time"><code>setValueAtTime</code></a> メソッドを実行します. 初期値として, 値を
              <code>0</code> に, また即時に初期化するように, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ (<code>t0</code>)
              を指定します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;

  envelopegenerator.gain.setValueAtTime(0, t0);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
})</code></pre>
          </section>
          <section id="section-envelope-generator-attack">
            <h4>attack</h4>
            <p>
              <b>attack</b> (アタック) は, ゲインが最大値, すなわち, <code>1</code> になるまでに要する時間です. そこで, attack の実装には,
              <a href="#section-audio-param-linear-ramp-to-value-at-time"><code>linearRampToValueAtTime</code></a> メソッドを利用します (一般的には,
              線形的に変化させますが, 指数関数的に変化させたい場合,
              <a href="#section-audio-param-exponential-ramp-to-value-at-time"><code>exponentialRampToValueAtTime</code></a>
              メソッドを使ってみてもよいでしょう). 注意が必要なのは, 第 2 引数です. attack time の値をそのまま指定してしまうとうまくいきません. なぜなら,
              <b>時間ではなく時刻</b>を指定する必要があるからです. したがって, サウンド開始時刻 (変数 <code>t0</code>) に attack を加算した値 (変数
              <code>t1</code>) を第 2 引数に指定します.
            </p>
            <p>
              attack は, もう少しくだいて表現すれば, 音の立ち上がりの速さを決定するパラメータと言えます. 楽器で具体例をあげると,
              ピアノやギターは比較的音の立ち上がりが速い楽器で, バイオリンやフルートなどは比較的音の立ち上がりが遅い楽器です. すなわち,
              アタックを短くするとピアノやギターのように音の立ち上がりが速くなり, アタックを長くするとバイオリンやフルートのように音の立ち上がりが遅くなります.
              ちなみに, 音の立ち上がりが比較的速い (アタックが短い) エレキギターでは, バイオリン奏法と呼ばれる奏法があります. これは, ピッキング時に,
              ギターのボリュームを0にすることによって, ピッキングした瞬間の音 (アタック音) を消し, そのあとに, ボリュームを増加させるという奏法です.
              エレキギターであるのに, まるでバイオリンのような音色を奏でることができます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack = 0.01;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});</code></pre>
          </section>
          <section id="section-envelope-generator-decay-and-sustain">
            <h4>decay / sustain</h4>
            <p>
              <b>decay</b> (ディケイ) は, ゲインが最大値 <code>1</code> から <b>sustain</b> (サステイン) にまで減衰する時間です.
              <a href="#section-audio-param-set-target-at-time"><code>setTargetAtTime</code></a> メソッドを利用することで実装できます. 注意が必要なのは, 第 2
              引数と第 3 引数です. 第 2 引数にはパラメータが変化を開始する時刻を指定し, 第 3 引数にはパラメータが, 現在の値と第 1 引数で指定した値の差分 (の約
              <code>63.2%</code>) まで変化するのに要する時間を指定します. したがって, 第 2 引数は <code>gain</code> プロパティが <code>1</code> となる時刻
              (減衰開始時刻) である変数 <code>t1</code> を指定し, 第 3 引数は decay time (減衰時間) である変数 <code>t2</code>を指定します. そして, 第 1 引数は
              <code>gain</code> プロパティが収束する値である sustain level (持続レベル) を指定します.
            </p>
            <p>attack, decay, release は物理量が<b>時間</b>なのに対して, sustain のみ<code>ゲイン</code>なので注意してください.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});</code></pre>
          </section>
          <section id="section-envelope-generator-release">
            <h4>release</h4>
            <p>
              <b>release</b> (リリース) は, ゲインが sustain から最小値 <code>0</code> に変化するまでの時間です. decay / sustain と同じく,
              <a href="#section-audio-param-set-target-at-time"><code>setTargetAtTime</code></a> メソッドを利用することで実装できます.
              <code>gain</code> プロパティを <code>0</code> に近づけていくので, 第 1 引数には <code>0</code> を指定します. 第 2
              引数に指定するリリースの開始時刻は, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ値です. また, 第 3
              引数には変化に要する時間, すなわち, release time (リリースタイム) を指定します.
            </p>
            <p>
              ドラムのような音の余韻が短い楽器をシミュレートしたり, スタッカート (音を短く切って演奏する楽譜の記号) を実現したりする場合は release を短く, 逆に,
              ダンパーペダルを踏んだピアノの音や, フェルマータ (音を長く伸ばして演奏する楽譜の記号) を実現したりする場合は release を長くします.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
            <p>
              リリースを実装する場合は, <code>OscillatorNode</code> インスタンスの <code>stop</code> メソッドの即時実行は不要です. その理由は,
              <code>stop</code> メソッドを即時実行すると, その時点で音が停止してしまうので, 音に余韻が生まれません. といっても, このままでは,
              <code>start</code> メソッドの多重呼び出しになります. すなわち, <code>start</code> メソッドと
              <code>stop</code> メソッドは一対ということが順守できていません.
            </p>
            <p>
              そこで, タイマー処理で <code>gain</code> プロパティをチェックして, <b>停止とみなせる値</b>になれば, <code>stop</code> メソッドを実行します.
              ここで, 最小値である <code>0</code> と表現しなかったのは理由があります. 確かに, 理論上は, 停止とみなせる値は <code>0</code> ですが, 実装上では,
              (原因はわかりませんが) 半永久的に <code>0</code> にはなりません. したがって, 停止とみなせる値を <code>0.001</code> 未満と設定しています.
            </p>
            <p>また, 停止とみなせる値になる前に, 再度, <code>mousedown</code> した場合は, <code>OscillatorNode</code> を即時停止します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;
let intervalid = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    oscillator.stop(0);
    oscillator = null;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;

  intervalid = window.setInterval(() =&gt; {
    if (envelopegenerator.gain.value &gt;= 1e-3) {
      return;
    }

    // Stop sound (If use `OscillatorNode`)
    oscillator.stop(0);
    oscillator = null;

    if (intervalid !== null) {
      window.clearInterval(intervalid);
      intervalid = null;
    }
  }, 0);
});</code></pre>
            <p>
              これで, 完成しました ... と言いたいところですが, 1 つ問題点があります. もし, attack time もしくは decay time が経過する前に,
              <code>mouseup</code> イベントが発生するとどうなるでしょう ? attack, decay のゲイン変化のスケジューリングと,
              release　におけるゲイン変化のスケジューリングが混在してしまいますね. つまり, 上記のコードだと,
              意図したスケジューリングにならない可能性があるという問題点があります. これを解決するには,
              イベント発生時にスケジューリングをすべて解除すれば解決します. そして, スケジューリングの解除には,
              <a href="#section-audio-param-cancel-scheduled-values"><b>cancelScheduledValues</b></a>メソッド, もしくは, 値をそのまま保持しておきたい場合は,
              <a href="#section-audio-param-cancel-and-hold-at-time"><b>cancelAndHoldAtTime</b></a> メソッドを利用します.
            </p>
            <p>
              具体的には, <code>mouseup</code> 時は, 値を保持しておきたいので, <code>cancelAndHoldAtTime</code> メソッドでスケジューリングを解除します. また,
              ボタンが連打された場合に不要なスケジューリングが解除されるように, <code>mousedown</code> 時は,
              <code>cancelScheduledValues</code> メソッドでスケジューリングを解除します (そのあと <code>setValueAtTime</code> メソッドで
              <code>0</code> に初期化されるので値を保持する必要がないので).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;
let intervalid = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    oscillator.stop(0);
    oscillator = null;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.cancelScheduledValues(t0);
  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.cancelAndHoldAtTime(t3);
  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;

  intervalid = window.setInterval(() =&gt; {
    if (envelopegenerator.gain.value &gt;= 1e-3) {
      return;
    }

    // Stop sound (If use `OscillatorNode`)
    oscillator.stop(0);
    oscillator = null;

    if (intervalid !== null) {
      window.clearInterval(intervalid);
      intervalid = null;
    }
  }, 0);
});</code></pre>
            <article id="section-audio-param-cancel-and-hold-at-time-if-firefox">
              <h5>Firefox での <code>cancelAndHoldAtTime</code> の実装状況とポリフィル</h5>
              <p>
                Firefox 125 の時点では, <code>cancelAndHoldAtTime</code> が実装されていません. しかしながら, <code>cancelScheduledValues</code> と
                <code>setValueAtTime</code> を使うことでポリフィルを実装することは可能です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">if (typeof envelopegenerator.gain.cancelAndHoldAtTime === &apos;function&apos;) {
  envelopegenerator.gain.cancelAndHoldAtTime(t3);
} else {
  const value = envelopegenerator.gain.value;

  envelopegenerator.gain.cancelScheduledValues(t3);
  envelopegenerator.gain.setValueAtTime(value, t3);
}</code></pre>
            </article>
          </section>
          <section id="section-apply-envelope-generator">
            <h4>エンベロープジェネレーターの応用</h4>
            <p>
              エンベロープジェネレーターの実装, すなわち, <code>gain</code> プロパティのスケジューリングは, オーディオソースに依存したことではないので,
              <code>AudioBufferSourceNode</code> をオーディオソースとして利用することで,
              ワンショットオーディオにもエンベロープジェネレーターを適用することが可能になります. また, attack と release を楽曲データの再生に適用することで,
              フェードイン・フェードアウトの実装も可能になります.
            </p>
          </section>
          <p>
            このセクションのまとめとして, エンベロープジェネレーターの制御となる <code>gain</code> プロパティの値を視覚化するデモとなります. attack, decay,
            sustain, release の値を変えてみて, <code>gain</code> プロパティの値の変化や, それにともなう音色の変化を体感してみてください.
          </p>
          <div class="app-container">
            <svg id="svg-envelopegenerator" class="svg-envelopegenerator" width="720" height="240"></svg>
            <div>
              <button type="button" id="button-envelopegenerator" class="button-envelopegenerator">start</button>
              <div class="ranges-envelopegenerator">
                <label><span>attack</span><input type="range" id="range-attack" value="0.01" min="0" max="1" step="0.01" /></label>
                <label><span>decay</span><input type="range" id="range-decay" value="0.3" min="0" max="1" step="0.01" /></label>
                <label><span>sustain</span><input type="range" id="range-sustain" value="0.5" min="0" max="1" step="0.01" /></label>
                <label><span>release</span><input type="range" id="range-release" value="1" min="0" max="1" step="0.01" /></label>
              </div>
            </div>
          </div>
        </section>
        <article id="section-audio-param-automation-rate">
          <h3><code>a-rate</code> と <code>k-rate</code> (<code>AutomationRate</code>)</h3>
          <p>
            <code>AudioParam</code> には, <b><code>automationRate</code></b> プロパティがあり, これは <b><code>&apos;a-rate&apos;</code></b> か
            <b><code>&apos;k-rate&apos;</code></b> の <code>AutomationRate</code> 型で列挙されるどちらかの値が設定されています.
            <code>&apos;a-rate&apos;</code> は, <b><code>1</code> サンプルごとに値を適用することができる <code>AudioParam</code> です</b>.
            <code>&apos;k-rate&apos;</code> は,
            <b><code>128</code> サンプル単位 (render quantum size) で値を適用することができる <code>AudioParam</code> です</b>. <code>AudioParam</code> ごとに,
            <code>AutomationRate</code> が仕様設定されているので, 重要度としては低くなりますが,
            <code>&apos;a-rate&apos;</code> のほうがパラメータを変化させるコストはやや高いぐらいに認識しておくとよいかもしれません (<a
              href="https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor/process#examples"
              target="_blank"
              rel="noopener noreferrer"
              >実装イメージ</a>. <code>&apos;k-rate&apos;</code> の場合, <code>128</code> サンプルのパラメータの <code>0</code> 番目だけ適用すればよいので最適化しやすい). また,
            <code>AudioWorkletProcessor</code> クラスで, <code>AudioParam</code> を定義する場合 (<code>parameterDescriptors</code> プロパティ),
            適切に選択する必要がある場合もあります (デフォルトは, <code>&apos;a-rate&apos;</code>).
          </p>
          <p>
            もっとも, <code>AudioParam</code> のほとんどは <code>&apos;a-rate&apos;</code> です. 現在の仕様では, 以下のリストにある <code>AudioParam</code> が
            <code>&apos;k-rate&apos;</code> です.
          </p>
          <dl>
            <dt>AudioBufferSourceNode</dt>
            <dd><code>playbackRate</code>, <code>detune</code></dd>
            <dt>DynamicsCompressorNode</dt>
            <dd><code>threshold</code>, <code>knee</code>, <code>ratio</code>, <code>attack</code>, <code>release</code></dd>
            <dt>PannerNode</dt>
            <dd><code>panningModel</code> が <code>&apos;HRTF&apos;</code> の場合, <code>&apos;k-rate&apos;</code> のようにふるまう</dd>
          </dl>
          <p>
            <code>128</code> サンプルというのは, Web Audio API における, オーディオ処理のバッファ単位です (仕様では,
            <b>render quantum size</b> という用語が使われています). 例えば, <code>AudioWorkletProcessor</code> では, <code>128</code> サンプルごとの入力に対して
            (必要があれば, オーディオ処理を適用して), <code>128</code> サンプル出力します. リアルタイム性が要求されるようなオーディオ API では, 多くは,
            このような, 仕様で定義されているバッファサイズごとにオーディオ処理を適用する (そして, それを繰り返す) という API になっています (<a
              href="#section-audio-worklet-processor-render-quantum-size"
              ><b>Web Audio API 1.1 以降では, 必ずしも <code>128</code> サンプルではないことに注意してください</b></a>).
          </p>
        </article>
        <section id="section-garbage-collection">
          <h3>Web Audio API におけるガベージコレクション</h3>
          <p>
            Web Audio API においてはこのセクションで解説したようなスケジューリングや,
            <code>DelayNode</code> などを利用した時に発生する遅延オーディオデータなどがあるので, JavaScript
            の仕様上のガベージコレクションの対象となるオブジェクトに追加して, いくつかの条件があります.
          </p>
          <ul>
            <li>参照が残っていない</li>
            <li>処理すべきサウンドデータが残っていない</li>
            <li>ノードが接続されていない</li>
            <li>サウンドが停止している</li>
            <li>スケジューリングが設定されていない</li>
          </ul>
          <p>
            上記 5 つの条件すべてを満たすオブジェクトが, ガベージコレクションの対象となります. ざっくり説明すれば,
            なにかしらで利用されているオブジェクトはガベージコレクションの対象にならないということです.
          </p>
          <section id="section-garbage-collection-no-reference-counting">
            <h4>参照が残っていない</h4>
            <p>これに関しては, Web Audio API に限らず, JavaScript, あるいは, ガベージコレクションが実装されているあらゆるプログラミング言語一般的なことです.</p>
          </section>
          <section id="section-garbage-collection-no-sound-data">
            <h4>処理すべきサウンドデータが残っていない</h4>
            <p>
              処理すべきサウンドデータが意図せずに残るケースとして, <code>DelayNode</code> や <code>ConvolverNode</code> を利用して,
              エフェクターであるディレイやリバーブを実装した場合が考えられますが, 実装的には対処する必要はありません. 処理すべきサウンドデータがある場合に,
              サウンドデータを完了状態にするのは, <code>DelayNode</code> や <code>ConvolverNode</code> の役割であるのと, そもそも,
              このような場合に処理が残っているサウンドデータを破棄するなどの手段が現状の仕様では存在しないからです.
            </p>
          </section>
          <section id="section-garbage-collection-no-connections">
            <h4>ノードが接続されていない</h4>
            <p>
              不要になった <code>AudioNode</code> インスタンスは, <b><code>disconnect</code></b> メソッドでノードの接続を解除しておくのが律儀ではありますが,
              参照を破棄することで, 同時にノードの接続も解除されるので, 明示的に実装する必要はありません. ちなみに,
              <code>disconnect</code> メソッドのユースケースとしては, 例えば,
              ユーザーインタラクティブな操作などで動的にノードの接続を解除する必要がある場合ぐらいです.
            </p>
            <p>以下のコードは, ノード接続状態のまま, 参照を破棄していますが, 同時にノード接続も解除されるのでメモリリークに陥ることはありません.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);
}, 10);</code></pre>
          </section>
          <section id="section-garbage-collection-stop-sound">
            <h4>サウンドが停止している</h4>
            <p>
              以下のコードは, サウンドが発音状態なので, ガベージコレクションが実行されず, メモリがしだいに不足していく例です. その理由は,
              コールバック関数実行のたびに, 以前のインスタンスへの参照は破棄されますが, それに対応するサウンドが停止していないからです.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  oscillator.start(0);
}, 10);</code></pre>
          </section>
          <section id="section-garbage-collection-no-schedulings">
            <h4>スケジューリングが設定されていない</h4>
            <p>
              以下のコードは, 参照を破棄して, サウンドを停止状態にしていますが, 時間が経過するほど,
              サウンドの開始が少しずつ遅延するようにサウンドスケジューリングしているので, ガベージコレクションの実行もそれにともなって遅れるので,
              メモリが不足していきます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

let counter = 0;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  const startTime = context.currentTime + counter;
  const stopTime  = startTime + 10;

  oscillator.start(startTime);
  oscillator.stop(stopTime);

  ++counter;
}, 10);</code></pre>
          </section>
        </section>
      </section>
      <section id="section-audio-signal-processing">
        <h2>デジタルオーディオ信号処理</h2>
        <section id="section-analog-to-digital-conversion">
          <h3>A/D 変換</h3>
          <p>
            アナログ信号である音 (媒体の振動) をコンピュータで処理するためには, <code>0</code> と <code>1</code> のみの情報, つまり,
            デジタル信号に変換する必要があります. この変換処理のことを, <b>A/D変換</b> (<b>Analog to Digital Conversion</b>) と呼びます. A/D 変換は, 大きく 3
            つの処理があります.
          </p>
          <ol>
            <li>サンプリング (標本化)</li>
            <li>量子化</li>
            <li>符号化</li>
          </ol>
          <p>
            サンプリング (標本化) と量子化の処理に共通することは, <b>連続した信号を離散した信号に変換することです</b>. コンピュータでは,
            <b>連続した値や無限大となる値を扱うことが不可能</b>だからです.
          </p>
          <p>
            <a href="#section-about-sound">「音」</a>のセクションでは, いくつか音の波形のイラストを記載しましたが, それらは常に 2 つの連続した物理量 (次元)
            をもっていました. <b>時間</b>と<b>振幅</b>です. サンプリング (標本化) と量子化は, これら 2 つの連続した物理量を離散信号に変換する処理となります.
          </p>
          <section id="section-analog-to-digital-conversion-sampling">
            <h4>サンプリング (標本化)</h4>
            <p>
              <b>サンプリング</b> (<b>標本化</b>)は, 時間を離散した値に変換する処理です. 離散信号, すなわち, とびとびの値をとっていくためには,
              その間隔を決定するパラメータが必要になります. それが, <b>サンプリング周期</b> (<b>標本化周期</b>) です. サンプリング周期の逆数となるパラメータは,
              <b>標本化周波数</b> (<b>サンプリング周波数</b>) です. 簡単に解説すれば, サンプリング周波数は, <code>1 sec</code> の間に, いくつのサンプル (離散点)
              をとるか ? ということを意味しています. 例えば, サンプリング周波数が <code>48000 Hz</code> の場合, <code>1 sec</code> の間に
              <code>48000</code> サンプル (離散点) をとることになります.
            </p>
            <figure>
              <svg id="svg-figure-sampling" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>
                <span>サンプリングとサンプリング周波数 (サンプリング周期)</span>
                <span>1 sec に 8 サンプルあるので, <code>8 Hz</code> (<code>0.125 sec</code>)</span>
              </figcaption>
            </figure>
            <p>
              サンプリング (標本化) では重要な定理があります. それは, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> 以上の周波数は元のアナログ信号に復元できないという定理です. この定理は,
              <b>サンプリング定理</b> (<b>標本化定理</b>, <b>シャノンの定理</b>) と呼ばれます. 逆の視点で表現すれば, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> より低い周波数は元のアナログ信号に復元可能ということです. また, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> は, <b>ナイキスト周波数</b>と呼ばれます. サンプリング定理から, 原信号に含まれる最大の周波数成分の
              <b>2 倍より大きい</b>サンプリング周波数に設定すれば, 元のアナログ信号に復元可能ということになります (実際には, 低域通過フィルタ (Low-Pass Filter)
              を利用して, 高い周波数成分を除去するプリプロセス処理を施します).
            </p>
            <p>
              サンプリング定理を満たさないサンプリング周波数, すなわち, 原信号に含まれる最大の周波数成分の 2 倍以下のサンプリング周波数でサンプリングすると,
              <b>折り返し歪み</b> (<b>エイリアス歪み</b>) が発生して, ノイズとして復元されてしまいます.
            </p>
            <p>
              例えば, <code>1 Hz</code> の信号に対して, 2 サンプル (サンプリング周波数 <code>2 Hz</code>, ナイキスト周波数 <code>1 Hz</code>)
              では原信号に復元できません.
            </p>
            <figure>
              <svg id="svg-figure-sampling-theorem-with-aliasing" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>
                <span>サンプリング定理 (折り返し歪みが発生する)</span>
              </figcaption>
            </figure>
            <p>
              <code>1 Hz</code> の信号に対して, 3 サンプル (サンプリング周波数 <code>3 hz</code>, ナイキスト周波数 <code>1.5 Hz</code>) だと, 精度は低いですが,
              原信号に復元できます
            </p>
            <figure>
              <svg
                id="svg-figure-sampling-theorem-without-aliasing"
                width="720"
                height="405"
                data-parameters="true"
                data-a="1"
                data-f="1"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>サンプリング定理 (定理を満たす場合)</figcaption>
            </figure>
            <p>
              サンプリングの精度を高くするほど, すなわち,
              サンプリング周波数を高くするほど元のアナログ信号に対してより精度の高いデジタル信号に変換可能となります. 一方で,
              データサイズはサンプリング周波数に比例して大きくなってしまいます.
            </p>
            <p>
              以下の図は, 充分なサンプル数 (サンプリング周波数) だと原信号により精度高く復元できること表しています. そのトレードオフとして,
              サンプル数が多くなるので, データサイズはより大きくなることも表しています.
            </p>
            <figure>
              <svg id="svg-figure-sampling-theorem" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>サンプリング定理</figcaption>
            </figure>
            <p>
              サンプリング周波数の具体例として, 音楽 CD は <code>44.1 kHz</code> に設定されています. 人間の聴覚が知覚可能な周波数はおよそ
              <code>20 kHz</code>であることを考慮してサンプリング定理を適用しているからです. さらに音質の高いものだと
              <code>96 kHz</code> 以上に設定されている音楽データもあります (ハイレゾオーディオのサンプリング周波数). 電話では
              <code>8 kHz</code> に設定されています. 音声の場合は, 多少音質が損なわれても相手の音声を聴きとることが可能なこと,
              楽器音ほど高い周波数成分が含まれないこと, リアルタイムに通信するので可能な限りデータサイズを減らす必要があることなどが理由としてあげられます.
            </p>
            <p>
              Web Audio API では, <code>AudioContext</code> インスタンス生成時の引数として, <b><code>AudioContextOptions</code></b> の
              <b><code>sampleRate</code></b> プロパティで明示的に指定することが可能です. 明示的に指定しない場合, デバイスのサンプリング周波数
              (<code>44100</code>, <code>48000</code> など) に設定されています.
            </p>
            <p>
              サウンドの視覚化の実装では, サンプリング周波数 (<code>AudioContext</code> インスタンス, または, <code>AudioBuffer</code> インスタンスの
              <code>sampleRate</code> プロパティ) にアクセスすることはよくあります. したがって, サンプリング周波数が何を意味しているのか ? ということと,
              サンプリング定理に関して理解しておくと役に立つでしょう.
            </p>
          </section>
          <section id="section-analog-to-digital-conversion-quantization">
            <h4>量子化</h4>
            <p>
              <b>量子化</b>は, <b>振幅を離散した値に変換する処理です</b>. サンプリングと同じく, とびとびの値をとっていくためには,
              その間隔を決定づけるパラメータが必要になります. それが, <b>量子化ビット</b> (<b>量子化精度</b>) です.
            </p>
            <p>
              サンプリングされたアナログ信号は時間軸方向は, 離散化されていますが, 振幅軸の方向は連続したままです. 量子化では,
              量子化ビットで指定された精度にしたがって, 振幅を整数値に丸める処理を実行します. 例えば, 量子化ビットが <code>2 bit</code> の場合, 4
              つのステップの値 (<span class="math-inline">$2^{2} = 4$</span>) のいずれかに, <code>3 bit</code> の場合, 8 つのステップの値 (<span
                class="math-inline"
                >$2^{3} = 8$</span>) のいずれかに振幅が丸められます.
            </p>
            <figure>
              <svg id="svg-figure-quantization" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>量子化 (量子化ビット <code>3 bit</code>)</figcaption>
            </figure>
            <p>
              量子化の丸め処理によって生じる誤差を, <code>量子化雑音</code>と呼びます. 量子化ビットが小さいほど, 丸め処理による誤差が大きくなり,
              原信号への復元も精度が低くなってしまいます. 逆に, 量子化の精度を高くするほど, すなわち, 量子化ビットを大きくするほど, 量子化雑音は少なくなり
              (誤差が小さくなり), 原信号への復元の精度も高くなりますが, データサイズは量子化ビットに比例して大きくなります.
            </p>
            <figure>
              <svg id="svg-figure-quantization-bits" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>量子化ビット</figcaption>
            </figure>
            <p>音楽 CD での量子化ビットは 16 bit に設定されています. ハイレゾオーディオの量子化ビットは <code>24 bit</code> 以上が必要条件となっています.</p>
          </section>
          <section id="section-analog-to-digital-conversion-coding">
            <h4>符号化</h4>
            <p>
              サンプリングによって, 時間軸方向に離散化し, それぞれのサンプル点を, 量子化によって丸めた整数値に 2 進数を割り当てていきます. 量子化した
              (整数値に丸めた) 振幅を 2 進数に符号化すると, コンピュータの内部で処理することが可能なデジタル信号となります.
            </p>
            <p>サンプリング周波数 <code>16 Hz</code>, 量子化ビット <code>4 bit</code>, 2 の補数方式で符号化した例です.</p>
            <figure>
              <svg id="svg-figure-coding" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>符号化</figcaption>
            </figure>
          </section>
          <article id="section-analog-to-digital-conversion-pcm">
            <h4>PCM (Pulse Code Modulation)</h4>
            <p>
              <b>PCM</b> (<b>Pulse Code Modulation</b>) とは, このセクションで解説したように, アナログ信号をデジタル信号に変換する変調方式のことです.
              厳密に表現すると, このセクションで解説した PCM は, 量子化の幅を均等 (線形的) に取得しているので, <b>Linear PCM</b> です. 量子化の方式によって,
              <b>log-PCM</b>, <b>DPCM</b> (<b>Differential PCM</b>), <b>ADPCM</b> (<b>Adaptive Differential PCM</b>) などがあります.
              どれが優れた方式というのはなく, ケースによって使い分けますが, 多くの場合, Linear PCM が使われているので, 単純に PCM と言った場合, Linear PCM
              を意味することが多いです. Web Audio API でもオーディオデータの実体である <code>AudioBuffer</code> では, <code>32 bit</code> (浮動小数点数) の
              Linear PCM による値を格納しています.
            </p>
          </article>
        </section>
        <section id="section-fourier-analysis">
          <h3>フーリエ解析</h3>
          <p>
            このセクションでは, デジタルオーディオ信号処理において, 中核となる数学的処理である, <b>フーリエ解析</b> (フーリエ級数とフーリエ級数を一般化した
            (非周期関数に拡張した) フーリエ変換, コンピュータでフーリエ変換を実行するための離散フーリエ変換, そして, 回転因子の性質を利用して,
            離散フーリエ変換の (時間) 計算量を <span class="math-inline">$O\left(N^{2}\right)$</span> から
            <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> に減らして実行する高速フーリエ変換) について解説します. もっとも,
            数式による厳密な解説や証明は, 最適なドキュメントや書籍がすでにたくさんあるので, できるだけ, Web Audio API での仕様を把握したり,
            <code>AudioWorklet</code> でオーディオ信号処理を実装したりする場合を想定して, 数式による (厳密な) 解説は最小限にとどめて,
            イラストやコードをベースに, 概念を理解するために役に立つ内容になればと思います.
          </p>
          <section id="section-fourier-series">
            <h4>フーリエ級数</h4>
            <p>
              <a href="#section-amplitude-and-frequency">周期関数</a>は, 周波数の異なる余弦波と正弦波の級数で近似することができます. この級数が,
              <b>フーリエ級数</b>であり, 周期関数をフーリエ級数で表現する場合, <b>フーリエ級数展開</b> と呼ばれます.
              <span class="math-inline">$x\left(t\right)$</span>が, 周期 <span class="math-inline">$T$</span> の場合, フーリエ級数は以下の数式で定義されます.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &f\left(t\right) = \frac{a_{0}}{2} + \sum_{n=1}^{\infty}\left(a_{n}\cos\left(n\frac{2 \pi}{T}t\right) + b_{n}\sin\left(n\frac{2 \pi}{T}t\right)\right) \\
                  &a_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\cos\left(n\frac{2 \pi}{T}t\right)dt \\
                  &b_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\sin\left(n\frac{2 \pi}{T}t\right)dt \\
                \end{flalign}
              $
            </div>
            <p>
              <span class="math-inline">$a_{n}$</span>, <span class="math-inline">$b_{n}$</span> は <b>フーリエ係数</b>で,
              物理的には各周波数成分の<b>振幅</b>を表しています. また, <span class="math-inline">$\frac{2 \pi}{T} = 2 \pi f$</span> は, <b>角速度</b>
              <span class="math-inline">$\omega$</span> で定義される場合もあります.
            </p>
            <p>
              厳密には, フーリエ級数が成立する条件は, 周期関数であるだけでは不十分で, <b>ディリクレの条件</b>と合わせて十分条件となります.
              これを理解するためには, 三角関数の基本的な性質 (高校レベルの数学) や三角関数の直交性などをもとに,
              <b>リーマン・ルベーグの補助定理</b>や<b>パーセバルの等式</b>などを理解する必要があるので, 数学的な厳密性を理解したい場合は,
              それぞれ最適なドキュメントを参考にしてください.
            </p>
            <p>
              ここでは, 視覚的に理解するために, 周波数の異なる正弦波の級数で, 矩形波やノコギリ波, 三角波を生成してみます. また, 級数を大きくする
              (項数を大きくする) ほど, 実際の波形により近似することもわかります.
            </p>
            <div class="app-container">
              <svg id="svg-fourier-series" class="svg-fourier-series" width="720" height="240"></svg>
              <div class="forms-fourier-series">
                <button type="button" id="button-plot-fourier-series">Plot</button>
                <button type="button" id="button-clear-fourier-series">Clear</button>
                <button type="button" id="button-animation-fourier-series">Animation</button>
                <label for="select-interval-fourier-series">Interval</label>
                <select id="select-interval-fourier-series">
                  <option value="500" selected>500 msec</option>
                  <option value="60 fps">60 fps</option>
                  <option value="50">50 msec</option>
                  <option value="100">100 msec</option>
                  <option value="250">250 msec</option>
                </select>
              </div>
              <div class="forms-fourier-series">
                <label for="select-function-fourier-series">Wave Type</label>
                <select id="select-function-fourier-series">
                  <option value="square" selected>Square</option>
                  <option value="sawtooth">Sawtooth</option>
                  <option value="triangle">Triangle</option>
                </select>
                <label>
                  <span id="output-sum-box-fourier-series">&Sigma; <sub>k</sub> = <span id="output-sum-fourier-series">1</span></span>
                  <input type="range" id="range-sum-fourier-series" value="1" min="1" max="100" step="1" />
                </label>
              </div>
            </div>
            <p>
              余弦波と正弦波で表現されるフーリエ級数に, <b>オイラーの公式</b> (<span class="math-inline">$j$</span> は
              <span class="math-inline">$j^{2} = -1$</span> となる虚数単位) を適用すると, <b>複素フーリエ級数</b>を導出可能です (つまり, 複素フーリエ級数は,
              より一般化したフーリエ級数と言えます. さらに一般化を進めると, フーリエ変換となります).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{align}
                  &e^{j\theta} = \cos\left(\theta\right) + j\sin\left(\theta\right) \quad (Euler's formula) \\
                  &f\left(t\right) = \sum_{n=-\infty}^{\infty}c_{n}e^{jn\frac{2 \pi}{T}t} \\
                  &c_{n} = \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)e^{-jn\frac{2 \pi}{T}t}dt \\
                \end{align}
              $
            </div>
            <p>
              物理的な観点で理解すると, 複素フーリエ級数は, 余弦波と正弦波の 2 次元の<b>振動現象</b>であるフーリエ級数を, 3 次元の<b>回転</b>へと拡張します.
              また, 複素フーリエ級数によって, フーリエ級数の問題点, すなわち, 位相をシフトするとフーリエ係数の値が変化する問題
              (余弦波と正弦波は位相の違いでしかないので, 原信号が同じでもフーリエ係数が異なることが起きうる) を発展的に解決します.
            </p>
          </section>
          <section id="section-fourier-transform">
            <h4>フーリエ変換</h4>
            <p>
              フーリエ級数が周期関数のみ適用可能だったのを, 非周期関数にも適用できるように, さらに拡張したフーリエ級数が<b>フーリエ変換</b>です.
              フーリエ級数から, フーリエ変換を導出するには, 非周期, つまり, <b>周期を <span class="math-inline">$\infty$</span></b> に拡張して導出します.
              具体的には, 複素フーリエ級数の係数 <span class="math-inline">$c_{n}$</span> の周期 <span class="math-inline">$T$</span> を
              <span class="math-inline">$\infty$</span> に拡張すると, 角速度
              <span class="math-inline">$\omega (= \frac{2 \pi}{T} = 2 \pi f)$</span> が連続的な角速度になることで導出できます (以下は,
              フーリエ変換と逆フーリエ変換の定義式です).
            </p>
            <p>
              フーリエ変換の厳密な導出を理解するには, <b>デルタ関数</b>や<b>単位階段関数</b>の理解が必要になります (さらに, フーリエ変換では,
              <b>絶対可積分</b> (<span class="math-inline">$\int_{-\infty}^{\infty}\left|f\left(t\right)\right|dt \lt \infty$</span> ) が必要条件となります.
              フーリエ級数からフーリエ変換の数学的な導出の詳細に関しては, それぞれ最適なドキュメントを参考にしてください).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &F\left(f\right) = \int_{-\infty}^{\infty}f\left(t\right)e^{-j2 \pi ft}dt \\
                  &f\left(t\right) = \frac{1}{2 \pi}\int_{-\infty}^{\infty}F\left(f\right)e^{j2 \pi tf}df \\
                \end{flalign}
              $
            </div>
            <section id="section-spectrum">
              <h5>スペクトル</h5>
              <p>
                フーリエ変換後の関数は, 物理的には<b>スペクトル</b>となります. スペクトルとは, 各周波数成分の振幅や位相を表す波形です (したがって,
                周波数領域の波形, 周波数ドメインの波形などと表現することもあります). つまり, 2 次元のグラフで考えると, 横軸の次元が周波数となり,
                縦軸が振幅であれば, <b>振幅スペクトル</b>, 位相であれば, <b>位相スペクトル</b>となります. もう少し厳密に説明すると, フーリエ変換後の関数は,
                複素数の関数となるので, <b>絶対値</b>を取得すれば振幅スペクトル, <b>偏角</b>を取得すれば位相スペクトルとなります (以下に, 複素数
                <span class="math-inline">$z = x + jy$</span> を定義した場合の絶対値 <span class="math-inline">$\left|z\right|$</span> と偏角
                <span class="math-inline">$\theta$</span> を記載します). フーリエ変換後の関数を逆フーリエ変換すると, 元の横軸を時間とした波形となります.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $\left|z\right| = \sqrt{x^{2} + y^{2}} \quad \cos\theta = \frac{x}{\sqrt{x^{2} + y^{2}}} \quad \sin\theta = \frac{y}{\sqrt{x^{2} + y^{2}}} \quad \tan\theta = \left(\frac{\sin\theta}{\cos\theta}\right) \quad \theta = \tan^{-1}\left(\frac{\sin\theta}{\cos\theta}\right) = \tan^{-1}\left(\frac{y}{x}\right)$
              </div>
              <p>
                ちなみに, <b>人間の聴覚は位相スペクトルの違いに鈍感</b>という特性があるので, 一般的に, スペクトルと表現した場合,
                振幅スペクトルを意味することがほとんどです.
              </p>
              <p>
                音響特徴量は振幅スペクトルにあらわれることが多く, したがって, オーディオ信号処理を適用する場合,
                周波数領域にて演算を実行することが頻繁にあります. このことが, デジタルオーディオ信号処理において, フーリエ解析 (コンピュータでは,
                高速フーリエ変換) が中核となる理由です.
              </p>
            </section>
          </section>
          <section id="section-discrete-fourier-transform">
            <h4>離散フーリエ変換 (DFT)</h4>
            <p>
              コンピュータで実現する場合, 無限区間の積分は原理上できないので, ある区間で和分を算出する必要があります. これが,
              <b>離散フーリエ変換</b> (<b>DFT</b>: <b>Discrete Fourier Transform</b>)です (余談ですが, コンピュータにおいて, 積分は和分,
              微分は差分で実装します).
            </p>
            <p>
              フーリエ変換から, 離散フーリエ変換を導出するには, 周波数 (周期) と, サンプリング周波数 (サンプリング周期) を数列 (離散値) で対応づけます.
              <span class="math-inline">$f_{s}$</span> は, サンプリング周波数 (<span class="math-inline">$T_{s}$</span> は, サンプリング周期). また,
              離散フーリエ変換は, 一定のサイズで変換する必要があるので <span class="math-inline">$N$</span> は, 離散フーリエ変換のサンプル数です (Web Audio API
              では, <code>AnalyserNode</code> の <b><code>fftSize</code></b> プロパティの値に相当します).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &t = nT_{s} = \frac{n}{f_{s}} \quad (n = 0. 1, 2, \cdots N - 1) \\
                  &f = k\frac{f_{s}}{N} \quad (k = 0, 1, 2, \cdots N - 1) \\
                \end{flalign}
              $
            </div>
            <p>
              そして, 積分は和分になるので, これらをフーリエ変換の式に適用して, 変形すると, 離散フーリエ変換と逆離散フーリエ変換の定義式が導出できます.
              <span class="math-inline">$x\left(n\right)$</span>, および, <span class="math-inline">$X\left(k\right)$</span> は, サンプリングした信号です.
              数学的には数列, プログラミング的には配列のような順序性をもつ数値のコレクションと考えると理解しやすいかもしれません.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)e^{-j\frac{2 \pi kn}{N}} \\
                  &x\left(n\right) = \frac{1}{N}\sum_{k = 0}^{N - 1}X\left(k\right)e^{j\frac{2 \pi nk}{N}} \\
                \end{flalign}
              $
            </div>
            <p>
              多くのプログラミング言語において, 配列のようなコレクションのインデックスは <code>0</code> から開始するので, 離散フーリエ変換の積和演算の範囲も,
              <code>0</code> から開始している点と有界となっている点に着目してください.
            </p>
            <p>また, <span class="math-inline">$e^{-j\frac{2 \pi n}{N}}$</span> は, <b>回転因子</b> (<b>回転子</b>) と呼ばれ, 以下のように定義されます.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $W^{n} = e^{-j\frac{2 \pi n}{N}} = \cos\left(\frac{2 \pi n}{N}\right) - j\sin\left(\frac{2 \pi n}{N}\right)$
            </div>
            <p>回転因子は, 例えば, <span class="math-inline">$N$</span> を <code>8</code> とした場合, 複素平面上の単位円を 8 分割するような回転を表現します.</p>
            <figure>
              <svg id="svg-figure-rotation-factors" width="720" height="405" />
              <figcaption><span class="math-inline">$N = 8$</span> の場合の回転因子</figcaption>
            </figure>
            <p>
              このことから, 回転因子は以下のような性質をもっています (また, 離散フーリエ変換のサイズ <span class="math-inline">$\frac{N}{2}$</span> は,
              ナイキスト周波数成分のインデックスに相当します).
            </p>
            <ul>
              <li><span class="math-inline">$W^{n + N} = W^{n}$</span></li>
              <li><span class="math-inline">$W^{n + \frac{N}{2}} = -W^{n}$</span></li>
            </ul>
            <p>
              以下は, 回転因子で定義した離散フーリエ変換と逆離散フーリエ変換です. 高速フーリエ変換では, 回転因子の性質
              (周期性による対称性や半周期性の負の対称性) を利用して, 各要素の計算量を減らして演算の高速化を実現しています.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)W^{n} \\
                  &x\left(n\right) = \frac{1}{N}\sum_{k = 0}^{N - 1}X\left(k\right)W^{-k} \\
                \end{flalign}
              $
            </div>
          </section>
          <section id="section-fast-fourier-transform">
            <h4>高速フーリエ変換 (FFT)</h4>
            <p>
              <b>高速フーリエ変換</b> (<b>FFT</b>: <b>Fast Fourier Transform</b>) は, 回転因子の性質を利用して, 離散フーリエ変換では (時間) 計算量を
              <span class="math-inline">$O\left(N^{2}\right)$</span> 要するのを,
              <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> にまで減らして, コンピュータでのフーリエ変換を高速化するアルゴリズムです.
              ただし, 回転因子の性質を利用する関係上, <b>フーリエ変換のサイズが 2 の冪乗</b>の場合のみ高速化できるという制約がつきます (この制約に関しては,
              <code>0</code> 埋め処理などによって, 強制的にフーリエ変換のサイズを 2 の冪乗にするなどして解決できます). Web Audio API においても,
              <code>AnalyserNode</code> の <code>fftSize</code> プロパティがとりうる値は, すべて 2 の冪乗です.
            </p>
            <p>実際に, どのように計算量を削減しているのかを解説していきます.</p>
            <section id="section-fast-fourier-transform-algorithm">
              <h5>高速フーリエ変換の導出</h5>
              <p>離散フーリエ変換の式を行列演算に書き換えます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)W^{n} \\
                  \end{flalign}
                $
              </div>
              <p><span class="math-inline">$N = 4$</span> として, 行列演算に変換します.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{3} & W^{6} & W^{9} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>ここで, 回転因子の性質を利用すると, <span class="math-inline">$W^{4} = W^{0}, W^{6} = W^{2}, W^{9} = W^{1}$</span> となるので,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{2} & W^{0} & W^{2} \\
                    W^{0} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>ここまでで, 離散フーリエ変換の演算を行列演算に変換することができました.</p>
              <p>
                行列演算に変換できたら, 行を偶奇で分割します. 偶数行を行列の上部に入れ替えて, 奇数行を行列の下部に入れ替えます. 変換行列の行を入れ替えるので,
                出力となる <span class="math-inline">$X_{k}$</span> も行が入れ替わることに注意してください.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{0} & W^{2} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <p>これを考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{2}) \\
                    (x_{1} + x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & W^{3} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{2}) \\
                    (x_{1} - x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <p>この変形によって, 乗算と加算の回数がおよそ半分まで減らすことができました. ここでさらに回転因子の性質を利用すると, 以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & -W^{0} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{2}) \\
                    (x_{1} + x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & -W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{2}) \\
                    (x_{1} - x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ところで, 行の偶奇を入れ替えたので, 出力となる <span class="math-inline">$X_{k}$</span> の順序が, 入力の順序と一致しなくなります (つまり,
                ある時間領域の値のスペクトルが一致しなくなります). 実は, 一見するとランダムに並びますが, 規則性があり, 各インデックスを 2 進数で表現した場合の,
                ビットを上下反転させた関係になっています. この関係を利用して, インデックスの並びを整列するアルゴリズムを<b>ビットリバース</b>と呼びます.
              </p>
              <table>
                <caption>
                  ビットリバースの対応表 2 ビット (<span class="math-inline">$N = 4$</span>)
                </caption>
                <thead>
                  <tr>
                    <th scope="col">Index</th>
                    <th scope="col"><span class="math-inline">$x_{n}$</span></th>
                    <th scope="col"><span class="math-inline">$X_{k}$</span></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>0</td>
                    <td>00</td>
                    <td>00</td>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>01</td>
                    <td>10</td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>10</td>
                    <td>01</td>
                  </tr>
                  <tr>
                    <td>3</td>
                    <td>11</td>
                    <td>11</td>
                  </tr>
                </tbody>
              </table>
              <p>同じように, <span class="math-inline">$N = 8$</span> として, 高速フーリエ変換になるように行列演算します.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                    X_{4} \\
                    X_{5} \\
                    X_{6} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0}  & W^{0}  & W^{0}  & W^{0}  & W^{0}  & W^{0}  \\
                    W^{0} & W^{1} & W^{2}  & W^{3}  & W^{4}  & W^{5}  & W^{6}  & W^{7}  \\
                    W^{0} & W^{2} & W^{4}  & W^{6}  & W^{8}  & W^{10} & W^{12} & W^{14} \\
                    W^{0} & W^{3} & W^{6}  & W^{9}  & W^{12} & W^{15} & W^{18} & W^{21} \\
                    W^{0} & W^{4} & W^{8}  & W^{12} & W^{16} & W^{20} & W^{24} & W^{29} \\
                    W^{0} & W^{5} & W^{10} & W^{15} & W^{20} & W^{25} & W^{30} & W^{35} \\
                    W^{0} & W^{6} & W^{12} & W^{18} & W^{24} & W^{30} & W^{36} & W^{42} \\
                    W^{0} & W^{7} & W^{14} & W^{21} & W^{28} & W^{35} & W^{42} & W^{49} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>回転因子の性質を利用すると,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                    X_{4} \\
                    X_{5} \\
                    X_{6} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} & W^{4} & W^{5} & W^{6} & W^{7} \\
                    W^{0} & W^{2} & W^{4} & W^{6} & W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{3} & W^{6} & W^{1} & W^{4} & W^{7} & W^{2} & W^{5} \\
                    W^{0} & W^{4} & W^{0} & W^{4} & W^{8} & W^{5} & W^{0} & W^{5} \\
                    W^{0} & W^{5} & W^{2} & W^{7} & W^{4} & W^{1} & W^{6} & W^{3} \\
                    W^{0} & W^{6} & W^{4} & W^{2} & W^{0} & W^{6} & W^{4} & W^{2} \\
                    W^{0} & W^{7} & W^{6} & W^{5} & W^{4} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                行列演算に変換できたら, 行を偶奇で分割します. 偶数行を行列の上部に入れ替えて, 奇数教を行列の下部に入れ替えます. 変換行列の行を入れ替えるので,
                出力となる <span class="math-inline">$X_{k}$</span> も行が入れ替わることに注意してください.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{4} \\
                    X_{6} \\
                    X_{1} \\
                    X_{3} \\
                    X_{5} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{4} & W^{6} & W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{4} & W^{0} & W^{4} & W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{6} & W^{4} & W^{2} & W^{0} & W^{6} & W^{4} & W^{2} \\
                    W^{0} & W^{1} & W^{2} & W^{3} & W^{4} & W^{5} & W^{6} & W^{7} \\
                    W^{0} & W^{3} & W^{6} & W^{1} & W^{4} & W^{7} & W^{2} & W^{5} \\
                    W^{0} & W^{5} & W^{2} & W^{7} & W^{4} & W^{1} & W^{6} & W^{3} \\
                    W^{0} & W^{7} & W^{6} & W^{5} & W^{4} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 4 行 4 列の行列と, 右上 4 行 4 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 4 行 4 列の行列と, 右下 4 行 4 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <p>これを考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{4} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{6} & W^{4} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} + x_{4} \\
                    x_{1} + x_{5} \\
                    x_{2} + x_{6} \\
                    x_{3} + x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                    X_{5} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{3} & W^{6} & W^{1} \\
                    W^{0} & W^{5} & W^{2} & W^{7} \\
                    W^{0} & W^{7} & W^{6} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} - x_{4} \\
                    x_{1} - x_{5} \\
                    x_{2} - x_{6} \\
                    x_{3} - x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここまでの処理によって, <span class="">$N = 8$</span> の高速フーリエ変換を <span class="">$N = 4$</span> に帰着することができたので, 再帰的に
                <span class="">$N = 4$</span> の場合も, 行の偶奇を入れ替えて回転因子の性質を利用して,
                <span class="math-inline">$N = 2$</span> の場合の高速フーリエ変換に帰着させます.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{6} & W^{4} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} + x_{4} \\
                    x_{2} + x_{6} \\
                    x_{1} + x_{5} \\
                    x_{3} + x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{5} & W^{2} & W^{7} \\
                    W^{0} & W^{3} & W^{6} & W^{1} \\
                    W^{0} & W^{7} & W^{6} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} - x_{4} \\
                    x_{2} - x_{6} \\
                    x_{1} - x_{5} \\
                    x_{3} - x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                こちらも, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は時計回りに
                <span class="math-inline">$\frac{2}{8}$</span> 回転, また, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は反時計回りに
                <span class="math-inline">$\frac{2}{8}$</span> 回転しているという対称性があります.
              </p>
              <p>これら考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & W^{4} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) + (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) + (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{2} \\
                    W^{0} & W^{6} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) - (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) - (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) + W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) + W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{3} \\
                    W^{0} & W^{7} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) - W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) - W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <p>ここでさらに回転因子の性質を利用すると, 以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & -W^{0} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) + (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) + (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{2} \\
                    W^{0} & -W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) - (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) - (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & -W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) + W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) + W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{3} \\
                    W^{0} & -W^{3} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) - W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) - W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <p>
                <span class="math-inline">$N = 2$</span> の場合の高速フーリエ変換に帰着できたので, 最後にビットリバースを適用してインデックスを並び替えます.
              </p>
              <table>
                <caption>
                  ビットリバースの対応表 3 ビット (<span class="math-inline">$N = 8$</span>)
                </caption>
                <thead>
                  <tr>
                    <th scope="col">Index</th>
                    <th scope="col"><span class="math-inline">$x_{n}$</span></th>
                    <th scope="col"><span class="math-inline">$X_{k}$</span></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>0</td>
                    <td>000</td>
                    <td>000</td>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>001</td>
                    <td>100</td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>010</td>
                    <td>010</td>
                  </tr>
                  <tr>
                    <td>3</td>
                    <td>011</td>
                    <td>110</td>
                  </tr>
                  <tr>
                    <td>4</td>
                    <td>100</td>
                    <td>001</td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>101</td>
                    <td>101</td>
                  </tr>
                  <tr>
                    <td>6</td>
                    <td>110</td>
                    <td>011</td>
                  </tr>
                  <tr>
                    <td>7</td>
                    <td>111</td>
                    <td>111</td>
                  </tr>
                </tbody>
              </table>
              <p>
                高速フーリエ変換のサイズを一般化して, <span class="math-inline">$N = 2^{m}$</span> の場合も,
                <span class="math-inline">$2^{m}, 2^{m - 1}, 2^{m - 2}\cdots 32, 16, 8, 4$</span> と再帰的に高速フーリエ変換を導出することが可能です.
              </p>
              <p>
                同様に, 逆離散フーリエ変換の (時間) 計算量も <span class="math-inline">$O\left(N^{2}\right)$</span> から
                <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> に減らすことができます (<b>逆高速フーリエ変換</b> (<b>IFFT</b>:
                <b>Inverse Fast Fourier Transform</b>)).
              </p>
              <p>
                このセクションで解説した高速フーリエ変換は, <b>時間間引き型高速フーリエ変換</b>のアルゴリズムとなります (次のセクションで解説する, FFT
                のバタフライ演算のフロー図において, 左側 (時間領域) から, 右側 (周波数領域) へフローするアルゴリズムです. 逆に, 右側 (周波数領域) から, 左側
                (時間領域) へフローするアルゴリズムは, <b>周波数間引き型高速フーリエ変換</b>と呼ばれます. しかし, この 2
                つは高速フーリエ変換と逆高速フーリエ変換の関係にあるだけで, アルゴリズムの本質は同じです).
              </p>
            </section>
            <section id="section-fast-fourier-transform-code">
              <h5>高速フーリエ変換の実装</h5>
              <p>
                離散フーリエ変換の定義式から, 高速フーリエ変換が導出できることはわかりましたが,
                回転因子の性質や行列の変形をコードに記述するのは少し難しいと思います. そこで, 一般的には,
                高速フーリエ変換と等価な<b>バタフライ演算</b>のフロー図を考えることで, より実装に近い形式で考えることができます.
              </p>
              <figure>
                <svg id="svg-figure-fft-symbols" width="720" height="88" />
                <figcaption>バタフライ演算のフロー図の記号</figcaption>
              </figure>
              <p><span class="math-inline">$N = 4$</span> の場合の, 高速フーリエ変換の導出をバタフライ演算のフロー図を記載します.</p>
              <figure>
                <svg id="svg-figure-fft-4" width="720" height="405" />
                <figcaption>FFT のサイズ <span class="math-inline">$N = 4$</span> の場合のバタフライ演算のフロー図</figcaption>
              </figure>
              <p>同様に, <span class="math-inline">$N = 8$</span> の場合の, 高速フーリエ変換の導出をバタフライ演算のフロー図を記載します.</p>
              <figure>
                <svg id="svg-figure-fft-8" width="1600" height="405" />
                <figcaption>FFT のサイズ <span class="math-inline">$N = 8$</span> の場合のバタフライ演算のフロー図</figcaption>
              </figure>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">function pow2(n) {
  return 2 ** n;
}

/**
 * FFT
 *
 * @param {Float32Array} reals This argument is instance of `Float32Array` for real number.
 * @param {Float32Array} imags This argument is instance of `Float32Array` for imaginary number.
 * @param {number} size This argument is FFT size (power of two).
 */
function FFT(reals, imags, size) {
  const indexes = new Uint16Array(size);  // FFT size is `0` between `65535`.

  const numberOfStages = Math.log2(size);

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      const rest = numberOfStages - stage;

      for (let j = 0; j &lt; pow2(rest); j++) {
        const n = i * pow2(rest + 1) + j;
        const m = pow2(rest) + n;
        const w = 2.0 * Math.PI * j * pow2(stage - 1);

        const areal = reals[n];
        const aimag = imags[n];
        const breal = reals[m];
        const bimag = imags[m];
        const wreal = Math.cos(w / size);
        const wimag = -1 * Math.sin(w / size);  // Clockwise

        if (stage &lt; numberOfStages) {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = (wreal * (areal - breal)) - (wimag * (aimag - bimag));
          imags[m] = (wreal * (aimag - bimag)) + (wimag * (areal - breal));
        } else {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = areal - breal;
          imags[m] = aimag - bimag;
        }
      }
    }
  }

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    const rest = numberOfStages - stage;

    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      indexes[pow2(stage - 1) + i] = indexes[i] + pow2(rest);
    }
  }

  for (let k = 0; k &lt; size; k++) {
    if (indexes[k] &lt;= k) {
      continue;
    }

    const real = reals[indexes[k]];
    const imag = imags[indexes[k]];

    reals[indexes[k]] = reals[k];
    imags[indexes[k]] = imags[k];

    reals[k] = real;
    imags[k] = imag;
  }
}

/**
 * IFFT
 *
 * @param {Float32Array} reals This argument is instance of `Float32Array` for real number.
 * @param {Float32Array} imags This argument is instance of `Float32Array` for imaginary number.
 * @param {number} size This argument is IFFT size (power of two).
 */
function IFFT(reals, imags, size) {
  const indexes = new Uint16Array(size);  // FFT size is `0` between `65535`.

  const numberOfStages = Math.log2(size);

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      const rest = numberOfStages - stage;

      for (let j = 0; j &lt; pow2(rest); j++) {
        const n = i * pow2(rest + 1) + j;
        const m = pow2(rest) + n;
        const w = 2.0 * Math.PI * j * pow2(stage - 1);

        const areal = reals[n];
        const aimag = imags[n];
        const breal = reals[m];
        const bimag = imags[m];
        const wreal = Math.cos(w / size);
        const wimag = Math.sin(w / size);  // Counterclockwise

        if (stage &lt; numberOfStages) {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = (wreal * (areal - breal)) - (wimag * (aimag - bimag));
          imags[m] = (wreal * (aimag - bimag)) + (wimag * (areal - breal));
        } else {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = areal - breal;
          imags[m] = aimag - bimag;
        }
      }
    }
  }

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    const rest = numberOfStages - stage;

    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      indexes[pow2(stage - 1) + i] = indexes[i] + pow2(rest);
    }
  }

  for (let k = 0; k &lt; size; k++) {
    if (indexes[k] &lt;= k) {
      continue;
    }

    const real = reals[indexes[k]];
    const imag = imags[indexes[k]];

    reals[indexes[k]] = reals[k];
    imags[indexes[k]] = imags[k];

    reals[k] = real;
    imags[k] = imag;
  }

  for (let k = 0; k &lt; size; k++) {
    reals[k] /= size;
    imags[k] /= size;
  }
}</code></pre>
              <p>
                FFT と IFFT の実装上の違いは, 回転因子の回転方向が互いに逆なのと, IFFT の場合 <span class="math-inline">$N$</span> 倍された値になるので,
                最後に正規化の処理がある点です.
              </p>
              <article id="section-fast-fourier-transform-algorithms">
                <h6>改良された高速フーリエ変換の実装</h6>
                <p>
                  記載した FFT / IFFT の実装は <b>Cooley-Tukey 型 FFT</b> をもとに, 少し簡素化して理解しやすくした実装です (実際の Cooley-Tukey 型 FFT は,
                  分割統治的な再帰呼び出しであったり, 回転因子の演算を事前にテーブルを確保して計算しておいてより高速化していたりします). また,
                  高速フーリエ変換のアルゴリズム・実装は 1 つではなく, さらに高速化するために基数を利用した実装 (<b>Radix-4 FFT</b>, <b>Radix-8 FFT</b>, さらに,
                  それらを組み合わせた <b>Split-Radix FFT</b> (加算と乗算のトータルの演算回数が最小となる FFT アルゴリズムです) などがあります) や,
                  <a href="https://qiita.com/habuyoshiaki/items/68d502c7bc0c35c168ef" target="_blank" rel="noopener noreferrer"
                    >2 の冪乗でなくても高速フーリエ変換を適用できるアルゴリズム (任意要素数の高速フーリエ変換)</a>
                  が知られています. アプリケーションの要件では, 上記で記載した FFT / IFFT, あるいは, それらを WebAssembly
                  で高速化した実装でも十分かもしれませんが, それ以上のパフォーマンスを必要とする場合は, ぜひ適切な FFT / IFFT
                  のアルゴリズムを調べて実装してみてください.
                </p>
              </article>
            </section>
          </section>
          <figure>
            <img src="images/fourier-transform.png" alt="" width="800" height="600" loading="lazy" />
            <figcaption>フーリエ解析 (フーリエ級数・フーリエ変換) のイメージ</figcaption>
          </figure>
        </section>
        <section id="section-fundamental-frequency-and-harmonic">
          <h3>基本周波数と倍音</h3>
          <p>
            音響特徴量は振幅スペクトルにあらわれることが多いことから, 音の分析, イコール, スペクトル分析と表現しても過言ではないぐらいです. したがって,
            このセクションでは スペクトルの基本構造に関して解説したいと思います.
          </p>
          <p>
            周波数成分は, <b>基本周波数</b> (略して <span class="math-inline">$f_{0}$</span> と呼ぶことも多いです) と<b>倍音</b>に分類することができます.
            最も低い周波数成分を基本周波数と呼び, 基本周波数の整数倍となる周波数成分を倍音と呼びます.
          </p>
          <p>
            <code>OscillatorNode</code> の <code>frequency</code> / <code>detune</code> プロパティは周波数を設定するプロパティ (<code>AudioParam</code>)
            と解説しましたが, 厳密には, <b>基本周波数を設定するプロパティ</b>です.
          </p>
          <p>基本波形を例にとって, 基本周波数と倍音をより具体的に解説します.</p>
          <p>
            基本波形の最小単位は正弦波です. 正弦波は倍音をもちません. 基本周波数の成分しかもたないので<b>純音</b>とも呼ばれます. そして,
            基本周波数と倍音を合成した波形が, 矩形波やノコギリ波, 三角波です.
          </p>
          <table>
            <caption>
              基本波形における, 基本周波数と倍音
            </caption>
            <thead>
              <tr>
                <th scope="col">Wave Type</th>
                <th scope="col">Spectrum</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>正弦波</td>
                <td>基本周波数成分のみをもつ</td>
              </tr>
              <tr>
                <td>矩形波</td>
                <td>基本周波数と奇数次の倍音成分をもつ</td>
              </tr>
              <tr>
                <td>ノコギリ波</td>
                <td>基本周波数と奇数次・偶数次の倍音成分をもつ</td>
              </tr>
              <tr>
                <td>三角波</td>
                <td>基本周波数と奇数次の倍音成分をもつ (高音域の倍音成分が小さい)</td>
              </tr>
            </tbody>
          </table>
          <div class="app-container">
            <dl>
              <dt>Time Domain</dt>
              <dd><svg id="svg-time" class="svg-time" width="720" height="240"></svg></dd>
              <dt>Frequency Domain (Spectrum)</dt>
              <dd><svg id="svg-spectrum" class="svg-spectrum" width="720" height="240"></svg></dd>
            </dl>
            <div>
              <button type="button" id="button-spectrum" class="button-spectrum">start</button>
              <form id="form-oscillator-type-spectrum" class="form-oscillator-type">
                <label><span>sine</span><input type="radio" name="radio-oscillator-type-spectrum" value="sine" checked /></label>
                <label><span>square</span><input type="radio" name="radio-oscillator-type-spectrum" value="square" /></label>
                <label><span>sawtooth</span><input type="radio" name="radio-oscillator-type-spectrum" value="sawtooth" /></label>
                <label><span>triangle</span><input type="radio" name="radio-oscillator-type-spectrum" value="triangle" /></label>
              </form>
              <div class="ranges-oscillator">
                <label><span>gain</span><input type="range" id="range-gain-spectrum" value="1" min="0" max="1" step="0.05" /></label>
                <label><span>frequency</span><input type="range" id="range-frequency-spectrum" value="440" min="27.5" max="1000" step="0.5" /></label>
                <label><span>detune</span><input type="range" id="range-detune-spectrum" value="0" min="-600" max="600" step="1" /></label>
              </div>
            </div>
          </div>
          <p>
            基本波形と同じように, 楽器音や音声も基本周波数と倍音の周波数成分によって構成されています. 厳密には,
            自然の音は必ずしもこのような整数倍になっていません. しかし, 実はこのことが人工的な音と感じさせない要因ともなっています. また, エフェクターの 1
            つである, オーバードライブやディストーションは, 本来発生しない倍音を発生させることによって歪みを与えます
            (オーディオ信号処理における<b>非線形処理</b>によって発生させることができるエフェクターです).
          </p>
          <p>また, ホワイトノイズのような雑音はすべての周波数成分を含んでいるので, そのスペクトルは一様になります.</p>
        </section>
      </section>
      <section id="section-effectors">
        <h2>エフェクター</h2>
        <p>
          このサイトのオーナーはエレキギターを弾くので, オーナー個人的には, オーディオプログラミングの最大の楽しみはエフェクターを実装することだと思っています.
        </p>
        <p>
          Web Audio API のユースケースとしても, エフェクターは考慮されており, <code>GainNode</code>, <code>DelayNode</code>, <code>BiquadFilterNode</code>,
          <code>WaveShaperNode</code>, <code>DynamicsCompressorNode</code> などによって,
          エフェクターの原理さえ簡単に理解していれば実装が容易なぐらいに抽象化されています (エフェクターのためにここまで抽象化されているオーディオ API は,
          現時点でおそらく他にありません).
        </p>
        <section id="section-effectors-overview">
          <h3>エフェクター実装の基本</h3>
          <p>一方で, 抽象化されているがゆえに, Web Audio API でエフェクターを実装する場合に理解しておくべきことが 2 つあります.</p>
          <ul>
            <li>LFO (Low Frequency Oscillator) の実装</li>
            <li><code>AudioParam</code> への接続</li>
          </ul>
          <section id="section-effectors-lfo">
            <h4>LFO (Low Frequency Oscillator)</h4>
            <p>
              エフェクターにはいくつかの種類があり, モジュレーション系と呼ばれるエフェクター (コーラス, フランジャー, フェイザー, トレモロ, ワウなど)
              を実装するためには, <b>特定のパラメータを時間経過とともに周期的に変化させる必要があります</b>. 具体的には, コーラス / フランジャーは,
              ディレイタイム (遅延時間) を時間経過とともに周期的に変化させることによって実装可能です. そして,
              特定のパラメータを時間経過とともに周期的に変化させる機能が <b>LFO</b> (<b>Low Frequency Oscillator</b>) です.
            </p>
            <p>
              LFO の実装は Web Audio API に限ったことではないのですが, <code>OscillatorNode</code> を利用して LFO を実装する場合には, Web Audio API
              特有のことをもう 1 つ理解している必要があります. それが, <code>AudioParam</code> への接続です.
            </p>
          </section>
          <section id="section-effectors-connect-to-audio-param">
            <h4>AudioParam への接続</h4>
            <p>
              結論から記載すると, <code>AudioNode</code> の <code>connect</code> メソッドはオーバーロードされており, 第 1 引数は
              <code>AudioNode</code> インスタンスだけでなく, <code>AudioParam</code> インスタンスを指定することも可能です. つまり,
              <code>OscillatorNode</code> の接続先を <code>AudioParam</code> にすることで,
              対象のパラメータを時間経過とともに周期的に変化させることが可能になります.
            </p>
            <p>
              また, LFO のソースとなる <code>OscillatorNode</code> に <code>GainNode</code> を接続することで, パラメータの変化量を調整することが可能になります.
              一般的なエフェクターのパラメータの, <b>Depth</b> は <code>GainNode</code> の <code>gain</code> プロパティの値に, <b>Rate</b> は
              <code>OscillatorNode</code> の <code>frequency</code> プロパティの値と <code>detune</code> プロパティの値に相当しますプロパティの値に相当します.
            </p>
          </section>
          <section id="section-effectors-vibrato">
            <h4>LFO の実装例 (ビブラート)</h4>
            <p>
              LFO と <code>AudioParam</code> への接続, Depth / Rate の制御を具体的に理解するために, 簡易的なビブラートを実装を記載します. (<code
                >OscillatorNode</code>
              の <code>frequency</code> プロパティのデフォルト値である) <code>440 Hz</code> を基準に, Depth で設定した値が Rate の周期で変化することになります.
              初期値で言うと, <code>440 Hz</code> &plusmn; <code>10 Hz</code> の範囲で, <code>frequency</code> プロパティの値が,
              <code>1 sec</code> の間に変化することになります.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-lfo-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-depth&quot; value=&quot;10&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-depth-value&quot;&gt;10&lt;/span&gt;
&lt;label for=&quot;range-lfo-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-rate&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;10&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-rate-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;
let depth      = null;

let depthValue = 10;
let rateValue  = 1;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDepthElement = document.getElementById(&apos;range-lfo-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-lfo-rate&apos;);

const spanPrintDepthElement = document.getElementById(&apos;print-lfo-depth-value&apos;);
const spanPrintRateElement  = document.getElementById(&apos;print-lfo-rate-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context, { frequency: 440 });
  lfo        = new OscillatorNode(context, { frequency: rateValue });
  depth      = new GainNode(context, { gain: depthValue });

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; OscillatorNode.frequency (AudioParam)
  // 440 Hz +- ${depthValue} Hz
  lfo.connect(depth);
  depth.connect(oscillator.frequency);

  // Start immediately
  oscillator.start(0);

  // Start LFO
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;
  depth      = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthValue = event.currentTarget.valueAsNumber;

  if (depth) {
    depth.gain.value = depthValue;
  }

  spanPrintDepthElement.textContent = Math.trunc(depthValue).toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = Math.trunc(rateValue).toString(10);
});</code></pre>
          </section>
          <section id="section-effectors-general-vibrato">
            <h4>汎用的な LFO と Depth の制御</h4>
            <p>
              基準値と Depth の関係から, パラメータ変化の最小値を考慮しておく必要があるのは, LFO の実装として汎用性に欠けます (先ほどのビブラートの実装だと,
              基準値を <code>27.5 Hz</code> にした場合, Depth の値によっては, 負数の周波数になってしまいます). より汎用的な LFO にするために,
              パラメータの変化量を直接 Depth に設定するのではなく, 基準値に対する変化割合を格納する変数を追加して, その比率と基準値から実際の Depth
              を算出します. このような実装にすることで, 基準値に関わらず, Depth の値は <code>0</code> から <code>1</code> (<code>0 %</code> から
              <code>100 %</code>) になるのでより汎用的な実装になります, また, 各 <code>AudioParam</code> のパラメータの値の範囲 (<code>AudioParam</code> の
              <code>minValue</code> / <code>maxValue</code> プロパティにそれぞれ, 最小値と最大値が設定されています)
              を意図せずに超えてしまうバグも防ぐことができます.
            </p>
            <div class="app-container app-vibrato">
              <button type="button" id="button-vibrato">start</button>
              <label for="range-oscillator-frequency">OscillatorNode frequency</label>
              <input type="range" id="range-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" />
              <span id="print-oscillator-frequency-value">440 Hz</span>
              <label for="range-lfo-depth">Depth</label>
              <input type="range" id="range-lfo-depth" value="0.1" min="0" max="1" step="0.05" />
              <span id="print-lfo-depth-value">0.1</span>
              <label for="range-lfo-rate">Rate</label>
              <input type="range" id="range-lfo-rate" value="1" min="1" max="10" step="1" />
              <span id="print-lfo-rate-value">1</span>
            </div>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-oscillator-frequency&quot;&gt;OscillatorNode frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-oscillator-frequency&quot; value=&quot;440&quot; min=&quot;27.5&quot; max=&quot;4000&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-oscillator-frequency-value&quot;&gt;440&lt;/span&gt;
&lt;label for=&quot;range-lfo-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-depth&quot; value=&quot;0.1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-lfo-depth-value&quot;&gt;10&lt;/span&gt;
&lt;label for=&quot;range-lfo-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-rate&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;10&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-rate-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;
let depth      = null;

let frequency = 440;
let depthRate = 0.1;
let rateValue = 1;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeFrequencyElement = document.getElementById(&apos;range-oscillator-frequency&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-lfo-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-lfo-rate&apos;);

const spanPrintFrequencyElement = document.getElementById(&apos;print-oscillator-frequency-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-lfo-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-lfo-rate-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context, { frequency });
  lfo        = new OscillatorNode(context, { frequency: rateValue });
  depth      = new GainNode(context, { gain: oscillator.frequency.value * depthRate });

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; OscillatorNode.frequency (AudioParam)
  lfo.connect(depth);
  depth.connect(oscillator.frequency);

  // Start immediately
  oscillator.start(0);

  // Start LFO
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;
  depth      = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  frequency = event.currentTarget.valueAsNumber;

  if (oscillator &amp;&amp; depth) {
    oscillator.frequency.value = frequency;
    depth.gain.value           = oscillator.frequency.value * depthRate;
  }

  spanPrintFrequencyElement.textContent = `${(Math.trunc(frequency * 10) / 10)} Hz`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  if (oscillator &amp;&amp; depth) {
    depth.gain.value = oscillator.frequency.value * depthRate;
  }

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = Math.trunc(rateValue).toString(10);
});</code></pre>
          </section>
          <article id="section-effectors-lfo-wave-type">
            <h4>LFO の波形</h4>
            <p>
              ここまでの解説やコードでは, LFO の波形は <code>OscillatorNode</code> のデフォルト値である sin 波 (<code>&apos;sine&apos;</code>) を使っていました.
              LFO としてはこれで十分機能しますが, 波形 (<code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorOptions</code>))
              をノコギリ波や三角波にしても LFO として機能します. また, ランダムなパラメータ変化をさせるためにホワイトノイズを使う場合もあります.
              もし必要であれば, LFO の波形も選択できるようにすると, エフェクトにバリエーションが出せるかもしれません.
            </p>
          </article>
        </section>
        <section id="section-effectors-delay-and-reverb">
          <h3>ディレイ・リバーブ</h3>
          <p>
            ディレイ・リバーブがどんなエフェクターかを簡単に表現すると, ディレイはやまびこ現象を再現するエフェクター, リバーブはコンサートホールなどの (主に,
            室内の) 音の響きを再現するエフェクターとなるでしょう.
          </p>
          <p>
            表現上はまったく別のエフェクターのように思えますが, その原理は同じです (また, ディレイのパラメータの設定しだいでは,
            簡易的なリバーブを再現することも可能です). ディレイ・リバーブともに, <b>FIR フィルタ</b>, つまり,
            加算・乗算・遅延というデジタルフィルタにおける基本処理で実装可能な点です.
          </p>
          <section id="section-effectors-delay">
            <h4>ディレイ</h4>
            <p>ディレイを実装するために必要な処理は, <code>DelayNode</code>の接続とフィードバックです.</p>
            <section id="section-effectors-delay-delay-node">
              <h5>DelayNode</h5>
              <p>
                遅延処理を (抽象化して) 実装するために定義されているのが, <b><code>DelayNode</code></b> です. コンストラクタの第 2 引数 (<b
                  ><code>DelayOptions</code></b>
                の <b><code>maxDelayTime</code></b> プロパティ) には (ファクトリメソッドの場合, 第 1 引数), 遅延時間 (ディレイタイム)
                の最大値を秒単位で指定します. 省略した場合のデフォルト値は <code>1 sec</code> です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { maxDelayTime: 5 });  // 5 sec

// If use `createDelay`
// const delay = context.createDelay(5);  // 5 sec</code></pre>
              <img src="images/delay-node.png" alt="DelayNode and delayTime property" width="1232" height="770" loading="lazy" />
              <p>
                <code>DelayNode</code> インスタンスには, <code>AudioParam</code> である <b><code>delayTime</code></b> プロパティが定義されています. これが,
                遅延時間 (ディレイタイム) を決定づけるプロパティです. 最小値は <code>0 sec</code>, 最大値はインスタンス生成時に指定した値 (sec) です.
              </p>
              <p>遅延した音を生成するには, サウンドの入力点となるノード (<code>OscillatorNode</code> など) を <code>DelayNode</code> に接続します.</p>
              <p>
                以下のコードは, サウンド出力点である <code>AudioDestinationNode</code> に対して 2 つの入力ノードが接続されています. 1 つは, 原音を出力するため,
                そして, もう 1 つは遅延音を出力するためです. このように, 複数のノードを入力ノードとして接続することで,
                それぞれ入力された音をミックスすることが可能になります. これは, ディレイだけではなく他のエフェクターを実装する場合においても,
                <b>原音とエフェクト音をミックスする</b>という処理は必要になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context);

// If use `createDelay`
// const delay = context.createDelay();

delay.delayTime.value = 0.5;

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>
                これで, ディレイが実装できました ... と言いたいところですが, このコードでは, <b>エフェクターとしてのディレイ</b>は実現できていません.
                <b>エフェクターとしての</b>という意味は, 上記のコードでも遅延した音は発生します. しかしながら, やまびこ現象のように,
                遅延音が少しずつ減衰しながら何度も繰り返し生成することが実装できていません.
              </p>
              <p>
                Web Audio API の設計思想からの観点で説明すると,
                <b><code>DelayNode</code> は, 指定された遅延時間で遅延音を 1 つだけ生成することだからです</b>. すなわち,
                エフェクターのディレイを実現するという役割までは担いません.
              </p>
              <p>
                そこで, エフェクターとしてのディレイを実装するには,
                <code>DelayNode</code> の接続と次のセクションで解説する<b>フィードバック</b>という処理が必要になります.
              </p>
            </section>
            <section id="section-effectors-delay-feedback">
              <h5>フィードバック</h5>
              <p>
                <b>フィードバック</b>とは, 出力された音を入力音として利用することです. つまり, <b>DelayNode</b> によって出力された遅延音を,
                再び入力音とすればディレイを実現することが可能です. これを, Web Audio APIで実装するには,
                <b>フィードバックのための <code>GainNode</code></b> を接続して, その入出力に同じ <code>DelayNode</code> を接続します. また,
                フィードバックの実装は, ディレイに限らず, 様々なエフェクターで必要となります.
              </p>
              <p>
                ちなみに, エレキギターではフィードバック奏法と呼ばれる奏法があります. この奏法は, アンプから出力された音で弦を振動させて,
                それをピックアップが拾い, 再びアンプから出力させることで, 理論上, 永遠の音の伸びを奏でる奏法です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context);

// If use `createDelay`
// const delay = context.createDelay();

delay.delayTime.value = 0.5;

const feedback = new GainNode(context, { gain: 0.5 });

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

// Connect nodes for feedback
// (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
delay.connect(feedback);
feedback.connect(delay);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>上記のコードでは, 大きく 3 つの接続ができました.</p>
              <ul>
                <li>原音を出力する接続</li>
                <li>エフェクト音 (遅延音) を出力する接続</li>
                <li>フィードバック (エフェクト音を再び入力する) のための接続</li>
              </ul>
              <p>
                フィードバック (エフェクト音を再び入力する) の接続によって, 遅延音が少しずつ減衰しながら何度も繰り返し生成されます. 具体例として, 原音のゲインを
                <code>1</code>, フィードバッグのゲインが <code>0.5</code> とすると, 1 つめの遅延音のゲインは, <code>0.5</code> (<code>1 x 0.5</code>), 2
                つめの遅延音のゲインは, <code>0.25</code> (<code>0.5 x 0.5</code>), 3 つめの遅延音のゲインは, <code>0.125</code> (<code>0.25 x 0.5</code>) ...
                といった繰り返しで, 遅延音が少しずつ減衰しながら生成されます. つまり, フィードバックの <code>GainNode</code> の
                <code>gain</code> プロパティを適切に設定すれば, エフェクターとしてのディレイにバリエーションが出せるというこでもあります.
              </p>
              <p>
                ただし, <b>フィードバックの値は <code>1</code> 未満にする必要があります</b>. これは, 直感的な説明をすれば,
                <code>1</code> 以上にすると減衰しない状態 (無限ループのような状態) になってしまうからです. 数学的・工学的な厳密性で説明すると,
                <a href="#section-fourier-transform">絶対可積分</a>を満たさなくなり, (のちのセクションで解説する) FIR
                フィルタが安定しないフィルタとなるからです.
              </p>
              <figure>
                <svg id="svg-animation-feedback" width="720" height="405" data-parameters="true" />
                <figcaption>
                  <span>フィードバックのイメージ</span>
                  <button type="button" id="button-feedback-animation">start</button>
                </figcaption>
              </figure>
            </section>
            <section id="section-effectors-delay-dry-and-wet">
              <h5>Dry / Wet</h5>
              <p>
                <b>Dry</b> / <b>Wet</b> とは, <b>原音とエフェクト音のゲインを調整するパラメータ</b> (または, そのような機能) のことです.
                現実世界のエフェクターにおいても, Dry (原音) / Wet (エフェクト音) としてコントロール可能になっているものが多いので,
                このドキュメントでもそれに従って Dry / Wet (あるいは, それらを合わせる Mix) と呼ぶことにします.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { delayTime: 0.5 });

// If use `createDelay`
// const delay = context.createDelay();
// delay.delayTime.value = 0.5;

const dry      = new GainNode(context, { gain: 0.7 });  // for gain of original sound
const wet      = new GainNode(context, { gain: 0.3 });  // for gain of delay sound
const feedback = new GainNode(context, { gain: 0.5 });  // for feedback

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(wet);
wet.connect(context.destination);

// Connect nodes for feedback
// (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
delay.connect(feedback);
feedback.connect(delay);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>
                上記のコードは, Dry / Wet のための <code>GainNode</code> を接続して, ディレイの実装完成コードです. Dry / Wet の制御を可能にすることで,
                原音とエフェクト音のゲインを調整するだけで, ディレイにさらなるバリエーションが生まれます. さらに, ノード接続を変更することなくエフェクターのオン
                / オフを切り替えることが可能になります. 例えば, Dry を <code>1</code>, Wet を <code>0</code> にすれば, エフェクターオフ (つまり, 原音のみ)
                のサウンドになります.
              </p>
              <figure>
                <svg id="svg-figure-node-connections-for-delay" width="1200" height="520" />
                <figcaption>ディレイのノード接続図</figcaption>
              </figure>
              <p>
                フィードバックと同様に, Dry / Web (あるいは, Mix) のパラメータ制御は, ディレイだけではなく, 他のエフェクターでも利用されるので, まずは,
                ノード接続が比較的単純なディレイの実装でそれらを理解しておくとよいでしょう.
              </p>
            </section>
            <article id="section-effectors-delay-ring-buffer">
              <h5>遅延音の実装とリングバッファ</h5>
              <p>
                ディレイはエフェクター実装における基本を理解するためにも最適なエフェクターですが, ディレイのコアとなる,
                遅延音はどのようにして実装するのでしょうか ? (Web Audio API では,
                <code>DelayNode</code> がそれを抽象化しているので気にすることはほとんどないと思いますが). 結論的には,
                <b>リングバッファ</b>に入力された音を格納する (enqueue) ことで, 過去の入力音をリングバッファのサイズだけ蓄積する事が可能です (<code
                  >DelayNode</code>
                コンストラクタで指定する遅延時間の最大値は, このリングバッファのサイズを決定するためにあります). 指定した
                <code>delayTime</code> の値が経過したら, リングバッファに格納した過去の入力音を, 原音 (現在時刻の音) と合成して出力します.
                加算・乗算・遅延はデジタルフィルタを構成する基本処理なので, 遅延に関しても,
                ローレイヤーでどのように実装されているかを理解しておくと応用が利くはずです. 実装言語は C++ になりますが, より詳細な解説は,
                <a href="https://www.utsbox.com/?p=1517" target="_blank" rel="noopener noreferrer">ディレイの実装例 | C++でVST作り</a>を参考にするとよいと思います.
              </p>
            </article>
          </section>
          <section id="section-effectors-reverb">
            <h4>リバーブ</h4>
            <p>
              簡易的なリバーブであれば, ディレイのパラメータを適切に設定することで実装することは可能です. しかしながら, 現実世界のエフェクターのリバーブは,
              ディレイと実装は異なり, シミュレートしたい<b>音響空間のインパルス応答</b> (<b>RIR</b>: <b>Room Impulse Response</b>)
              と呼ばれるオーディオデータを利用します. インパルス応答の詳細に関しては, 後半のセクションで解説しますので, とりあえず,
              リバーブを実装してみましょう.
            </p>
            <section id="section-effectors-reverb-convolver-node">
              <h5>ConvolverNode</h5>
              <p>
                インパルス応答のオーディオデータをエフェクターとして利用するには, <b><code>ConvolverNode</code></b> を利用します. <code>ConvolverNode</code> は,
                <b>コンボリューション積分</b> (<b>畳み込み積分</b>, <b>合成積</b>) という数学的な演算を抽象化する <code>AudioNode</code> です
                (デジタルフィルタの視点では, <b>FIR フィルタ</b>を抽象化します. のちほど解説しますが, 見立ての違いであり, 本質的には, コンボリューション積分も
                FIR フィルタも同じです. コンボリューション積分も FIR フィルタも次のセクション以降で解説しています).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const convolver = new ConvolverNode(context);

// If use `createConvolver`
// const convolver = context.createConvolver();</code></pre>
              <img src="images/convolver-node.png" alt="ConvolverNode" width="1232" height="770" loading="lazy" />
              <p>
                <code>ConvolverNode</code> には, <b><code>buffer</code></b> プロパティが定義されており, この <code>buffer</code> プロパティに, インパルス応答
                (RIR) のオーディオデータの <code>AudioBuffer</code> インスタンスを設定します (コンストラクタ生成であれば,
                <b><code>ConvolverOptions</code></b> の <code>buffer</code> プロパティで設定することも可能です).
              </p>
              <p><code>AudioBuffer</code> インスタンスの生成は, <a href="#section-create-audio-buffer">ワンショットオーディオ</a>と同じです.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/rirs/rir.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const convolver = new ConvolverNode(context);

      // If use `ConvolverOptions`
      // const convolver = new ConvolverNode(context, { buffer: audioBuffer });

      // If use `createConvolver`
      // const convolver = context.createConvolver();

      convolver.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                サウンドの入力点となるノード (<code>OscillatorNode</code> など) を <code>ConvolverNode</code> に接続して, <code>ConvolverNode</code> を
                <code>AudioDestinationNode</code> に接続します.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/rirs/rir.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const convolver = new ConvolverNode(context);

      // If use `ConvolverOptions`
      // const convolver = new ConvolverNode(context, { buffer: audioBuffer });

      // If use `createConvolver`
      // const convolver = context.createConvolver();

      convolver.buffer = audioBuffer;

      const dry = new GainNode(context, { gain: 0.6 });  // for gain of original sound
      const wet = new GainNode(context, { gain: 0.4 });  // for gain of reverb sound

      const oscillator = new OscillatorNode(context);

      // Connect nodes for original sound
      // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
      oscillator.connect(dry);
      dry.connect(context.destination);

      // Connect nodes for reverb sound
      // OscillatorNode (Input) -&gt; ConvolverNode (Reverb) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
      oscillator.connect(convolver);
      convolver.connect(wet);
      wet.connect(context.destination);

      oscillator.start(0);
      oscillator.stop(context.currentTime + 5);
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                ディレイの場合と同じように, 原音とエフェクト音の接続, そして, Dry / Wet の <code>GainNode</code> も接続しています.
                遅延音の生成処理と遅延音に対する演算処理は, <code>ConvolverNode</code> が抽象化しているので, フィードバックのための接続は不要です.
              </p>
              <figure>
                <svg id="svg-figure-node-connections-for-reverb" width="800" height="520" />
                <figcaption>リバーブのノード接続図</figcaption>
              </figure>
              <p>
                以上で, リバーブが完成しました. ところで, リバーブを利用するためには, インパルス応答 (RIR) のオーディオファイルが必要です.
                楽曲やワンショットオーディオファイルはあっても, インパルス応答のオーディオデータをもっている人は少ないと思います.
                機材をもっていれば実際に測定するのも可能ですが, そこまでするのはちょっとめんどうです. となると, Web
                上で公開されているファイルを利用することになるのですが, 利用条件や著作権の関係から無償で自由に利用できるのは意外とありません. とりあえず, 1
                つ紹介するのは,
                <a href="http://legacy.spa.aalto.fi/projects/poririrs/" target="_blank" rel="noopener noreferrer"
                  >Concert Hall Impulse Responses - Pori, Finland</a>
                です. <a href="http://legacy.spa.aalto.fi/projects/poririrs/docs/readme.txt" target="_blank" rel="noopener noreferrer">readme.txt</a> の 5.
                Copyright のセクションに, 「<b
                  >The data are provided free for noncommercial purposes, provided the authors are cited when the data are used in any research application.</b>」と記載されているので, 非商用利用であれば, 自身が開発されている Web アプリケーションに利用しても問題なさそうです.
              </p>
            </section>
            <section id="section-effectors-reverb-rir">
              <h5>インパルス応答</h5>
              <p>
                インパルス応答を理解するには, まずは, <b>インパルス音</b>について理解する必要があります. インパルス音とは,
                ごく短時間において瞬間的に発生する音です. 具体的には, ピストルの音や風船が破裂するときの音がインパルス音と言えるでしょう.
                インパルス音のイメージは以下のグラフのようになります. このような物理現象を数式でモデリングするための最適な関数が, <b>デルタ関数</b>です.
              </p>
              <figure>
                <svg id="svg-figure-impulse" width="720" height="405" data-parameters="true" />
                <figcaption>インパルス音 (デルタ関数) のグラフ表現</figcaption>
              </figure>
              <p>
                以下は, デルタ関数の定義です. デルタ関数は, <span class="math-inline">$t = t_{r}$</span> において, 横幅が
                <span class="math-inline">$\lim_{\mathrm{w}\to 0}$</span>, 高さが <span class="math-inline">$\lim_{\mathrm{h}\to \infty}$</span> となる関数
                (数学での関数をより一般化した, <b>超関数</b>に分類されます) なので, 無限の区間において積分すると, その値 (つまり, 面積) が
                <code>1</code> となります.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &\delta\left(t - t_{r}\right)dt =
                    \begin{cases}
                      \infty & (\mathrm{if} \quad t = t_{r}) \\
                      0 & (\mathrm{if} \quad t \neq t_{r})
                    \end{cases}
                  \end{flalign}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &\int_{-\infty}^{\infty}\delta\left(t - t_{r}\right)dt = 1 \\
                  \end{flalign}
                $
              </div>
              <p>
                インパルス音を室内で発生させると, 音が天井や壁などに反射して, 音の響き, すなわち, <b>残響</b>が発生します. カラオケが好きなかたであれば,
                <b>エコー</b>のような効果と考えてもよいでしょう.
              </p>
              <figure>
                <img src="images/reverb.png" alt="" width="800" height="600" loading="lazy" />
                <figcaption>直接音以外にも反射音 (初期反射音) や何度も反射して聴こえる残響音 (後期残響音) が室内では発生します</figcaption>
              </figure>
              <figure>
                <svg id="svg-animation-impulse-response" width="720" height="405" data-parameters="true" />
                <figcaption>
                  <span>インパルス応答のイメージ</span>
                  <button type="button" id="button-impulse-response-animation">start</button>
                </figcaption>
              </figure>
              <p>
                これが, <b>インパルス応答</b>です. つまり, <b>インパルス音を音響空間に対する入力としたときの出力 (応答) </b>ということです. そして, その出力が<b
                  >音響空間における, 残響特性を表すことになるので</b>, インパルス応答を利用することによって,
                コンサートホールなどの音響空間の音の響きをシミュレートするエフェクターであるリバーブが実現できるというわけです. ちなみに,
                室内でのインパルス応答をシミュレートする事が多いので, <b>RIR</b> (Room Impulse Response) と表現されることもあります.
              </p>
              <figure>
                <svg id="svg-rir" width="720" height="405" data-parameters="true" data-a="1" />
                <figcaption>
                  <span>実際の RIR の波形</span>
                  <button type="button" id="button-rir">start</button>
                </figcaption>
              </figure>
              <p>
                また, この残響が, ディレイにおけるフィードバックと類似した音響効果を発生させることになります
                (フィードバックのアニメーションとインパルス応答のアニメーションが類似していることに気付いたかもしれません).
              </p>
              <p>
                イメージでインパルス応答は理解できるかと思いますが, それをコンピュータで実現する場合, 数式でモデリングできる必要があります.
                <code>ConvolverNode</code> の命名 (<b>Convolve</b>: 畳み込む) が表すように, その演算処理が<b>コンボリューション積分</b> (<b>畳み込み積分</b>,
                <b>合成積</b>) です. 言い換えると, <code>ConvolverNode</code> は, コンボリューション積分を抽象化した <code>AudioNode</code> です.
              </p>
            </section>
          </section>
          <section id="section-effectors-delay-and-reverb-convolution">
            <h4>コンボリューション積分</h4>
            <p>
              <b>コンボリューション積分</b>とは, 以下の数式で定義されるように, 信号の遅延と乗算, それらの無限区間の積分によって構成されています.
              <span class="math-inline">$x\left(t\right)$</span> は入力信号, <span class="math-inline">$y\left(t\right)$</span> は出力信号,
              <span class="math-inline">$h\left(t_{r}\right)$</span> は, インパルス応答の信号 (<code>ConvolverNode</code> の
              <code>buffer</code> プロパティに設定する, <code>AudioBuffer</code> のオーディオデータと考えてもよいでしょう) です.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(t\right) = \int_{0}^{\infty}x\left(t - t_{r}\right)h\left(t_{r}\right)dt
              \end{flalign}
              $
            </div>
            <p>
              コンピュータでは, 連続信号をあつかうことはできないので, サンプリングされた離散信号の遅延と乗算, それらの加算となります. また,
              無限の加算はできないので, 有界な値 (サンプル数) <span class="math-inline">$N$</span> となります.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right)
              \end{flalign}
              $
            </div>
            <p>具体的に理解するために, <span class="math-inline">$N = 5$</span> として, 展開してみます.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(n\right) = x\left(n\right)h\left(0\right) + x\left(n - 1\right)h\left(1\right) + x\left(n - 2\right)h\left(2\right) + x\left(n - 3\right)h\left(3\right) + x\left(n - 4\right)h\left(4\right) + x\left(n - 5\right)h\left(5\right)
              \end{flalign}
              $
            </div>
            <p>
              理解しやすいように, 入力信号 <span class="math-inline">$x\left(n\right)$</span> は, 振幅が <code>1</code> のパルス列とします. また,
              出力信号の実際の値を算出するために, インパルス応答の信号は, 以下の値とします.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &h\left(0\right) = 1.0    \\
                &h\left(1\right) = 0.75   \\
                &h\left(2\right) = 0.5    \\
                &h\left(3\right) = 0.25   \\
                &h\left(4\right) = 0.125  \\
                &h\left(5\right) = 0.0625 \\
              \end{flalign}
              $
            </div>
            <p>これらの信号のコンボリューション積分をイラストにすると, 以下のようなグラフになります.</p>
            <figure>
              <svg id="svg-figure-convolution" width="720" height="600" />
              <figcaption>コンボリューション積分</figcaption>
            </figure>
            <p>
              上部が入力信号のパルス列 (<span class="math-inline">$x\left(n\right)$</span>), 中央がインパルス応答 (<span class="math-inline"
                >$h\left(m\right)$</span>), 下部がコンボリューション積分結果の出力信号 (<span class="math-inline">$y\left(n\right)$</span>) となります.
              出力信号の振幅のみスケールが異なることに注意してください. ここで, <span class="math-inline">$n = 3$</span> に相当する時刻までの値を,
              先ほどの展開したコンボリューション積分に適用して実際の値を算出すると,
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = x\left(0\right)h\left(0\right) + x\left(-1\right)h\left(1\right) + x\left(-2\right)h\left(2\right) + x\left(-3\right)h\left(3\right) + x\left(-4\right)h\left(4\right) + x\left(-5\right)h\left(5\right) \\
                &y\left(1\right) = x\left(1\right)h\left(0\right) + x\left(0\right)h\left(1\right) + x\left(-1\right)h\left(2\right) + x\left(-2\right)h\left(3\right) + x\left(-3\right)h\left(4\right) + x\left(-4\right)h\left(5\right) \\
                &y\left(2\right) = x\left(2\right)h\left(0\right) + x\left(1\right)h\left(1\right) + x\left(0\right)h\left(2\right) + x\left(-1\right)h\left(3\right) + x\left(-2\right)h\left(4\right) + x\left(-3\right)h\left(5\right) \\
                &y\left(3\right) = x\left(3\right)h\left(0\right) + x\left(2\right)h\left(1\right) + x\left(1\right)h\left(2\right) + x\left(0\right)h\left(3\right) + x\left(-1\right)h\left(4\right) + x\left(-2\right)h\left(5\right) \\
              \end{flalign}
              $
            </div>
            <p>負数に相当する時刻は, 振幅が <code>0</code> とみなせるので, <span class="math-inline">$n \geq 0$</span> の項のみ記述すると,</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = x\left(0\right)h\left(0\right) \\
                &y\left(1\right) = x\left(1\right)h\left(0\right) + x\left(0\right)h\left(1\right) \\
                &y\left(2\right) = x\left(2\right)h\left(0\right) + x\left(1\right)h\left(1\right) + x\left(0\right)h\left(2\right) \\
                &y\left(3\right) = x\left(3\right)h\left(0\right) + x\left(2\right)h\left(1\right) + x\left(1\right)h\left(2\right) + x\left(0\right)h\left(3\right) \\
              \end{flalign}
              $
            </div>
            <p>また, 入力信号は, 振幅 <code>1</code> のパルス列なので, <span class="math-inline">$x\left(n\right) = 1$</span> となるので,</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = h\left(0\right) \\
                &y\left(1\right) = h\left(0\right) + h\left(1\right) \\
                &y\left(2\right) = h\left(0\right) + h\left(1\right) + h\left(2\right) \\
                &y\left(3\right) = h\left(0\right) + h\left(1\right) + h\left(2\right) + h\left(3\right) \\
              \end{flalign}
              $
            </div>
            <p>
              最後に, <span class="math-inline">$h\left(m\right)$</span> の実際の値を適用すれば, 出力信号 <span class="math-inline">$y\left(n\right)$</span> の
              <span class="math-inline">$n = 3$</span> までの値が算出できます.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = 1.0 \\
                &y\left(1\right) = 1.0 + 0.75 = 1.75 \\
                &y\left(2\right) = 1.0 + 0.75 + 0.5 = 2.25 \\
                &y\left(3\right) = 1.0 + 0.75 + 0.5 + 0.25 = 2.5 \\
              \end{flalign}
              $
            </div>
            <p>
              目視レベルではありますが, グラフで表示されている出力信号と (大きく) 値の相違がないことが確認できます. 計算結果を抽象化すると,
              コンボリューション積分で生成された出力信号は, <b>現在の時刻の信号だけではなく, 過去の信号の影響も受ける</b>ということですということです (逆に,
              それを, 厳密に数式で定義したのがコンボリューション積分と言えます).
            </p>
            <figure>
              <svg id="svg-animation-convolution" width="720" height="600" />
              <figcaption>
                <span><span class="math-inline">$N = 5$</span>, <span class="math-inline">$n = 3$</span> の場合の, コンボリューション積分のイメージ</span>
                <button type="button" id="button-convolution-animation">start</button>
              </figcaption>
            </figure>
            <p>
              入力信号やインパルス応答が, 現実世界のようにより複雑になると, 計算自体も複雑になりますが, コンボリューション積分がどうのような演算か, そして,
              <b><code>ConvolverNode</code> が抽象化している演算</b>のイメージを理解するのに役立てばと思います.
            </p>
            <p>
              もっとも, 数学・物理的に理解しようとすると少し難しく感じるかもしれませんが, プログラミング言語で実装すれば, 2
              重ループと積和演算で構成される単純な処理です.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// コンボリューション積分のコード片

const X_L = 2400;
const H_L = 1200;
const Y_L = X_L + H_L - 1;

const x = new Float32Array(X_L);
const h = new Float32Array(H_L);
const y = new Float32Array(Y_L);

// 入力信号
for (let n = 0; n &lt; X_L; n++) {
  x[n] = Math.sin((2 * Math.PI * n * 2) / 1200);
}

// インパルス応答
for (let m = 0; m &lt; H_L; m++) {
  h[m] = 0.5 * Math.exp(-m);
}

// 出力信号
for (let n = 0; n &lt; Y_L; n++) {
  for (let m = 0; m &lt; H_L; m++) {
    if (((n - m) &gt;= 0) &amp;&amp; ((n - m) &lt; X_L)) {
      y[n] += (x[n - m] * h[m]);
    }
  }
}</code></pre>
            <p>
              ただし, コンボリューション積分した出力信号のサイズ <span class="math-inline">$Y_{L}$</span> は, 入力信号のサイズを
              <span class="math-inline">$X_{L}$</span>, インパルス応答のサイズを <span class="math-inline">$H_{L}$</span> とすると,
              <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> (つまり, 出力信号の配列 (コレクション) の末尾のインデックスは,
              <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span>) となることに注意してください. 現在時刻以降の入力信号が
              <code>0</code> (無音) でも, コンボリューション積分によって過去の信号の積和演算の影響を受けるからです.
              信号の末尾をイラストにイメージすると理解しやすいと思います.
            </p>
            <figure>
              <svg id="svg-figure-convolution-output-signal-size" width="720" height="600" />
              <figcaption>コンポリューション積分と出力信号のサイズ (末尾部分)</figcaption>
            </figure>
            <p>
              上記のイラストは, インパルス応答のサイズを <code>3</code> (<span class="math-inline">$H_{L} = 3$</span>) とした場合の, 出力信号の末尾の値
              (インデックス <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span> の値) の算出イメージです.
              過去の入力信号の積和演算の影響で, 出力信号のサイズが <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> となることが確認できると思います
              (出力信号の末尾から <span class="math-inline">$j$</span> 手前のインデックス
              <span class="math-inline">$I_{\mathrm{max} - j} = (X_{L} + H_{L} - 1) - (1 + j)$</span> の値も同様に,
              過去の入力信号の積和演算によって算出されます). そして, インデックス
              <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span> より後の時刻に相当するインデックスにおいては,
              過去の入力信号の積和演算の値が常に <code>0</code> (無音) となるので, 出力信号のサイズは,
              <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> より大きいサイズにはならないことも確認できると思います.
            </p>
            <article id="section-effectors-delay-and-reverb-cyclic-convolution">
              <h5>巡回畳み込み</h5>
              <p>
                コンボリューション積分は, <b>周波数領域においては乗算</b>となります. したがって, 時間領域の入力信号
                <span class="math-inline">$x\left(n\right)$</span> を (離散) フーリエ変換した信号を <span class="math-inline">$X\left(k\right)$</span>,
                インパルス応答を (離散) フーリエ変換した信号を <span class="math-inline">$H\left(k\right)$</span>, 出力信号
                <span class="math-inline">$y\left(n\right)$</span> を (離散) フーリエ変換した信号を
                <span class="math-inline">$Y\left(k\right)$</span> と定義すると, コンボリューション積分は以下のように定義することもできます,
                <span class="math-inline">$F$</span> はフーリエ変換 (離散フーリエ変換), <span class="math-inline">$F^{-1}$</span> は逆フーリエ変換
                (逆離散フーリエ変換) です.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right) = F^{-1}\left[Y\left(k\right)\right] = F^{-1}\left[X\left(k\right)H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                この性質を利用して, 時間領域ではコンボリューション積分になるオーディオ信号処理を, 周波数領域に変換して, 乗算のみで信号処理を適用して,
                時間領域に変換する<b>巡回畳み込み</b>という処理 (ある種のテクニック的な処理です) があります.
              </p>
              <p>
                インパルス応答も数学的には, デルタ関数のフーリエ変換と周波数領域で定義される<b>伝達関数</b>の乗算の逆フーリエ変換で定義することもできます (以下,
                簡単にですが, 導出を記載しておきます).
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &h\left(n\right) = \sum_{m = 0}^{\infty}\delta\left(n - m\right)h\left(m\right) = F^{-1}\left[F\left[\delta\left(n\right)\right]H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                ここで, デルタ関数のフーリエ変換を導出すると (導出の理解が難しければ, とりあえず
                <span class="math-inline">$F\left[\delta\left(t\right)\right] = 1$</span> と覚えてしまっていいでしょう. 他に覚えやすいフーリエ変換関係だと,
                矩形関数とシンク関数 (<span class="math-inline">$\frac{\sin\left(x\right)}{x}$</span>) は, 互いにフーリエ変換の関係にあります),
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &F\left[\delta\left(t\right)\right] = \int_{-\infty}^{\infty}\delta\left(t\right)e^{-j2 \pi ft}dt
                  \end{flalign}
                $
              </div>
              <p>
                デルタ関数の定義より, 上記のフーリエ変換において, <span class="math-inline">$t = 0$</span> 以外の積分区間では
                <span class="math-inline">$\delta\left(t\right) = 0$</span> となるので,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &F\left[\delta\left(t\right)\right] = \int_{-\infty}^{\infty}\delta\left(0\right)e^{-j2 \pi f \cdot 0}dt = \int_{-\infty}^{\infty}\delta\left(0\right)dt \cdot e^{0} = 1 \cdot 1 = 1
                  \end{flalign}
                $
              </div>
              <p>
                (数学的な厳密性は欠いてしまいますが, 同じように離散信号にも適用すると) デルタ関数のフーリエ変換は <code>1</code> です. つまり,
                <span class="math-inline">$F\left[\delta\left(n\right)\right] = 1$</span> なので,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &h\left(n\right) = \sum_{m = 0}^{\infty}\delta\left(n - m\right)h\left(m\right) = F^{-1}\left[F\left[\delta\left(n\right)\right]H\left(k\right)\right] = F^{-1}\left[H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                Web Audio API は (他のオーディオ API と比較すると) 抽象度が高いので, 巡回畳み込みまで駆使するケースはほとんどないかもしれませんが,
                一応知っておくと実装のヒントになることがあるかもしれません.
              </p>
            </article>
          </section>
          <section id="section-effectors-delay-and-reverb-fir-filter">
            <h4>FIR フィルタ</h4>
            <p>
              デジタルフィルタを数学的な厳密性まで含めて解説すると, それだけで 1 冊の書籍になるぐらいの解説になるので, FIR
              フィルタをディレイ・リバーブの観点で解説します.
            </p>
            <p><b>FIR フィルタ</b> (<b>Finite Impulse Response filter</b>) は, 以下の数式で定義されるデジタルフィルタです.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right)
                \end{flalign}
              $
            </div>
            <p>
              数式的には, コンボリューション積分と同じです. すなわち, <b>見立ての違い</b>でしかありません. 数学的にはコンボリューション積分, 工学的には FIR
              フィルタと表現しています. 言い換えると, コンボリューション積分を実装に落とし込んだのが FIR フィルタということです.
            </p>
            <p>
              具体的に, <span class="math-inline">$N = 3$</span> (乗算器の数 <span class="math-inline">$N + 1$</span>)
              の加算器・乗算器・遅延器の要素を利用してブロック図として表現します (遅延器の <span class="math-inline">$z^{-1}$</span> の表記は
              <b><span class="math-inline">$z$</span> 変換</b>に由来しますが, ブロック図としては, 1 サンプル分遅延させる要素 (素子) の理解で問題ありません).
            </p>
            <figure>
              <svg id="svg-figure-fir-filter" width="720" height="520" />
              <figcaption>FIR フィルタ</figcaption>
            </figure>
            <p>FIR フィルタの図から, 逆に, コンボリューション積分の数式を導出することが可能なこともわかります.</p>
            <p>
              乗算器の数 (遅延器の数もそれに比例) と係数は, ディレイは制御可能なパラメータで決定できますが, リバーブは室内の特性によって決定されます. すなわち,
              <b>乗算器の数と係数の算出がディレイとリバーブの実装に違いに表れます</b>.
              <b>ディレイは遅延時間とフィードバックよって乗算器の数と係数を決定する</b>のに対して,
              <b>リバーブはインパルス応答 (RIR) のオーディオデータから乗算器の数と係数が決定されます</b>.
            </p>
            <article id="section-effectors-delay-and-reverb-schroeder-reverberator">
              <h5>Schroeder Reverberator (シュレーダーリバーブ)</h5>
              <p>
                コムフィルタ (遅延音を生成) と All-Pass Filter (位相変化によって, 遅延音の間を補間して残響音の密度を高める) を駆使して, 人工的にリバーブ
                (人工インパルス応答) を生成する実装が知られています. 実装言語は Python になりますが,
                <a href="https://www.wizard-notes.com/entry/asp/schroeder-reverberator" target="_blank" rel="noopener noreferrer"
                  >シュレーダーリバーブ（人工残響エフェクタ）のPython実装と試聴デモ</a>などが参考になります.
              </p>
            </article>
          </section>
        </section>
        <section id="section-effectors-chorus-and-flanger">
          <h3>コーラス・フランジャー</h3>
          <p>
            <b>コーラス</b>は, 音に揺らぎを与えるエフェクターです. 合唱では, どんなに歌唱力の高い人が集まって歌っても多少なりともピッチのずれは生じてしまいます.
            しかし, この微妙なずれが合唱らしさを生み出している要因でもあります. コーラスでは, この微妙なピッチのずれをオーディオ信号処理で再現します.
          </p>
          <p><b>フランジャー</b>は, ジェット機のエンジン音のように, 音に強烈なうねりを与えるエフェクターです.</p>
          <p>
            コーラスとフランジャーは, エフェクターとしてはまったく異なるように感じます. 現実世界のエフェクターでも,
            コーラスとフランジャーはそれぞれ別に存在していますが, その原理は同じです. <b>ディレイタイム (遅延時間) を周期的に変化させる</b>ことによって,
            <b>FM 変調</b>を発生させている点です. 原理が同じであるにも関わらず, 別のエフェクターとして感じるのは,
            パラメータの設定値やフィードバックの有無が影響しています (また, 音楽的に目的が異なるので, それぞれ,
            コーラス・フランジャーとして別になっていると思います).
          </p>
          <p>
            コーラス・フランジャーはディレイが基本となっているので, ディレイの実装がよくわからないという場合は, (前のセクション解説しているので)
            ディレイの実装を理解してから, このセクションを進めてください.
          </p>
          <section id="section-effectors-chorus">
            <h4>コーラス</h4>
            <p>
              コーラスは, <b>ディレイタイムを周期的に変化させたエフェクト音</b>を原音とミックスすることにより実装できます.
              ディレイタイムを周期的に変化させることが実装のポイントになりますが, ここで LFO を利用することで, ディレイタイムを周期的に変化させることができます.
              つまり, Web Audio API においては, LFO の接続先を <code>DelayNode</code> の AudioParam である
              <code>delayTime</code> プロパティに接続すれば実装完了です.
            </p>
            <p>まず, 原音の出力の接続と, エフェクト音の出力の接続 (<code>DelayNode</code> の接続) のみを実装します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { delayTime: 0.02 });

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
            <p>
              そして, <a href="#section-effectors-lfo">LFO</a> の実装で解説したように, LFO のための <code>OscillatorNode</code> インスタンスと
              <code>GainNode</code> インスタンス (Depth パラメータ) を生成して, <b><code>DelayNode</code> の <code>delayTime</code> プロパティ</b> に接続します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseDelayTime = 0.020;
const depthValue    = 0.005;
const rateValue     = 1;

const delay = new DelayNode(context, { delayTime: baseDelayTime });

const oscillator = new OscillatorNode(context);

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

// Connect nodes for LFO that changes delay time periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
lfo.connect(depth);
depth.connect(delay.delayTime);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              ディレイのノード接続からフィードバックを除いて, LFO を <code>DelayNode</code> の <code>delayTime</code> プロパティ (<code>AudioParam</code>)
              に接続したノード接続と同じです. パラメータに関しては, コーラスの場合, 基準となるディレイタイムを <code>20 - 30 msec</code> にして,
              <code>&plusmn; 5 ~ 10 msec</code>, Rate はゆっくりと <code>1 Hz</code> ぐらいがよいでしょう. (もっとも, 実際のプロダクトでは,
              ある程度自由度高く設定できるように, <a href="#section-effectors-general-vibrato">汎用的な LFO</a> になるように実装することになるでしょう).
            </p>
            <p>
              コーラスの原理的な実装としてはこれで完了ですが, エフェクターとしてはまだコーラスっぽくありません.
              原音とエフェクト音が同じゲインで合成されているので, 原音とエフェクト音が別々に出力されているように聴こえると思います.
              原音が少し揺れているぐらいにエフェクト音を合成するとコーラスらしくなるので, Dry / Wet のための <code>GainNode</code> を接続して,
              原音とエフェクト音のゲインを調整します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseDelayTime = 0.020;
const depthValue    = 0.005;
const rateValue     = 1;

const delay = new DelayNode(context, { delayTime: baseDelayTime });

const oscillator = new OscillatorNode(context);

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

const dry = new GainNode(context, { gain: 0.7 });  // for gain of original sound
const wet = new GainNode(context, { gain: 0.3 });  // for gain of chorus sound

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(wet);
wet.connect(context.destination);

// Connect nodes for LFO that changes delay time periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
lfo.connect(depth);
depth.connect(delay.delayTime);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <figure>
              <svg id="svg-figure-node-connections-for-chorus" width="1200" height="520" />
              <figcaption>コーラスのノード接続図</figcaption>
            </figure>
            <p>
              以上で, コーラスの実装は完了です. ハードコーディングしているパラメータが多いので, ある程度実際のアプリケーションを想定して, UI
              からパラメータ設定を可能にすると以下のようなコードとなるでしょう (Dry / Wet は同時に設定する Mix としています).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-chorus-delay-time&quot;&gt;Delay time&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-delay-time&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-chorus-delay-time-value&quot;&gt;0 msec&lt;/span&gt;
&lt;label for=&quot;range-chorus-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-chorus-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-chorus-mix&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-mix&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-mix-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;

let depthRate  = 0;
let rateValue  = 0;
let mixValue   = 0;

const delay = new DelayNode(context);
const depth = new GainNode(context, { gain: delay.delayTime.value * depthRate });
const dry   = new GainNode(context, { gain: 1 - mixValue });
const wet   = new GainNode(context, { gain: mixValue });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDelayTimeElement = document.getElementById(&apos;range-chorus-delay-time&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-chorus-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-chorus-rate&apos;);
const rangeMixElement       = document.getElementById(&apos;range-chorus-mix&apos;);

const spanPrintDelayTimeElement = document.getElementById(&apos;print-chorus-delay-time-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-chorus-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-chorus-rate-value&apos;);
const spanPrintMixElement       = document.getElementById(&apos;print-chorus-mix-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  // Connect nodes for original sound
  // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
  oscillator.connect(dry);
  dry.connect(context.destination);

  // Connect nodes for delay sound
  // OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
  oscillator.connect(delay);
  delay.connect(wet);
  wet.connect(context.destination);

  // Connect nodes for LFO that changes delay time periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
  lfo.connect(depth);
  depth.connect(delay.delayTime);

  // Start oscillator and LFO immediately
  oscillator.start(0);
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDelayTimeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  delay.delayTime.value = event.currentTarget.valueAsNumber * 0.001;
  depth.gain.value      = delay.delayTime.value * depthRate;

  spanPrintDelayTimeElement.textContent = `${Math.trunc(delay.delayTime.value * 1000)} msec`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = delay.delayTime.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeMixElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  mixValue = event.currentTarget.valueAsNumber;

  dry.gain.value = 1 - mixValue;
  wet.gain.value = mixValue;

  spanPrintMixElement.textContent = mixValue.toString(10);
});</code></pre>
            <div class="app-container app-chorus">
              <button type="button" id="button-chorus">start</button>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-chorus-delay-time">Delay time</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-delay-time" value="0" min="0" max="50" step="1" />
                      <span id="print-chorus-delay-time-value">0 msec</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-rate" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-rate-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-mix">Mix</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-mix" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-mix-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-flanger">
            <h4>フランジャー</h4>
            <p>
              <b>フランジャー</b>は, コーラスの実装にエフェクト音のフィードバックの接続を追加するだけです (ディレイの実装に LFO を追加して,
              ディレイタイムを周期的に変化させるとも言えます). つまり, コーラスの実装をより汎用的にした実装となります. パラメータしだいで, フランジャーになり,
              コーラスにもなります. 原理的には, 同じなのでこのあたりの区別は, 音楽的な感覚による違いでしかありません.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-flanger" width="1200" height="520" />
              <figcaption>フランジャーのノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-flanger-delay-time&quot;&gt;Delay time&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-delay-time&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-flanger-delay-time-value&quot;&gt;0 msec&lt;/span&gt;
&lt;label for=&quot;range-flanger-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-flanger-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-mix&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-mix&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-mix-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-feedback&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-feedback&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;0.9&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-feedback-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;

let depthRate  = 0;
let rateValue  = 0;
let mixValue   = 0;

const delay    = new DelayNode(context);
const depth    = new GainNode(context, { gain: delay.delayTime.value * depthRate });
const dry      = new GainNode(context, { gain: 1 - mixValue });
const wet      = new GainNode(context, { gain: mixValue });
const feedback = new GainNode(context, { gain: 0 });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDelayTimeElement = document.getElementById(&apos;range-flanger-delay-time&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-flanger-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-flanger-rate&apos;);
const rangeMixElement       = document.getElementById(&apos;range-flanger-mix&apos;);
const rangeFeedbackElement  = document.getElementById(&apos;range-flanger-feedback&apos;);

const spanPrintDelayTimeElement = document.getElementById(&apos;print-flanger-delay-time-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-flanger-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-flanger-rate-value&apos;);
const spanPrintMixElement       = document.getElementById(&apos;print-flanger-mix-value&apos;);
const spanPrintFeedbackElement  = document.getElementById(&apos;print-flanger-feedback-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  // Connect nodes for original sound
  // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
  oscillator.connect(dry);
  dry.connect(context.destination);

  // Connect nodes for delay sound
  // OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
  oscillator.connect(delay);
  delay.connect(wet);
  wet.connect(context.destination);

  // Connect nodes for feedback
  // (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
  delay.connect(feedback);
  feedback.connect(delay);

  // Connect nodes for LFO that changes delay time periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
  lfo.connect(depth);
  depth.connect(delay.delayTime);

  // Start oscillator and LFO immediately
  oscillator.start(0);
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDelayTimeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  delay.delayTime.value = event.currentTarget.valueAsNumber * 0.001;
  depth.gain.value      = delay.delayTime.value * depthRate;

  spanPrintDelayTimeElement.textContent = `${Math.trunc(delay.delayTime.value * 1000)} msec`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = delay.delayTime.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeMixElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  mixValue = event.currentTarget.valueAsNumber;

  dry.gain.value = 1 - mixValue;
  wet.gain.value = mixValue;

  spanPrintMixElement.textContent = mixValue.toString(10);
});

rangeFeedbackElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const feedbackValue = event.currentTarget.valueAsNumber;

  feedback.gain.value = feedbackValue;

  spanPrintFeedbackElement.textContent = feedbackValue.toString(10);
});</code></pre>
            <div class="app-container app-flanger">
              <button type="button" id="button-flanger">start</button>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-flanger-delay-time">Delay time</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-delay-time" value="0" min="0" max="50" step="1" />
                      <span id="print-flanger-delay-time-value">0 msec</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-flanger-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-rate" value="0" min="0" max="10" step="0.5" />
                      <span id="print-flanger-rate-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-mix">Mix</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-mix" value="0" min="0" max="1" step="0.05" />
                      <span id="print-flanger-mix-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-feedback">Feedback</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-feedback" value="0" min="0" max="0.9" step="0.05" />
                      <span id="print-flanger-feedback-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-chorus-and-flanger-fm-modulation">
            <h4>ディレイタイムの周期的な変化とFM 変調</h4>
            <p><b>FM 変調</b> (<b>Frequency Modulation</b>) とは, 時間の経過とともに信号の周波数を変化させることです.</p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-frequency-modulation-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-frequency-modulation-spectrum" width="720" height="240" /></dd>
              </dl>
              <figcaption>
                <span>FM 変調のイメージ (スペクトルのピークが <code>880 Hz &plusmn; 440 Hz</code> の間で変調します)</span>
                <button type="button" id="button-frequency-modulation-animation">start</button>
              </figcaption>
            </figure>
            <p>
              コーラス・フランジャーは, 結果的に FM 変調を発生させていると解説しましたが, これに対して疑問に思うことがあるかもしれません.
              周波数を周期的に変化させるために, なぜ, ディレイタイムを周期的に変化させているのかということです
              (ディレイタイムの周期的な変化が原理となっているかということです). <a href="section-effectors-vibrato">LFO の実装例</a>で解説したように,
              直接的に周波数を周期的に変化させればよいはずです. しかしながら, 基本波形のように, 基本周波数が明確な場合はそれで問題ないのですが, アンサンブル
              (楽曲) や音声において, 一般的に, 基本周波数を (精度高く) 推定するアルゴリズム (<span class="math-inline">$f_{0}$</span> 推定)
              は複雑になってしまい, 計算量も多くなります. したがって, 汎用的なコーラス・フランジャーを実装するとなると,
              直接的に周波数を変化させることは難しくなります.
            </p>
            <p>
              ここで, コンボリューション積分とフーリエ変換の性質から,
              <b>時間領域における遅延は, 周波数領域においては周波数成分の変化となります</b> (数学的な詳細を知る必要はないですが,
              <a href="#section-effectors-delay-and-reverb-cyclic-convolution">巡回畳み込み</a>のセクションが参考になると思います). すなわち,
              <b>時間領域においてディレイタイムを周期的に変化させることは, 周波数領域において各周波数成分を周期的に変化させる</b>こととなり, 結果として
              (汎用的な) FM 変調となります.
            </p>
            <p>
              ちなみに, すでにサンプルコードを実行して気づいたかもしれませんが,
              <b>コーラス・フランジャーで, エフェクト音のみの出力にした場合, ビブラートとなります</b>. ビブラートはまさに FM 変調であり, 言い換えれば,
              コーラス・フランジャーは, ビブラートをベースにしたエフェクターであり, 汎用的な実装とパラメータ設定でビブラートにすることも可能ということです.
            </p>
          </section>
        </section>
      </section>
    </main>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/prism.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"]
          ]
        }
      };
    </script>
    <script defer src="docs.js"></script>
  </body>
</html>
