<!doctype html>
<html lang="ja" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="UTF-8" />
    <title>Web Music Documentation</title>
    <meta name="description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta name="keywords" content="Web Music, Web Audio, Audio Signal Processing, Music" />
    <meta name="author" content="Korilakkuma (Tomohiro IKEDA)" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, viewport-fit=cover" />
    <meta name="format-detection" content="telephone=no" />
    <meta name="theme-color" content="#fafafa" />
    <meta property="og:description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta property="og:image" content="https://korilakkuma.github.io/Web-Music-Documentation/images/icon.png" />
    <meta property="og:site_name" content="Web Music Documentation" />
    <meta property="og:title" content="Web Music Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <meta name="twitter:card" content="summary" />
    <link rel="canonical" href="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <link rel="icon" href="images/icon.png" type="image/png" />
    <link rel="apple-touch-icon" href="images/icon.png" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-okaidia.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="docs.css" type="text/css" media="all" />
  </head>
  <body>
    <main id="document-top">
      <h1>Web Music ドキュメント</h1>
      <section id="section-about-web-music">
        <h2>Web Music</h2>
        <p>
          <strong>Web Music</strong> とは, Web (ブラウザ) をプラットフォームにした音楽アプリケーション, そして, そのような Web
          アプリケーションを実装するために必要となる, クライアントサイド JavaScript API (ブラウザ API) の総称です. これは, 一般的な技術用語ではなく,
          技術マーケティング的な造語です.
        </p>
        <p>具体的には, 以下のような, クライアントサイド JavaScript API の総称です.</p>
        <ul>
          <li><a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API</a></li>
          <li><a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement (HTMLAudioElement)</a></li>
          <li><a href="https://www.w3.org/TR/webrtc/" target="_blank" rel="noopener noreferrer">WebRTC</a></li>
          <li><a href="https://www.w3.org/TR/webmidi/" target="_blank" rel="noopener noreferrer">Web MIDI API</a></li>
          <li><a href="https://www.w3.org/TR/webcodecs/" target="_blank" rel="noopener noreferrer">WebCodecs API</a></li>
          <li><a href="https://www.w3.org/TR/mediasession/" target="_blank" rel="noopener noreferrer">Media Session</a></li>
        </ul>
        <p>
          本サイト制作開始時点の 2023 年時点で Web Audio API と WebRTC に関しては, <b>W3C recommendation</b>, HTMLMediaElement に関しては,
          <b>HTML Living Standard</b> (2019 年 6 月以降, HTML や DOM に関わる仕様策定は <abbr title="World Wide Web Consortium">W3C</abbr> ではなく
          <abbr title="Web Hypertext Application Technology Working Group">WHATWG</abbr> が仕様策定の主体になることが決定されたので, HTMLMediaElement は HTML
          Living Standard です) となっており, モダンブラウザであれば利用することが可能です (ただし, クライアントサイド JavaScript の宿命ではありますが, OS
          やブラウザによって挙動が異なることは少なからずあるので, 移植性まで考慮すると, そのためのクロスブラウザ対応の問題は必要となります).
          これらのクライアントサイド JavaScript API は 2010 年代前半ごろは, <b>HTML5</b> というバズワード化したカテゴリに分類される API でした. 現在は, HTML5
          という仕様, あるいは, 用語が定着したからか, HTML5 というワードが使われることはほぼなくなりました. したがって, Web Music に関係する API も,
          膨大なクライアントサイド JavaScript API のうちのいくつかです (という認識が一般的と言えます).
        </p>
        <article id="section-web-browser-javascript">
          <h3>クライアントサイド JavaScript とは ?</h3>
          <p>
            クライアントサイド JavaScript とは, JavaScript の仕様の標準である ECMAScript (JavaScript の実行コンテキストに依存しない言語仕様. これに準拠している
            JavaScript のコードであれば, Web ブラウザでも, Node.js でも, ブラウザ拡張でも使うことができます) と, 実行コンテキスト
            (広義な意味でのプラットフォーム) である Web ブラウザで実行する場合にのみアクセス可能な API です (例えば, Web Music の API は Node.js
            で使うことはできません. また, Web ブラウザでも Web Workers が生成したスレッドでは, メインスレッド (UI スレッド)
            と実行コンテキストが異なるので使うことができません).
          </p>
        </article>
      </section>
      <section id="section-web-audio-api-overview">
        <h2>Web Audio API</h2>
        <p>
          Web Music のなかで, もっともコアな API が <strong>Web Audio API</strong> です. 言い換えると, Web
          をプラットフォームとした音楽アプリケーションを制作するほとんどの場合で必要になる API ということです. なぜなら,
          <code>HTMLAudioElement</code> はオーディオファイルを再生するための API で, 高度なオーディオ処理をすることはできず (<a
            href="https://egonelbre.com/project/jsfx/"
            target="_blank"
            rel="noopener noreferrer"
            >jsfx</a>
          のようにハッキーな実装をすることでエフェクトをかけるぐらいは可能ですが, 仕様のユースケースとして想定されている使い方ではありません),
          リアルタイム性やインタラクティブ性も考慮された API ではないからです (厳密には, 考慮された経緯もあって,
          <code>Audio</code> コンストラクタが定義されています). また, Web Music として, Web MIDI API や WebRTC を使う場合, 実際のオーディオ処理は Web Audio API
          が実行することになります.
        </p>
        <section id="section-web-music-history">
          <h3>Web Music の歴史</h3>
          <p>
            古くは, Internet Explorer が独自に
            <a href="https://www.tohoho-web.com/html/bgsound.htm" target="_blank" rel="noopener noreferrer"><code>bgsound</code></a> という HTML
            タグを実装しており, (Internet Explorer のみではありますが) ブラウザでオーディオをファイルを再生することが可能でした (現在の
            <code>HTMLAudioElement</code> に相当する HTML タグと言えます). そのあと, Java アプレットや ActionScript (Flash) によって, Web Audio API
            で実現できているような高度なオーディオ処理が可能となりました.
          </p>
          <p>
            しかし, これらは特定のベンダーに依存していたので, Flash や Silverlight などブラウザの拡張機能 (プラグイン) という位置づけでした. Web 2.0
            (もっと言えば, Ajax) を機にブラウザでも, ネイティブアプリケーションのような Web アプリケーションが実装されてくるようになると, これまで拡張機能
            (オーディオ処理だけでなく, グラフィックス, ストレージ, ローカルファイルへのアクセス, ソケットなど) に依存していたような機能をブラウザ標準で
            (クライアントサイド JavaScript API で) 実現できる流れが 2010 年ごろから活発になりました (このころ, HTML5 という位置づけで仕様策定され,
            モダンブラウザで実装されるようになりました).
          </p>
          <p>
            ドキュメントプラットフォームとしての Web に, アプリケーションプラットフォームが追加されていく転換期に, Web Audio API
            も仕様策定されて現在に至っています.
          </p>
          <ol>
            <li>
              <a href="https://www.w3.org/TR/2011/WD-webaudio-20111215/" target="_blank" rel="noopener noreferrer">草案 (Working Draft)</a> (2011 年 12 月 15
              日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio-1.0/" target="_blank" rel="noopener noreferrer">Web Audio API 1.0 勧告 (W3C recommendation)</a> (2021 年 6
              月 17 日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API 1.1 (最新仕様)</a> (2024 年 11 月 5 日に公開)
            </li>
          </ol>
        </section>
        <article id="section-about-audio-data-api">
          <h3>Audio Data API</h3>
          <p>
            厳密な歴史を記載すると, Web Audio API よりわずかに先行して Firefox で
            <a href="https://wiki.mozilla.org/Audio_Data_API" target="_blank" rel="noopener noreferrer">Audio Data API</a> というブラウザオーディオ API
            が実装されていました. <code>HTMLAudioElement</code> の拡張という位置づけで, 出力するオーディオデータを直接演算する API がメインでした (Web Audio API
            の <code>ScriptProcessorNode</code> に相当する API). 間もなくして, Web Audio API に統一される方針となり, Firefox も Web Audio API
            のサポートを開始したので現在は削除されています.
          </p>
        </article>
      </section>
      <section id="section-about-document">
        <h2>このサイトに関して</h2>
        <p>
          このサイト (ドキュメント) の目的は, Web Music, その中核となる Web Audio API について解説しますが, W3C
          が公開している仕様のすべてを解説するわけではありません. また, JavaScript の言語仕様の解説は, サイトの目的ではないこともご了承ください (ただし, Web
          Audio API を使う上で, 必要となってくるクライアントサイド JavaScript API に関しては必要に応じて解説をします (例. <code>File API</code>,
          <code>Fetch API</code> など).
        </p>
        <p>このサイトは W3C が公開している仕様にとって代わるものではなく, Web Audio API の仕様の理解を補助するリファレンスサイトと位置づけてください.</p>
        <p>
          デスクトップブラウザでは少なくなりましたが, モバイルブラウザでは仕様とブラウザの実装に差異があり,
          仕様では定義されているのに動作しないということもあります. その場合には, 開発者ツールなどを活用して,
          実装されているプロパティやメソッドを確認してみてください.
        </p>
        <section id="section-about-sample-code">
          <h3>解説の JavaScript コードに関して</h3>
          <p>
            <b>ECMAScript 2015</b> 以降の仕様に準拠したコードで記載します. また, ビルドツールなどを必要としないように, TypeScript
            での記述やモジュール分割などもしません (端的には, コピペすればブラウザコンソールなどで実行できるようなサンプルコード, あるいは,
            コード片を記載します). 具体的には, 以下のような構文を使います.
          </p>
          <ul>
            <li><code>const</code>, <code>let</code> による変数宣言</li>
            <li>Template Strings</li>
            <li>アロー関数</li>
            <li>クラス</li>
            <li><code>Promise</code>, または, <code>async</code>/<code>await</code></li>
          </ul>
          <p>
            Web Audio API のコードも仕様で推奨されているコードを基本的に記載します (例えば, <code>AudioNode</code> インスタンスを生成する場合,
            コンストラクタ形式が推奨されているので, そちらを使います). ただし, 現時点であまりにも実装の乖離が大きい場合は, フォールバック的な解説として,
            実装として動作するコードを記載します.
          </p>
        </section>
        <section id="section-recommendation-browsers">
          <h3>推奨ブラウザ</h3>
          <p>
            閲覧自体は, モダンブラウザであれば特に問題ありませんが, 実際のサンプルコードを動作させることを考慮すると, デスクトップブラウザ, 特に, Web Audio API
            の仕様に準拠している <a href="https://www.google.com/chrome/" target="_blank" rel="noopener noreferrer">Google Chrome</a> もしくは
            <a href="https://www.mozilla.org/ja/firefox/" target="_blank" rel="noopener noreferrer">Mozilla Firefox</a> (いずれも最新バージョン) を推奨します
            (Google Chrome の場合, より高度な
            <a href="https://web.dev/articles/profiling-web-audio-apps-in-chrome" target="_blank" rel="noopener noreferrer">Web Audio API 専用のプロファイラ</a>があるのでおすすめです).
          </p>
        </section>
        <section id="section-prerequisite">
          <h3>前提知識と経験</h3>
          <p>
            前提知識としては, ECMAScript 2015 以降の JavaScript の言語仕様を理解していることと, Web ブラウザを実行環境にした JavaScript による Web
            アプリケーションを実装した経験ぐらいです. Web Audio API は, ユースケースにおいて想定されるオーディオ信号処理を抽象化しているので,
            オーディオ信号処理に対する理解がなくても, それなりのアプリケーションは制作できます (アプリケーションの仕様しだいでは不要になるぐらいです). もちろん,
            オーディオ信号処理の理解や Web 以外のプラットフォームでのオーディオプログラミングの経験 (特に, GUI
            で必要なリアルタイム性のオーディオプログラミングの経験) があれば, それは Web Audio API を理解するうえで活きますし, Web Audio API
            が標準でサポートしないようなオーディオ処理を実現したいケースではむしろ必要になります.
          </p>
          <p>
            また, 音楽理論に対する知識も不要です. Web Audio API はユースケースとして, 音楽用途に限定していないからです. したがって, このサイトでは,
            アプリケーションによっては必要になるドメイン知識として位置づけます (もちろん, ユースケースとして, 音楽用途も想定されているので, Web
            をプラットフォームにした音楽アプリケーションを制作する場合には必要となるケースが多いでしょう).
          </p>
          <p>
            このサイトでは, オーディオ信号処理や音楽理論など必要に応じて解説します. Web Audio API が解説の中心ではありますが, Web Music
            アプリケーションを制作するための標準ドキュメントとなることを目指すからです (オーディオ信号処理や音楽理論を深入りする場合は,
            それぞれ最適なドキュメントや書籍がたくさんあるのでそちらを参考にしてください).
          </p>
        </section>
        <article id="section-skeptical">
          <h4>Web Audio API に対する懐疑的な意見</h4>
          <p>
            Web Audio API は他のプラットフォームのオーディオ API と比較すると, やや奇怪な API 設計であったり, 仕様策定されたころの JavaScript の事情と, 現代の
            JavaScript の事情が様変わりしたことから, 懐疑的な意見もあります (参考
            <a href="https://zenn.dev/okuoku/articles/13c39882596c92" target="_blank" rel="noopener noreferrer">WebAudioは何故あんな事になっているのか</a>).
            しかしながら, この記事でも述べられているように,
            <q>実はWebAudioはオーディオAPIのオープンスタンダードとしては唯一生き残っている存在と言える。</q>
            これはたしかで, その点において学ぶ意義はありますし, 音楽アプリケーションとして Web をプラットフォームにする必要がある場合は必須となるでしょう.
          </p>
        </article>
        <section id="section-contribution-to-document">
          <h3>Issue と Pull Requests</h3>
          <p>
            プロローグの最後に, このサイト (ドキュメント) はオープンソースとして
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation" target="_blank" rel="noopener noreferrer">GitHub</a> に公開しています.
            このサイトのオーナーも完璧に理解しているわけではないので, 間違いもあるかと思います. その場合には, GitHub に
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/issues" target="_blank" rel="noopener noreferrer">issue</a> を作成したり,
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/pulls" target="_blank" rel="noopener noreferrer">Pull Requests</a>
            を送っていただいたりすると大変ありがたいです.
          </p>
          <p>それでは, Web Music の未来を一緒に開拓していきましょう !</p>
        </section>
      </section>
      <section id="section-getting-started">
        <h2>Getting Started</h2>
        <section id="section-audio-context">
          <h3>AudioContext</h3>
          <p>
            Web Audio API を使うためには, <code>AudioContext</code> クラスのコンストラクタを呼び出して,
            <code>AudioContext</code> インスタンスを生成する必要があります. <code>AudioContext</code> インスタンスが Web Audio API
            で可能なオーディオ処理の起点になるからです. <code>AudioContext</code> インスタンスを生成することで, Web Audio API
            が定義するプロパティやメソッドにアクセス可能になるわけです.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();</code></pre>
          <p>
            何らかの理由で, レガシーブラウザ (特に, モバイルブラウザ) もサポートしなければならない場合, ベンダープレフィックスつきの
            <code>webkitAudioContext</code> もフォールバックとして設定しておくとよいでしょう (少なくとも, デスクトップブラウザでは不要な処理で,
            これから将来においては確実に不要になる処理ではありますが).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">window.AudioContext = window.AudioContext || window.webkitAudioContext;

const context = new AudioContext();</code></pre>
          <p><code>AudioContext</code> インスタンスをコンソールにダンプしてみます.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

console.dir(context);</code></pre>
          <p>
            <code>AudioContext</code> インスタンスに様々なプロパティやメソッドが実装されていることがわかるかと思います. このドキュメントではこれらを
            (すべてではありませんが) メインに解説していくことになります. また, このように実装を把握することで, 仕様と実装の乖離を調査することにも役立ちます.
          </p>
          <img src="images/audio-context.png" alt="AudioContext" width="1232" height="770" loading="lazy" />
          <p>
            Web Audio API でオーディオ処理を実装するうえで意識することはほとんどありませんが, <code>AudioContext</code> は <code>BaseAudioContext</code> を拡張
            (継承) したクラスであることもわかります.
          </p>
          <img src="images/base-audio-context.png" alt="BaseAudioContext" width="1232" height="770" loading="lazy" />
          <section id="section-autoplay-policy">
            <h4>Autoplay Policy 対策</h4>
            <p>
              Web Audio API に限ったことではないですが, ページが開いたときに, ユーザーが意図しない音を聞かせるのはよくないという観点から (つまり, UX
              上好ましくないという観点から), ブラウザでオーディオを再生する場合,
              <a href="https://developer.chrome.com/blog/autoplay#web_audio" target="_blank" rel="noopener noreferrer">Autoplay Policy</a>
              という制限がかかります. これを解除するためには, <b>ユーザーインタラクティブなイベント</b> 発火後に
              <code>AudioContext</code> インスタンスを生成するか, もしくは, <code>AudioContext</code> インスタンスの <code>resume</code> メソッドを実行して
              <code>AudioContextState</code> を <code>&apos;running&apos;</code> に変更する必要があります. これをしないと, オーディオを鳴らすことができません.
              また, <code>decodeAudioData</code> など一部のメソッドが Autoplay Policy 解除まで実行されなくなります. ユーザーインタラクティブなイベントとは,
              <code>click</code>, <code>mousedown</code> や <code>touchstart</code> などユーザーが明示的に操作することによって発火するイベントのことです.
              したがって, <code>load</code> イベントや <code>mousemove</code> など, 多くのケースにおいてユーザが明示的に操作するわけではないようなイベントでは
              Autoplay Policy の制限を解除することはできません.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">document.addEventListener(&apos;click&apos;, () =&gt; {
  const context = new AudioContext();
});</code></pre>
            <p>
              <code>resume</code> メソッドで解除する場合 (この場合, コンソールには警告メッセージが表示されますが, Autoplay Policy
              は解除できるので無視して問題ありません).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

document.addEventListener(&apos;click&apos;, async () =&gt; {
  await context.resume();
});</code></pre>
            <p>
              <b>これ以降のセクションでは, 本質的なコードを表記したいので, Autoplay Policy は解除されている状態を前提とします.</b>
            </p>
          </section>
        </section>
        <section id="section-audio-node">
          <h3>AudioNode</h3>
          <p>
            Web Audio API におけるオーディオ処理の基本は, <code>AudioNode</code> クラスのインスタンス生成と <code>AudioNode</code> がもつ
            <code>connect</code> メソッドで <code>AudioNode</code> インスタンスを接続していくことです. <code>AudioNode</code> クラスは,
            それ自身のインスタンスを生成することはできず, <code>AudioNode</code> を拡張 (継承) したサブクラスのインスタンスを生成して, オーディオ処理に使います.
            <code>AudioNode</code> はその役割を大きく 3 つに分類することができます.
          </p>
          <ul>
            <li>サウンドの入力点となる <code>AudioNode</code> のサブクラス (<code>OscillatorNode</code>, <code>AudioBufferSourceNode</code> など)</li>
            <li>サウンドの出力点となる <code>AudioNode</code> のサブクラス (<code>AudioDestinationNode</code>)</li>
            <li>
              音響特徴量を変化させる <code>AudioNode</code> のサブクラス (<code>GainNode</code>, <code>DelayNode</code>, <code>BiquadFilterNode</code> など)
            </li>
          </ul>
          <p>
            現実世界のオーディオ機器に例えると, サウンドの入力点に相当する <code>AudioNode</code> のサブクラスが, マイクロフォンや楽器, 楽曲データなどに相当,
            サウンドの出力点に相当する <code>AudioNode</code> のサブクラスが. スピーカーやイヤホンなどに相当, そして, 音響特徴量を変化させる
            <code>AudioNode</code> のサブクラスがエフェクターやボイスチェンジャーなどが相当します.
          </p>
          <p>
            これらの, <code>AudioNode</code> のサブクラスを使うためには, <b>コンストラクタ呼び出し</b>, または,
            <b><code>AudioContext</code> インスタンスに実装されているファクトリメソッド</b> 呼び出す必要があります (ただし, サウンドの出力点となる
            <code>AudioDestinationNode</code> は <code>AudioContext</code> インスタンスの <code>destination</code> プロパティでインスタンスとして使えるので,
            コンストラクタ呼び出しやファクトリメソッドは定義されていません).
          </p>
          <p>例えば, 入力として, オシレーター (<code>OscillatorNode</code>) を使う場合, コンストラクタ呼び出しの実装だと以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);</code></pre>
          <p>
            インスタンス生成時には, その <code>AudioNode</code> のサブクラスに定義されているパラメータ (<code>OscillatorNode</code> の場合,
            <code>OscillatorOptions</code>) を指定することも可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 880 });</code></pre>
          <p>ファクトリメソッドでインスタンス生成する場合, 以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = context.createOscillator();</code></pre>
          <p>
            コンストラクタ呼び出しによる, <code>AudioNode</code> のサブクラスのインスタンス生成は, Web Audio API の初期には仕様策定されておらず,
            <code>AudioContext</code> インスタンスに実装されているファクトリメソッド呼び出す実装のみでした. インスタンス生成時に,
            パラメータを変更可能なことから, どちらかと言えば, コンストラクタ呼び出しによるインスタンス生成が推奨されているぐらいですが,
            ファクトリメソッドが将来非推奨になることはなく, また, 初期の仕様には仕様策定されていなかったことから,
            <b>レガシーブラウザの場合, コンストラクタ呼び出しが実装されていない場合もあります</b>. したがって, サポートするブラウザが多い場合は,
            ファクトリメソッドを, サポートするブラウザが限定的であれば, コンストラクタ呼び出しを使うのが現実解と言えるでしょう.
          </p>
          <section id="section-connect-audio-node">
            <h4>connect メソッド (AudioNode の接続)</h4>
            <p>
              現実世界の音響機器では, 入力と出力, あるいは, 音響変化も接続することで, その機能を果たします. 例えば, エレキギターであれば,
              サウンド入力を担うギターとサウンド出力を担うアンプ (厳密にはスピーカー) は, 単体ではその機能を果たしません.
              シールド線などで接続することによって機能します.
            </p>
            <p>
              このことは, Web Audio API の世界も同じです. (<code>AudioContext</code> インスタンスを生成して,) サウンド入力点となる
              <code>AudioNode</code> のサブクラスのインスタンス (先ほどのコード例だと, <code>OscillatorNode</code> インスタンス) と, サウンド出力点となる
              <code>AudioDestinationNode</code> インスタンスを生成しただけではその機能を果たしません. 少なくとも,
              サウンド入力点と出力点を接続する処理が必要となります (さらに, Web Audio API が定義する様々なノードと接続することで, 高度なオーディオ処理を実現する
              API として真価を発揮します).
            </p>
            <p>
              Web Audio API のアーキテクチャは, 現実世界における音響機器のアーキテクチャと似ています. このことは, Web Audio API
              の理解を進めていくとなんとなく実感できるようになると思います.
            </p>
            <p>
              Web Audio APIにおいて「接続」の役割を担うのが, <code>AudioNode</code> がもつ <b><code>connect</code> メソッド</b>です. 実装としては,
              <code>AudioNode</code> サブクラスのインスタンスの, <code>connect</code> メソッドを呼び出します. このメソッドの第 1 引数には, 接続先となる
              <code>AudioNode</code> のサブクラスのインスタンスを指定します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);</code></pre>
            <p>
              サウンドの入力点と出力点を接続し, 最小の構成を実装できました. しかし, まだ音は出せません. なぜなら,
              サウンドを開始するための音源スイッチをオンにしていないからです. 現実世界の音響機器も同じです. 現実世界がそうであるように, Web Audio API
              においても, 音源のスイッチをオン, オフする必要があります. そのためには, <code>OscillatorNode</code> クラスがもつ
              <b><code>start</code> メソッド</b>, <b><code>stop</code> メソッド</b> を呼び出します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);</code></pre>
            <p>
              <code>start</code> メソッドの引数に <code>0</code> を指定していますが, これはメソッドが呼ばれたら, 即時にサウンドを開始します.
              <code>stop</code> メソッドの引数には, <code>AudioContext</code> インスタンスの <b><code>currentTime</code></b> プロパティに
              <code>2.5</code> を加算した値を指定していますが, これは, <code>stop</code> メソッドを実行してから, 2.5
              秒後に停止することをスケジューリングしています (詳細は, のちほどのセクションで Web Audio API におけるスケジューリングとして解説しますが,
              <code>AudioContext</code> インスタンスの <code>currentTime</code> は,
              <b><code>AudioContext</code> インスタンスが生成されてからの経過時間を秒単位で計測した値</b>が格納されています). <code>stop</code> メソッドの引数も
              <code>0</code> を指定すれば即時にサウンドを停止します. ちなみに, <code>start</code> メソッド, <code>stop</code> メソッドもデフォルト値は
              <code>0</code> なので, 引数を省略して呼び出した場合, 即時にサウンドを開始, 停止します.
            </p>
            <p>これで, とりあえず, ブラウザ (Web) で音を鳴らすことができました !</p>
          </section>
        </section>
        <section id="section-audio-param">
          <h3>AudioParam</h3>
          <p>
            サウンドの入力点と出力点を生成して, それらを接続するだけでは, 元の入力音をそのまま出力するだけなので高度なオーディオ処理はできません. むしろ, Web
            Audio API において重要なのは, この入力と出力の間に, 音響変化をさせる <code>AudioNode</code> を接続することです. 音響変化をさせるためには,
            音響変化のためのパラメータを取得・設定したり, 周期的に変化させたり (LFO) できる必要があります. Web Audio API において, その役割を担うのが
            <b><code>AudioParam</code></b> クラスです. <code>AudioNode</code> が現実世界の音響機器と例えをしましたが, それに従うと,
            <code>AudioParam</code> クラスはノブやスライダーなど音響機器のパラメータを設定するコントローラーのようなものです.
          </p>
          <p>
            <code>AudioParam</code> クラスは直接インスタンス化することはありません. <code>AudioNode</code> のプロパティとして,
            <code>AudioNode</code> のサブクラスのインスタンスを生成した時点でインスタンス化されているのでプロパティアクセスで参照することが可能です.
          </p>
          <p>
            <code>AudioParam</code> では, 単純なパラメータの取得や設定だけでなく, そのパラメータを周期的に変化させたり (LFO), スケジューリングによって変化させる
            (エンベロープジェネレーターなど) ことが可能です (ここはオーナーの経験からですが, Web Audio API で高度なオーディオ処理を実装するためには,
            <code>AudioParam</code> を理解して音響パラメータを制御できるようになるかが非常に重要になっていると思います).
          </p>
          <section id="section-gain-node">
            <h4>GainNode</h4>
            <p>
              <code>AudioParam</code> の詳細は, のちほどのセクションで解説しますので, このセクションでは, 最初のステップとして,
              <b><code>GainNode</code></b> を使って, パラメータの取得・設定を実装します. <code>GainNode</code> はその命名のとおり,
              <b>ゲイン</b> (<b>増幅率</b>), つまり, 入力に対する出力の比率 (入力を <code>1</code> としたときに出力の値) を制御するための
              <code>AudioNode</code> で, Web Audio API におけるオーディオ処理で頻繁に使うことになります. このセクションでは, 単純に, <code>GainNode</code> の
              <b><code>gain</code></b> プロパティ (<code>AudioParam</code> インスタンス) を参照して, そのパラメータを取得・設定してみます (このセクションでは,
              音量の制御と考えても問題ありません).
            </p>
            <p>
              <code>GainNode</code> も <code>AudioNode</code> のサブクラスなので, コンストラクタ呼び出し, または, ファクトリメソッドで
              <code>GainNode</code> インスタンスを生成できます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context);</code></pre>
            <p>コンストラクタ呼び出しで生成する場合, 初期パラメータ (<code>GainOptions</code>) を指定することも可能です.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context, { gain: 0.5 });</code></pre>
            <p>ファクトリメソッドで生成する場合.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const gain = context.createGain();</code></pre>
            <p><code>GainNode</code> インスタンスを生成したら, <code>OscillatorNode</code> と <code>AudioDestinationNode</code> の間に接続します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context, { gain: 0.5 });

// OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);
</code></pre>
            <p>これで実際にサウンドを発生させると, 音の大きさが小さく聴こえるはずです.</p>
            <p>
              このコードだと, 初期値を変更しているだけなので, 例えば, ユーザー操作によって変更するといったことができないので,
              インスタンス生成時以外でパラメータを設定したり, 取得したりする場合は, <code>GainNode</code> の <code>gain</code> プロパティを参照します. これは,
              先ほども記載したように, <code>AudioParam</code> インスタンスです. パラメータの取得や設定をするには, その
              <b><code>value</code></b> プロパティにアクセスします.
            </p>
            <p>簡単な UI として, 以下の HTML があるとします.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;label for=&quot;range-gain&quot;&gt;gain&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-gain&quot; value=&quot;1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-gain-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <p>
              この <code>input[type=&quot;range&QUOT;]</code> のイベントリスナーで, <code>input[type=&quot;range&QUOT;]</code> で入力された値 (JavaScript の
              <code>number</code> 型) を <code>gain</code> (<code>AudioParam</code> インスタンス) の <code>value</code> プロパティに設定し, また,
              その値を取得して, HTML に動的に表示します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context);

// OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

const spanElement = document.getElementById(&apos;print-gain-value&apos;);

document.getElementById(&apos;range-gain&apos;).addEventListener(&apos;input&apos;, (event) =&gt; {
  gain.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = gain.value;
});</code></pre>
            <p>
              <code>AudioParam</code> のパラメータの取得や設定は, このように, JavaScript のオブジェクトに対するプロパティの getter や setter
              と同じなので特に違和感なく理解できるのではないでしょうか.
            </p>
          </section>
        </section>
        <p>
          このセクションでは, Web Audio API の設計の基本となる ((Web Audio API のアーキテクチャを決定づけている), <code>AudioContext</code>,
          <code>AudioNode</code>, <code>AudioParam</code> の関係性とそのパラメータの取得・設定の実装のを解説しました. 以降のセクションでは,
          ユースケースに応じて, これら 3 つのクラスの詳細についても解説を追加していきます.
        </p>
      </section>
      <section id="section-about-sound">
        <h2>「音」とは ?</h2>
        <p>
          このセクションでは, そもそも「音」とはなにか ? からスタートして, 音の特性について簡単に解説します (いわゆる,
          <b>音響学</b>の基本のほんの一部分を解説します). 網羅的な解説はしないので, Web Audio API を理解するうえで, 最低限の解説をできるだけ簡単に解説します.
          また, そのために, 厳密さは犠牲にしている解説も多くあると思います. 音のスペシャリストの方からすると, ちょっと違う ...
          という部分はたくさんあるかと思いますがご了承ください (ただし, あきらかに間違った解説や誤解を招く可能性のある解説については遠慮なく Issue を作成したり,
          Pull Requests を送ったりしていただければと思います).
        </p>
        <p>
          Web Audio API について解説するセクションではないので, 音の特性 (音響学) に関して学んだことあれば,
          このセクションはスキップしていただくのがよいでしょう.
        </p>
        <section id="section-what-is-sound">
          <h3>音の実体</h3>
          <p>
            そもそも, 「音」って何なのでしょうか？ 結論としては, 音とは媒体の振動が聴覚に伝わったものと定義することができます.
            「媒体」というものが抽象的でよくわからないかもしれませんが, 具体的には, 空気や水です. 日常の多くの音は空気を媒体として,
            空気の振動が聴覚に伝わることで音として知覚するわけですが, 同じことは水中でも起きますし, 普段聴いている自分の声は骨を媒体にして伝わっている音です.
          </p>
        </section>
        <section id="section-modeling-sound">
          <h3>音のモデリング</h3>
          <p>
            音をコンピュータで表現するためには, 媒体の振動を数式で表現して, その数式によって導出される数値を 2 進数で表現できる必要があります.
            音の実体は媒体の振動というのを説明しましたが, この振動を表現するのに適した数学的な関数が, <b>sin 関数</b>です (cos 関数は sin
            関数の位相の違いでしかないので本質的に同じと考えてもよいでしょう. また, tan 関数は含まれません. その理由は,
            <span class="math-inline">$\frac{\pi}{2}$</span> や <span class="math-inline">$-\frac{\pi}{2}$</span> で
            <span class="math-inline">$\infty$</span> や
            <span class="math-inline">$-\infty$</span> になるので振動を表現するには都合が悪いからと考えてよいでしょう).
          </p>
          <p>
            Web Audio APIでも, <code>OscillatorNode</code> の <code>type</code> プロパティがとりうる値 (<code>OscillatorType</code>) の 1 つとして
            <code>&#039;sine&#039;</code> が定義されています.
          </p>
          <p>
            音を扱う学問や工学では, この sin 関数が, 音の波 (<b>音波</b>) をモデリングしていることから, <b>正弦波</b> (<b>sin 波</b>) と呼ぶことが多いです.
            とちらであっても, 実体は同じなのですが, このドキュメントではこれ以降, 慣習にしたがって, 正弦波 (sin 波) と記述することにします.
          </p>
          <section id="section-sine-wave">
            <h3>正弦波 (sin 波)</h3>
            <p>ここからは少し数学・物理的な話になってきます. 正弦波 (sin 関数) ってどんな形か覚えてらっしゃいますか？</p>
            <figure>
              <svg id="svg-figure-sin-function" width="720" height="405" data-a="1.0" data-f="1" />
              <figcaption>正弦波 (sin 関数)</figcaption>
            </figure>
            <p>具体的に解説するためにパラメータを設定します.</p>
            <figure>
              <svg
                id="svg-figure-sin-function-with-parameters-1-1Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="1"
                data-f="1"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>パラメータつき正弦波 (sin 関数)</figcaption>
            </figure>
            <section id="section-amplitude-and-frequency">
              <h4>振幅と周波数 (周期)</h4>
              <p>
                まず, 縦軸に着目してみます. 縦軸のパラメータは, <b>振幅</b>と呼ばれ, 単位はありません. ちなみに, 振幅 <code>1</code> の正弦波と表現した場合,
                上記のように振幅の最大値が <code>1</code>, 最小値が <code>-1</code>の 正弦波のことを意味しています. 次に, 横軸に着目してみます.
                横軸のパラメータは, <b>時間</b>を表しています. 縦軸との関係で表現すると, ある時刻における正弦波の振幅値を表した図 (グラフ) と言えます. ここで,
                パラメータつきの正弦波を見てみます. すると, 山 1 つと谷 1 つを最小の構成として, それが繰り返されている, すなわち,
                <b>周期性</b>をもつことがわかります. 数学的には, すべての時間
                <span class="math-inline">$t \left(0 \leqq {t} &lt; \infty \right)$</span> に対して,
                <span class="math-inline">$f\left(t + L\right) = f\left(t\right)$</span> となる定数が存在するとき,
                <span class="math-inline">$f\left(t\right)$</span> は周期 <span class="math-inline">$L$</span> の<b>周期関数</b>と定義されます. そして, sin
                関数は, 周期 <span class="math-inline">$L$</span> としたとき
                <span class="math-inline">$\sin\left(t + L\right) = \sin\left(t\right)$</span> が成立するので, <b>正弦波 (sin 関数) は周期関数</b>です.
              </p>
              <p>
                この波の最小の構成が発生するために要する時間を<b>周期</b>と呼びます. 例として, 上記の正弦波で考えると, 最小の構成の発生までに
                <code>1 sec</code> の時間を要しているので, 周期は <code>1 sec</code> となります. この真逆の概念を表す用語が<b>周波数</b>です. すなわち,
                <code>1 sec</code> の間に, 波の最小の構成が何回発生するか ? ということを表し, 単位は <b>Hz</b> (ヘルツ) です. Hz (ヘルツ) という名前ですが,
                日本語に翻訳すれば, 何回の「回」に相当するでしょう. 上記の正弦波で考えると. この正弦波は, <code>1 sec</code> の間に最小の構成が
                <code>1</code> 回発生しているので, 周波数は, <code>1 Hz</code> ということになります.
              </p>
              <p>
                周期と周波数は互いに真逆の概念ですが, これは数学的には, 互いに<b>逆数</b>の関係にあります. すなわち,
                <b>周期の逆数は周波数を表し, 周波数の逆数は周期を表します</b>. 互いに関係のある値なので, 周期の話をすれば周波数の話も同時にしていることであり,
                周波数の話をすれば周期の話も同時にしていることになります. ただ, 周波数という用語のほうがよく使われる傾向にあると思うので, このドキュメントでは,
                周波数の用語を優先的に利用することにします.
              </p>
              <p>少し慣れるために, パラメータ (振幅や周波数) を変えた正弦波 (sin 波) を見てましょう.</p>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-2Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="0.5"
                  data-f="1"
                  data-t="0.0,0.5,1.0"
                />
                <figcaption>振幅 <code>0.5</code>, 周波数 <code>1 Hz</code> (周期 <code>1 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-0.5-1Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="2"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>2 Hz</code> (周期 <code>0.5 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-0.5Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="1"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>0.5 Hz</code> (周期 <code>2 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
            </section>
            <p>いかがでしたか ? 振幅と周波数は Web Audio API の解説においても頻出する用語なので, ある程度理解しておくと, Web Audio API の理解も進むでしょう.</p>
          </section>
          <section id="section-synthesizer-waveforms">
            <h3>基本波形</h3>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値は, 正弦波を生成する文字列
              <code>&apos;sine&apos;</code> 以外にも, 矩形波を生成する <code>&apos;square&apos;</code> やノコギリ波を生成する <code>&apos;sawtooth&apos;</code>,
              三角波を生成する <code>&apos;triangle&apos;</code> があります. 正弦波の形はわかりましたが, それ以外はどのような形をしているのか見てみましょう.
            </p>
            <figure>
              <svg
                id="svg-figure-square-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の矩形波 (<code>&apos;square&apos;</code>)</figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-sawtooth-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>
                振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) のノコギリ波 (<code>&apos;sawtooth&apos;</code>)
              </figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-triangle-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の三角波 (<code>&apos;triangle&apos;</code>)</figcaption>
            </figure>
            <p>
              矩形波・ノコギリ波・三角波のいずれも正弦波と同じように, 周期性をもつ波 (関数) であるということです. 周期性をもつので,
              周波数の概念を適用することができます. そして, 最も重要な点ですが, 周期性をもつ波は周波数の異なる正弦波を合成してできるということです.
              矩形波・ノコギリ波・三角波はいずれも周期性をもちます. 周期性をもつので,
              矩形波・ノコギリ波・三角波はいずれも周波数の異なる正弦波を合成して生成することができます. シンセサイザーでも,
              正弦波・矩形波・ノコギリ波・三角波は基本波形として, サウンド生成のベースとなる波形です. そして, Web Audio API においても, 基本波形はサウンド生成
              (<code>OscillatorNode</code>) のベースになる波形です.
            </p>
          </section>
        </section>
        <section id="section-the-three-components-of-sound">
          <h3>音の 3 要素</h3>
          <p>ここまで, 数学・物理的な話が続いたので, 少し気分を変えて, 感覚視点 (知覚) から音を考えてみましょう.</p>
          <p>
            日常でも, 「音が大きい・小さい」, 音楽を聴いていて「音が高い・低い」,
            楽器を演奏していて「この楽器の音色が好き」などと表現することがあるかと思います. これらは, 音を感覚視点, すなわち, <b>音を知覚するときの視点</b>で,
            どんな音か ? を表現しています. これらの表現にある, <b>音の大きさ</b>・<b>音の高さ</b>・<b>音色</b>を<b>音の 3 要素</b>と呼びます.
          </p>
          <p>音の 3 要素と, 先に解説した振幅・周波数・波形と大きな関わりがあります.</p>
          <dl>
            <dt>音の大きさ (Loudness)</dt>
            <dd>振幅が大きく影響する</dd>
            <dt>音の高さ (Pitch)</dt>
            <dd>周波数が大きく影響する</dd>
            <dt>音色 (Timbre)</dt>
            <dd>波形 (エンベロープ) が大きく影響する</dd>
          </dl>
          <p>
            <b>大きく影響する</b>という表現に注意してください. 例えば, 音の大きさは振幅のみで決定されるわけではないということです. 知覚は主観的な指標であり,
            振幅・周波数・波形は物理量だからです. 物理現象である音と知覚を関連づける指標として, <b>音響特徴量</b> (等ラウドネス曲線や基本周波数,
            セントロイドなど) が知られていますが, Web Audio API を理解するうえでそこまで知っている必要はないので, 詳細を知りたい場合は,
            これらのキーワードをもとに, より最適なドキュメントや書籍がたくさんあるのでそちらを参考にしてください.
          </p>
        </section>
        <section id="section-web-audio-api-relation-to-sound">
          <h3>Web Audio API と音の関係</h3>
          <p></p>
          <section id="section-gain-relation-to-sound">
            <h4>GainNode の gain プロパティと音の大きさ</h4>
            <p>
              <code>GainNode</code>の <code>gain</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の大きさを変えることができます.
              物理的な視点で見ると, 振幅を操作することによって, 音の大きさを変えています.
            </p>
            <img src="images/gain-gain.png" alt="GainNode gain" width="1232" height="770" loading="lazy" />
          </section>
          <section id="section-frequency-relation-to-sound">
            <h4>OscillatorNode の frequency プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>frequency</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の高さを変えることができます.
              物理的な視点で見ると, 周波数を操作することによって, 音の高さを変更しています.
            </p>
            <img src="images/oscillator-frequency.png" alt="OscillatorNode frequency" width="1232" height="770" loading="lazy" />
            <p>
              仕様では, <code>frequency</code> プロパティのとりうる値の範囲は, 負のナイキスト周波数からナイキスト周波数までですが (ナイキスト周波数は,
              <a href="#section-analog-to-digital-conversion-sampling">サンプリング</a>のセクションで解説しています. ナイキスト周波数について理解がなければ,
              おおよそ, <code>-20 kHz</code> ~ <code>20 kHz</code> と大雑把に把握していただいて問題ないです),
              音楽アプリケーションなどで出力する音としてはそこまで設定できてもあまり意味はないでしょう. その理由は,
              <b>人間が聴きとることが可能な音の周波数の範囲は <code>20 Hz</code> ~ <code>20000 Hz</code> (<code>20 kHz</code>) 程度だからです</b>.
            </p>
            <p>
              さらに, <b>音程</b> (音の高さの差) として知覚可能な周波数の上限, 言い換えると, 音楽として有効な音の周波数はもっと低くなります (ピアノ 88
              鍵の音域を参照してください).
            </p>
            <figure>
              <svg id="svg-figure-frequency-and-piano-frequency" width="1196" height="282" data-highlights="0,87" />
              <figcaption>ピアノ 88 鍵と周波数</figcaption>
            </figure>
          </section>
          <section id="section-detune-relation-to-sound">
            <h4>OscillatorNode の detune プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>detune</code> プロパティ (<code>AudioParam</code>) を利用することでも, 音の高さを変えることができます.
              物理的な視点も <code>frequency</code> プロパティと同じです. ただし, <code>detune</code> プロパティは, 音楽的な視点で音の高さを変更します.
              <code>detune</code> プロパティの用途は, (音楽で言う) 半音よりも小さい範囲で音の高さを調整したり,
              オクターブ違いの音を生成・合成したりするために利用します. この機能によって, きめ細かいサウンド生成が可能になったり,
              サウンドを合成する場合において厚みをもたせることが可能になったりします. シンセサイザーのファインチューン機能や, エフェクターの 1
              種であるオクターバーを実現するためにあると言えるでしょう.
            </p>
            <img src="images/oscillator-detune.png" alt="OscillatorNode detune" width="1196" height="770" loading="lazy" />
            <p>
              <code>frequency</code> プロパティの単位は Hz (ヘルツ) で, 波が 1 sec の間に何回発生するのかを意味していました. 一方で,
              <code>detune</code> プロパティの単位は <b>cent</b> (セント) です. これは, 音楽の視点から音の高さをとらえた単位で,
              <b>1 オクターブの音程を 1200 で等分した値</b>です.
            </p>
            <p>
              1 つ高いラとか, 1 つ低いラのことを, 1 オクターブ高いラ, 1 オクターブ低いラと表現することがあります.
              音楽的な視点でのオクターブはまさにそういう意味です.
            </p>
            <p>
              オクターブを物理的な視点でみると, <b>周波数比が 1 : 2 の関係にある音程</b>を意味しています. 具体的に説明すると, いわゆる普通のラ (A) (ギターの第 5
              弦の開放弦) の周波数は <code>440 Hz</code> です (キャリブレーションチューニングなどしている場合は別ですが ...). この音を基準に考えると, 1
              オクターブ高いラの周波数は <code>880 Hz</code> です. 周波数比が, 440 : 880 = 1 : 2 になります.
            </p>
            <p>
              話を cent に戻すと, この 1 : 2 の音程を 1200 で割った値が <code>1 cent</code> というわけです. なぜ, 1200 ?
              と疑問に思う方もいらっしゃると思いますが, ピアノをされる方は直感で理解できると思います. ピアノをされない方のために, 1
              オクターブの音程間にピアノの鍵盤がいくつあるか数えてみましょう. 1 オクターブ間であればいいので, 好きな音から始めてください.
            </p>
            <figure>
              <svg id="svg-figure-12-equal-temperament" width="1196" height="162" data-highlights="39,40,41,42,43,44,45,46,47,48,49,50" />
              <figcaption>1 オクターブの鍵盤数</figcaption>
            </figure>
            <p>
              数えてみると, <b>12</b> 個の鍵盤があります. 1 オクターブ間の音程を 1200 で割った (1200 分割した) 値が <code>1 cent</code> でしたので, 1
              オクターブ間の音程を 12 分割すると, <code>100 cent</code> ということになります. つまり, <code>100 cent</code> 値が高くなると,
              右隣の鍵盤の音の高さに変わるということです.
            </p>
            <p>
              例として, <code>440 Hz</code> のラ (A) の音を <code>100 cent</code> 高くすると, 右隣の鍵盤の ラ# (A#) に, さらに <code>100 cent</code> 高くすると,
              シ (B) になります. このように, <code>-100 cent</code> ~ <code>100 cent</code> の間の値を設定することによって,
              半音以下の音の高さの調整が可能になるわけです. また, <code>1200 cent</code>, あるいは, <code>-1200 cent</code> と <code>1200 cent</code>
              ごとに値を設定することにより, オクターブ単位で調整することも可能です.
            </p>
            <p>
              音楽では, 1 オクターブの音程を 12 等分した周波数比の関係を 12 平均音律と呼びます. 12 平均音律においては, 隣り合う音, つまり, 半音の周波数比は,
              およそ, 1 : 1.059463 (正確には, 1 : <span class="math-inline">$2^{\left(1 / 12\right)}$</span>) で, これが <code>100 cent</code> となるわけです.
            </p>
          </section>
          <section id="section-type-relation-to-sound">
            <h4>OscillatorNode の type プロパティと音色</h4>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値を利用することで, 正弦波だけでなく,
              矩形波やノコギリ波, 三角波を生成することができます. それによって, 音色を変化させることが可能です. ちなみに,
              波形の概形は<b>エンベロープ</b>と呼ばれます. <code>OscillatorNode</code> のみで制御可能な範囲では, この
              <code>type</code> プロパティに応じたエンベロープが音色に大きく影響しています.
            </p>
          </section>
          <p>
            このセクションのまとめとして, 基本波形, 振幅, 周波数を変化させたときの波形を視覚化するデモとなります. 波形の変化とともに, 知覚する音 (音の 3 要素)
            の変化を体感してみてください.
          </p>
          <div class="app-container">
            <svg id="svg-oscillator" class="svg-oscillator" width="720" height="240"></svg>
            <div>
              <button type="button" id="button-oscillator" class="button-oscillator">start</button>
              <form id="form-oscillator-type" class="form-oscillator-type">
                <label><span>sine</span><input type="radio" name="radio-oscillator-type" value="sine" checked /></label>
                <label><span>square</span><input type="radio" name="radio-oscillator-type" value="square" /></label>
                <label><span>sawtooth</span><input type="radio" name="radio-oscillator-type" value="sawtooth" /></label>
                <label><span>triangle</span><input type="radio" name="radio-oscillator-type" value="triangle" /></label>
              </form>
              <div class="ranges-oscillator">
                <label><span>gain</span><input type="range" id="range-gain" value="1" min="0" max="1" step="0.05" /></label>
                <label><span>frequency</span><input type="range" id="range-frequency" value="440" min="27.5" max="8000" step="0.5" /></label>
                <label><span>detune</span><input type="range" id="range-detune" value="0" min="-600" max="600" step="1" /></label>
              </div>
            </div>
          </div>
        </section>
      </section>
      <section id="section-oscillator-node">
        <h2>OscillatorNode</h2>
        <p>
          Web Audio API のアーキテクチャを解説するうえで, <code>OscillatorNode</code> は少し説明しましたが, このセクションでは, Web Audio API
          におけるサウンド生成・合成のベースとなる, <code>OscillatorNode</code> についてその詳細を解説します.
        </p>
        <p>
          シンセサイザーの基本波形の生成・合成, モジュレーション系エフェクターで必須となる LFO (Low-Frequency Oscillator) など, Web Audio API
          において用途の広い, コアとなる <code>AudioNode</code> です. LFO に関しては, エフェクターのセクションで解説するので,
          このセクションでは基本波形の生成・合成に関して解説します.
        </p>
        <section id="section-oscillator-node-type">
          <h3>type プロパティ (OscillatorOptions)</h3>
          <p>
            ただし, <code>&apos;custom&apos;</code> のみは特殊で, 直接値を設定するとエラーが発生します. これは, <code>OscillatorNode</code> の
            <code>setPeriodicWave</code> メソッドによって, 自動的に <code>&apos;custom&apos;</code> に設定されます. また, その引数として,
            <code>AudioContext</code> の <code>createPeriodicWave</code> メソッドで波形テーブルを生成する必要があります. 波形テーブルの生成は,
            スペクトルや倍音などオーディオ信号処理の知識が必要になるので, 別のセクションで解説します.
          </p>
        </section>
        <section id="section-oscillator-node-frequency-and-detune">
          <h3>frequency プロパティ (AudioParam) / detune プロパティ (AudioParam)</h3>
          <p>
            周波数を制御して音の高さを変更します. <code>frequency プロパティ</code> と <code>detune</code> プロパティを合わせて算出される周波数 (<span
              class="math-inline"
              >$f_{\mathrm{computed}}$</span>) は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$f_{\mathrm{computed}} = \mathrm{frequency} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
          <p>
            この数式は, <code>frequency</code> は物理的な視点 (Hz) で周波数を制御, <code>detune</code> は音楽的な視点 (cent)
            で周波数を制御することを意味しています.
          </p>
        </section>
        <section id="section-oscillator-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>OscillatorNode</code> のプロパティを設定して音の高さや音色を制御することはそれほど難しくないかと思います. また, 発音し続けるか, 1 度だけ発音
            (<code>start</code> メソッド)・停止 (<code>stop</code> メソッド) する場合も直感的に実装可能です. おそらく, 多くの場合, ハマってしまうのが,
            <code>OscillatorNode</code> の発音と停止を繰り返す場合です.
          </p>
          <p>
            <code>OscillatorNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>OscillatorNode</code> インスタンスは再度, 発音 (停止)
            することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合, <code>OscillatorNode</code> インスタンスを再生成して,
            再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code>stop</code> メソッド) を実行する必要があります.
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 発音・停止することを期待していますが, 2 回目のクリック以降は, 発音されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();
const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  // Start immediately
  // But, cannot start since the second times ...
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  // Stop immediately
  oscillator.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>
            期待する動作, つまり, 発音・停止を繰り返すするには, 一度 <code>start</code>・<code>stop</code>した
            <code>OscillatorNode</code> インスタンスは破棄して, 再度 <code>OscillatorNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // Start immediately
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (oscillator === null) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);

  // GC (Garbage Collection)
  oscillator = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>このような仕様なので, <code>start</code> メソッドを続けて呼んだり, <code>stop</code> メソッドを続けて呼んだりしても, エラーが発生します.</p>
          <p>
            <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は, さまざまなプラットフォームのオーディオ API のなかでも Web Audio
            API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web ではないプラットフォームのオーディオ API はここまで抽象化されている API
            すら少ないと思います).
          </p>
        </section>
        <section id="section-oscillator-node-synthesize">
          <h3>基本波形の合成</h3>
          <p>
            基本波形の合成, すなわち, Web Audio API における <code>OscillatorNode</code> の合成は直感的で, 必要なだけ
            <code>OscillatorNode</code> インスタンスを生成して, (最後の) 接続先として <code>AudioDestinationNode</code> を指定するだけです.
          </p>
          <p>
            ただし, そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています
            (逆に, この音割れ (クリッピング) をエフェクトとして使うのが歪み系エフェクターです). もしくは,
            <code>DynamicsCompressorNode</code> を接続して振幅を制御して, 意図しない音割れを防ぐこともできます (ただし, 厳密には,
            コンプレッサーは振幅の小さい音も相対的に大きくするので, 物理的にはまったく同じではありません).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let oscillatorC = null;
let oscillatorE = null;
let oscillatorG = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillatorC !== null) || (oscillatorE !== null) || (oscillatorG !== null)) {
    return;
  }

  oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
  oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
  oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

  const gain = new GainNode(context, { gain: 0.25 });

  // OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  oscillatorC.connect(gain);
  oscillatorE.connect(gain);
  oscillatorG.connect(gain);
  gain.connect(context.destination);

  // Start immediately
  oscillatorC.start(0);
  oscillatorE.start(0);
  oscillatorG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillatorC === null) || (oscillatorE === null) || (oscillatorG === null)) {
    return;
  }

  // Stop immediately
  oscillatorC.stop(0);
  oscillatorE.stop(0);
  oscillatorG.stop(0);

  // GC (Garbage Collection)
  oscillatorC = null;
  oscillatorE = null;
  oscillatorG = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
        </section>
      </section>
      <section id="section-audio-buffer-source-node">
        <h2>AudioBufferSourceNode</h2>
        <p>
          <b><code>AudioBufferSourceNode</code></b> は, <b>ワンショットオーディオの再生</b>を目的に利用します. ワンショットオーディオとは,
          ピアノやギターなど実際の楽器の音源を収録した WAVE ファイルや MP3 ファイルのことです. Web Audio API の仕様では, ユースケースとして,
          楽曲データに関しては, <b><code>MediaElementAudioSourceNode</code></b> を利用することを想定しているので, この点は注意が必要です. ただし,
          <code>AudioBufferSourceNode</code> を楽曲データの再生に使うこともできます. 現実解としてユースケースに反した利用をすることも多いです (これは,
          <code>AudioBufferSourceNode</code> がオーディオデータの実体である <code>AudioBuffer</code> インスタンスをもつので,
          オーディオ信号処理が適用しやすいことが理由として考えられます).
        </p>
        <p>このセクションでは, 仕様上のユースケースであるワンショットオーディオの再生を目的に, <code>AudioBufferSourceNode</code> を解説します.</p>
        <p>
          ところで, ワンショットオーディオの再生であれば, 同じことは <code>HTMLAudioElement</code> (<code>audio</code> タグ) でも可能な場合もあります. 事実, Web
          Audio API が仕様策定される以前は, そのようなユースケースも想定して, <code>Audio</code> コンストラクタが定義されています. しかしながら,
          <code>HTMLAudioElement</code> (<code>Audio</code> コンストラクタ) によるワンショットオーディオの再生は以下のような問題があります.
        </p>
        <ul>
          <li>JavaScript のタイマー (<code>setInterval</code> や <code>setTimeout</code>) では, 正確なスケジュールングが難しい</li>
          <li><code>HTMLAudioElement</code> のイベントハンドラでも精度が粗く, 正確なスケジュールングが難しい</li>
          <li>同時発音数の制限</li>
          <li>ワンショットオーディオに対して, さらにオーディオ処理を付加したいユースケース</li>
        </ul>
        <p>
          これらの問題を, ある程度容易に解決してくれるのが <code>AudioBufferSourceNode</code> です (もっとも, <code>AudioBufferSourceNode</code> を利用しても,
          コンピュータのリソースは有限なので, 計算量が多い場合や他のプロセスがリソースを多く消費している場合などは,
          少なからずスケジューリングも正確でなくなります).
        </p>
        <img src="images/audio-buffer-source-node.png" alt="AudioBufferSourceNode" width="1232" height="770" loading="lazy" />
        <section id="section-audio-buffer-source-node-buffer">
          <h3>buffer プロパティ</h3>
          <p>
            <code>AudioBufferSourceNode</code> において, 最も重要と言えるのが, <b><code>buffer</code></b> プロパティであり, これは,
            <b><code>AudioBuffer</code></b> インスタンスを参照します. <code>AudioBuffer</code> とは, オーディオデータの実体 (を抽象化するクラス) です.
          </p>
          <img src="images/audio-buffer.png" alt="AudioBuffer" width="1232" height="770" loading="lazy" />
          <section id="section-audio-buffer">
            <h4>AudioBuffer</h4>
            <p>
              <code>AudioBuffer</code> クラスは, オーディオデータの実体ですが, 直接的にアクセスすることはできません. そのためのメソッドや,
              デジタル化されたオーディオデータに必要なパラメータ (サンプリングレートやチャンネル数, オーディオデータ全体のサイズなど) を定義しています.
            </p>
            <section id="section-audio-buffer-sample-rate">
              <h5>sampleRate プロパティ</h5>
              <p>
                サンプリング周波数です. これは, <code>AudioContext</code> インスタンスの<code>sampleRate</code> プロパティと同じ値です. つまり,
                注意しておきたいのは, オーディオデータのサンプリグ周波数ではなということです (ちなみに,
                なぜこのような仕様なのかこのサイトのオーナーも理解できていません. 直感的にはオーディオデータのサンプリング周波数に思いますが).
              </p>
            </section>
            <section id="section-audio-buffer-length">
              <h5>length プロパティ</h5>
              <p>
                1 チャネルにおける, オーディオデータのサイズです. つまり, <code>sampleRate</code> プロパティの逆数である<code>サンプリング周期</code>と
                <code>length</code> プロパティを乗算した値が, オーディオデータの再生時間となります (次に解説する,
                <code>duration</code> プロパティの値と同じになります).
              </p>
              <div class="math-block">$\mathrm{duration} = \frac{\mathrm{length}}{\mathrm{sampleRate}}$</div>
            </section>
            <section id="section-audio-buffer-duration">
              <h5>duration プロパティ</h5>
              <p>
                オーディオデータの再生時間 (単位は <code>sec</code>) です. 先ほど解説したように, <code>sampleRate</code> プロパティと
                <code>length</code> プロパティと関連している値となります.
              </p>
            </section>
            <section id="section-audio-buffer-number-of-channels">
              <h5>numberOfChannels プロパティ</h5>
              <p>
                オーディオデータのチャンネル数です. 例えば, モノラルであれば <code>1</code>, ステレオであれば <code>2</code>, 5.1 チャンネルであれば
                <code>6</code> になります. 次に解説する, <code>getChannelData</code> メソッドの引数の上限を決めている値になっています.
              </p>
            </section>
            <section id="section-audio-buffer-get-channel-data">
              <h5>getChannelData メソッド</h5>
              <p>
                <code>getChannelData</code> メソッドで引数で指定したチャンネルのオーディオデータを <code>Float32Array</code> として取得することが可能です.
                引数となるチャンネルの指定は <code>0</code> から <code>numberOfChannels - 1</code> までです. 例えば, ステレオ (<code>numberOfChannels</code> が
                <code>2</code>)であれば, <code>getChannelData(0)</code> で左チャンネルのオーディオをデータを <code>Float32Array</code> で取得し,
                <code>getChannelData(1)</code> で右チャンネルのオーディオデータを<code>Float32Array</code> で取得することができます.
              </p>
            </section>
            <section id="section-audio-buffer-copy">
              <h5>copyFromChannel メソッド / copyToChannel メソッド</h5>
              <p>
                他に, <code>AudioBuffer</code> をコピーするためのメソッドがあります. ワンショットオーディオの再生においてはおそらく使うことはないので,
                必要であれば, 仕様や MDN などを参考にしてください.
              </p>
            </section>
          </section>
          <section id="section-create-audio-buffer">
            <h4>AudioBuffer の生成</h4>
            <p>
              <code>AudioBuffer</code> クラスに関して簡単に解説しましたが, 肝心なのは
              <code>AudioBuffer</code> インスタンスをどうやって生成するのかということだと思います. Web Audio API では,
              <b><code>decodeAudioData</code></b> メソッドを利用するか, <b><code>createBuffer</code></b> メソッドを利用することによって,
              <code>AudioBuffer</code> インスタンスを生成可能です.
            </p>
            <p>
              もっとも, ワンショットオーディオ再生目的であれば, <code>createBuffer</code> メソッドを利用することはおそらくなく,
              <code>ArrayBuffer</code> インスタンスから <code>AudioBuffer</code> インスタンスを生成する
              <code>decodeAudioData</code> メソッドを利用することになると思います. したがって, まずは,
              <code>ArrayBuffer</code> インスタンスの取得に関して解説します (これは Web Audio API の解説というよりは, JavaScript で
              <code>ArrayBuffer</code> インスタンスを取得する方法なので, すでにご存知の場合はスキップして問題ないです).
            </p>
            <section id="section-array-buffer-and-decode-audio-data">
              <h5>ArrayBuffer の取得と decodeAudioData メソッド</h5>
              <p>
                クライアントサイド JavaScript で <code>ArrayBuffer</code> を取得するには, Web にあるリソースであれば, <code>Fetch API</code> (もしくは,
                <code>XMLHttpRequest</code>), ユーザーのファイルシステムから選択するのであれば <code>File API</code> と
                <code>FileReader API</code> を使うことになります.
              </p>
              <p>
                ワンショットオーディオ再生の場合, アプリケーション側であらかじめオーディオデータを Web にアップロードしているケースがほとんどなので,
                このセクションでは, <code>Fetch API</code> で <code>ArrayBuffer</code> を取得する実装を解説します.
              </p>
              <p>
                <code>Fetch API</code> は, <code>fetch</code> 関数, <code>Headers</code> オブジェクト, <code>Request</code> オブジェクト,
                <code>Response</code> オブジェクトの総称ですが, ほとんどのケースで明示的に利用するのは, <code>fetch</code> 関数の呼び出しです.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    // TODO: Create instance of `ArrayBuffer` by calling `decodeAudioData`
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                <code>fetch</code> 関数のデフォルトの HTTP メソッドは GET なので, ワンショットオーディオの取得であれば, そのリソースの URL
                を指定すればよいでしょう. あとは, 取得した <code>Response</code> オブジェクトの <code>arrayBuffer</code> メソッドを呼び出して,
                <code>ArrayBuffer</code> インスタンスを取得するだけです. いずれの関数・メソッドも, <code>Promise</code> を返します. 可読性重視などであれば,
                <code>async</code>/<code>await</code> で実装してもよいでしょう.
              </p>
              <p>
                <code>ArrayBuffer</code> インスタンスが取得できたら, <code>AudioContext</code> インスタンスの <b><code>decodeAudioData</code></b> メソッドの第 1
                引数に, <code>ArrayBuffer</code> インスタンスを指定して, 第 2 引数に, 成功時のコールバック関数を指定します. このコールバック関数の引数に,
                <code>AudioBuffer</code> インスタンスが渡されます. 失敗した場合, 第 3 引数のコールバック関数が実行されます. このコールバック関数の引数には,
                <code>DOMException</code> インスタンスが渡されます.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      // Create instance of `AudioBufferSourceNode`
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                初期の頃は上記のような仕様でしたが, 最新の仕様では, 成功時は <code>Promise&lt;AudioBuffer&gt;</code> を返すので, 戻り値から
                <code>AudioBuffer</code> インスタンスを取得することも可能です.
              </p>
              <p>
                <code>decodeAudioData</code> メソッドの実行で 1 つ注意しなければならないのは, <code>decodeAudioData</code> メソッドも
                <b><a href="#section-autoplay-policy">Autoplay Policy</a></b> の影響を受けるということです. したがって,
                ユーザーインタラクティブなイベント発生後に実行する必要があります.
              </p>
            </section>
            <section id="section-create-buffer">
              <h5>createBuffer メソッド</h5>
              <p>
                <code>AudioBuffer</code> インスタンスを生成するには, <code>AudioContext</code> インスタンスの
                <b><code>createBuffer</code></b> メソッドを利用することでも可能です. 引数は, 第 1 引数にチャンネル数, 第 2 引数に 1
                チャンネルのオーディオデータのサイズ, 第 3 引数にサンプリング周波数を指定します. しかしながら, インスタンスは生成できるものの,
                オーディオデータをもっているわけではないので, ワンショットオーディオの再生において利用することはないでしょう. ユースケースとしては,
                オーディオデータから生成した <code>AudioBuffer</code> インスタンスからコピー (<code>copyFromChannel</code> メソッドや
                <code>copyToChannel</code> メソッドが必要なケース) が考えられます.
              </p>
            </section>
            <p>
              これで, ワンショットオーディオを再生する最低限の処理ができているので, あとは <code>AudioBufferSourceNode</code> のインスタンスを生成します
              (ファクトリメソッドで生成する場合, <code>createBufferSource</code> メソッドを利用します).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

      // If use `createBufferSource`
      // const source = context.createBufferSource();
      //
      // source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          </section>
        </section>
        <section id="section-audio-buffer-source-node-playback-rate-and-detune">
          <h3>playbackRate プロパティ / detune プロパティ</h3>
          <p>
            音楽用途でワンショットオーディオを使う場合, 対応するピッチの数だけ, ワンショットオーディオデータを作成するのは大変ですし, また, HTTP
            リクエストの送受信や <code>decodeAudioData</code> メソッドの実行も多くなってしまうのでパフォーマンス的にもよくありません. それを解決するのが,
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティです. これらは, 音の物理的な性質, つまり,
            <b>再生速度を変化させるとピッチも変化する</b>という性質を利用して, ピッチ (と再生時間) を変更します. 例えば, <code>playbackRate</code> を
            <code>2</code> に設定すれば, ピッチも 2 倍, つまり, 1 オクターブ高いピッチのオーディオデータの再生を同一の
            <code>AudioBuffer</code> インスタンスから可能です. <code>detune</code> は, cent 単位でピッチを変更します. ピッチを変更すると,
            再生時間も変わりますが, ワンショットオーディオは再生時間が短時間なので, この点が問題になることはほとんどないでしょう. いずれも,
            <code>AudioParam</code> インスタンスなので, 値を取得したり, 設定する場合は, <code>value</code> プロパティにアクセスします.
          </p>
          <p>
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティを考慮した, 実際の再生速度
            <span class="math-inline">$p_{\mathrm{computed}}$</span> は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$p_{\mathrm{computed}} = \mathrm{playbackRate} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
        </section>
        <section id="section-audio-buffer-source-node-loop">
          <h3>loop プロパティ / loopStart プロパティ / loopEnd プロパティ</h3>
          <p>
            ワンショットオーディオをループ再生させたい場合, <code>loop</code> プロパティを <code>true</code> に設定します. また, <code>loop</code> プロパティを
            <code>true</code> に設定することで, <code>loopStart</code> プロパティと <code>loopEnd</code> プロパティが有効になります. これらのプロパティは,
            ループ再生するオーディオデータの開始位置, 終了位置を秒単位で指定します.
          </p>
        </section>
        <section id="section-audio-buffer-source-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>AudioBufferSourceNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>AudioBufferSourceNode</code> インスタンスは再度,
            発音 (停止) することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合,
            <code>AudioBufferSourceNode</code> インスタンスを再生成して, 再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code
              >stop</code>
            メソッド) を実行する必要があります. この仕様は, <code>OscillatorNode</code> とまったく同じです (ただし,
            <code>AudioBuffer</code> インスタンスは使い回すことが可能です).
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 再生・停止することを期待していますが, 2 回目のクリック以降は, 再生されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const source = new AudioBufferSourceNode(context);

// AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Start immediately
  // But, cannot start since the second times ...
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            期待する動作, つまり, 再生・停止を繰り返すには, 一度 <code>start</code>・<code>stop</code> した (あるいは, <code>duration</code> まで再生した)
            <code>AudioBufferSourceNode</code> インスタンスは破棄して, 再度 <code>AudioBufferSourceNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let source = null;
let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  source = new AudioBufferSourceNode(context, { buffer });

  // AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);

  // Start immediately
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (source === null)) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            ワンショットオーディオも, 複数の <code>AudioBufferSourceNode</code> インスタンスを <code>AudioDestinationNode</code> に接続することで合成が可能です
            (そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています).
          </p>
          <p>
            また, 3 つの <code>AudioBufferSourceNode</code> インスタンスで, それぞれ <code>detune</code> プロパティの値を調整して, C
            メジャーコードを再生しています.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let sourceC = null;
let sourceE = null;
let sourceG = null;

let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  sourceC = new AudioBufferSourceNode(context, { buffer });
  sourceE = new AudioBufferSourceNode(context, { buffer });
  sourceG = new AudioBufferSourceNode(context, { buffer });

  sourceC.detune.value = 0;
  sourceE.detune.value = 400;
  sourceG.detune.value = 700;

  const gain = new GainNode(context, { gain: 0.25 });

  // AudioBufferSourceNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  sourceC.connect(gain);
  sourceE.connect(gain);
  sourceG.connect(gain);

  gain.connect(context.destination);

  // Start immediately
  sourceC.start(0);
  sourceE.start(0);
  sourceG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (sourceC === null) || (sourceE === null) || (sourceG === null)) {
    return;
  }

  // Stop immediately
  sourceC.stop(0);
  sourceE.stop(0);
  sourceG.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            <code>AudioBufferSourceNode</code> でも, <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は,
            さまざまなプラットフォームのオーディオ API のなかでも Web Audio API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web
            ではないプラットフォームのオーディオ API はここまで抽象化されている API すら少ないと思います).
          </p>
        </section>
      </section>
      <section id="section-media-element-audio-source-node">
        <h2>MediaElementAudioSourceNode</h2>
        <p>
          Web Audio API において, 楽曲データに対してなんらかのオーディオ信号処理を適用したい場合に利用するのが
          <b><code>MediaElementAudioSourceNode</code></b> です. 具体的には, <code>HTMLMediaElement</code> (<code>HTMLAudioElement</code> や
          <code>HTMLVideoElement</code>) のオーディオデータに対するオーディオ信号処理を適用する場合に利用します.
        </p>
        <img src="images/media-element-audio-source-node.png" alt="MediaElementAudioSourceNode" width="1232" height="770" loading="lazy" />
        <p>
          <code>HTMLMediaElement</code> を音源にするので, <code>MediaElementAudioSourceNode</code> コンストラクタの第 2 引数 (<b
            ><code>MediaElementAudioSourceOptions</code></b>
          型) の <b><code>mediaElement</code></b> プロパティ (もしくは, ファクトリメソッドの <code>createMediaElementSource</code> の引数) に,
          <code>HTMLMediaElement</code> を指定します.
        </p>
        <p>
          また, コンストラクタやファクトリメソッドに指定する <code>HTMLMediaElement</code> が HTML パース時点で,
          <code>src</code> 属性に指定しているメディアファイルが同一オリジンでない場合, クロスオリジン制限にかかってしまうので,
          <b><code>crossorigin</code> 属性に <code>&apos;anonymous&apos;</code> を設定</b>しておく必要あります. この属性と値の設定によって,
          <b>オリジン間リソース共有</b> (<b>CORS</b>: <b>Cross-Origin Resources Sharing</b>) が可能となります (<b
            ><code>HTMLMediaElement</code> のみで再生する場合は不要です</b>).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;!-- シューベルト 交響曲 第8番 ロ短調 D759 「未完成」 第1楽章 (余談ですが, X JAPAN の「ART OF LIFE」のモチーフになっている楽曲です) --&gt;
&lt;audio src=&quot;https://korilakkuma.github.io/Web-Music-Documentation/assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&quot; crossorigin="anonymous" controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const audioElement = document.querySelector(&apos;audio&apos;);

const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

// If use `createMediaElementSource`
// const source = context.createMediaElementSource(audioElement);

// MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);</code></pre>
        <p>
          <code>MediaElementAudioSourceNode</code> インスタンス生成には 2 点注意すべき点があります. 上記のサンプルコードのように,
          <code>HTMLMediaElement</code> に HTML パース時点で, <code>src</code> 属性にメディアファイルが指定されている場合は, 特に問題ありませんが,
          インタラクティブに, 例えば, ユーザーのファイルシステムからメディアファイルを選択するような場合,
          <b><code>HTMLMediaElement</code> の <code>loadstart</code> イベント発火以降にインスタンスを生成する必要があります</b> (逆に, HTML パース時点で
          <code>src</code> 属性にメディアファイルを指定している場合, <code>loadstart</code> イベントは発火しないので注意が必要です).
          <code>loadstart</code> イベント以降に発火するイベントであればよいので, <code>canplaythrough</code> イベントハンドラなどで
          <code>MediaElementAudioSourceNode</code> インスタンスを生成してもよいでしょう.
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <p>
          もう 1 点は, 1 つの <code>HTMLMediaElement</code> に対して 1 つの <code>MediaElementAudioSourceNode</code> インスタンスが対応しているという点です.
          例えば, <code>HTMLMediaElement</code> の <code>src</code> 属性のみを変更する場合,
          <code>MediaElementAudioSourceNode</code> インスタンスを再度生成するとエラーが発生します (逆に, 別のオブジェクトとなる
          <code>HTMLMediaElement</code> を指定する場合, <code>MediaElementAudioSourceNode</code> インスタンスを生成する必要があります).
        </p>
        <p>
          したがって, 先ほどのサンプルコードだと, 2 回以上, ファイルを選択してしまうと, 同じ <code>HTMLAudioElement</code> に対して, 複数回
          <code>MediaElementAudioSourceNode</code> インスタンスが生成されてエラーが発生してしまうので, 以下のように変更します.
        </p>
        <p>
          また, <code>File API</code> から選択した楽曲データを, <code>HTMLMediaElement</code> の <code>src</code> 属性に指定する場合, Object URL を利用します
          (<code>FileReader API</code> を使って Data URL を利用しても可能ですが, 実装が増えるだけなので, なんらかの理由がなければ
          <code>createObjectURL</code> を利用して Object URL を設定するのがよいでしょう).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

let source = null;

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <section id="section-media-element-audio-source-node-start-and-stop">
          <h3>再生と停止</h3>
          <p>
            <code>MediaElementAudioSourceNode</code> に楽曲データを再生・停止するためのメソッドはありません. 再生や一時停止は, コンストラクタの引数に指定した
            <code>HTMLMediaElement</code> の <code>play</code> / <code>pause</code> メソッドを実行します. したがって, <code>OscillatorNode</code> や
            <code>AudioBufferSourceNode</code> のように使い捨てのノードではない, つまり, インスタンスを再度生成して
            <code>AudioDestinationNode</code> に再度接続する必要もないので, この点は直感的な仕様と言えます.
          </p>
          <p>
            あとは, <code>AudioDestinationNode</code> に接続すれば, 再生・停止することは簡単ですが, これでは
            <code>HTMLMediaElement</code> をそのまま利用するほうが合理的なので, 簡易例として, オーディオ信号処理を適用していることがわかるように,
            <code>BiquadFilterNode</code> を利用して Low-Pass Filter (低域通過フィルタ) を使ったサンプルコードです. カットオフ周波数を変更すると,
            音の輪郭が変わることを確認してみてください (<code>BiquadFilterNode</code> に関しては,
            <a href="#section-effectors-filter-biquad-filter-node">フィルタのセクション</a>で詳細を解説します).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

const inputCutoffElement = document.getElementById(&apos;range-cutoff&apos;);
const spanElement        = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = `${lowpass.frequency.value} Hz`;
});

// UI (by `controls` attribute) plays and pauses media
audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});</code></pre>
        </section>
        <section id="section-html-media-element-and-media-element-audio-source-node">
          <h3>HTMLMediaElement と MediaElementAudioSourceNode</h3>
          <p>
            すでにサンプルコードを実行して, お気づきになったかもしれませんが,
            <code>HTMLMediaElement</code> のプロパティやイベントハンドラはすべて利用することが可能です. <code>volume</code> や <code>muted</code>,
            <code>playbackRate</code> は再生する楽曲データそのものに影響します. <code>autoplay</code> や <code>loop</code> は再生における UX に影響します. また,
            実際のプロダクトでは, <code>loadedmetadata</code> イベント, <code>canplaythrough</code>イベント, <code>timeupdate</code> イベント,
            <code>ended</code> イベントなどで, UI を更新するイベントハンドラを実行することも多いでしょう. このドキュメントですべてを解説することはできないので,
            <a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement</a>
            の仕様などを参考にしてください.
          </p>
          <p>
            よくある実装として, <code>loadedmetadata</code> イベントで <code>duration</code> プロパティ (トータルの再生時間秒数) を取得,
            <code>timeupdate</code> イベントで <code>currentTime</code> プロパティ (現在の再生位置) を更新,
            <code>ended</code> イベントで初期表示に戻すというのは Web Audio API に直接関係はありませんが, メディアデータをあつかう Web
            アプリケーションでは必須になるような実装なので理解しておいて損はないでしょう. また, <code>MediaElementAudioSourceNode</code> の解説に着目するために
            <code>HTMLMediaElement</code> の <code>controls</code> 属性での UI で再生・一時停止を実装していましたが,
            再生・停止ボタンも実装したサンプルコードです. コードをご覧になると理解できるかもしれませんが, Web Audio API
            のコードは変更されていないことにも着目してみてください.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;play&lt;/button&gt;
&lt;span id=&quot;print-current-time&quot;&gt;00 : 00&lt;/span&gt; / &lt;span id=&quot;print-duration&quot;&gt;00 : 00&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;audio /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const inputElement  = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement  = document.querySelector(&apos;audio&apos;);

const spanCurrentTimeElement = document.getElementById(&apos;print-current-time&apos;);
const spanDurationElement    = document.getElementById(&apos;print-duration&apos;);
const inputCutoffElement     = document.getElementById(&apos;range-cutoff&apos;);
const spanCutoffElement      = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanCutoffElement.textContent = `${lowpass.frequency.value} Hz`;
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});

audioElement.addEventListener(&apos;loadedmetadata&apos;, () =&gt; {
  spanDurationElement.textContent = `${Math.trunc(audioElement.duration / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.duration) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;timeupdate&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = `${Math.trunc(audioElement.currentTime / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.currentTime) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;ended&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = &apos;00 : 00&apos;;
});

buttonElement.addEventListener(&apos;click&apos;, async () =&gt; {
  if (audioElement.paused) {
    await audioElement.play();

    buttonElement.textContent = &apos;pause&apos;;
  } else {
    audioElement.pause();

    buttonElement.textContent = &apos;play&apos;;
  }
});</code></pre>
        </section>
      </section>
    </main>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/prism.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"]
          ]
        }
      };
    </script>
    <script defer src="docs.js"></script>
  </body>
</html>
