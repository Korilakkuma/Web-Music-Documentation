<!doctype html>
<html lang="ja" prefix="og: http://ogp.me/ns#">
  <head>
    <meta charset="UTF-8" />
    <title>Web Music Documentation</title>
    <meta name="description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta name="keywords" content="Web Music, Web Audio, Audio Signal Processing, Music" />
    <meta name="author" content="Korilakkuma (Tomohiro IKEDA)" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes, viewport-fit=cover" />
    <meta name="format-detection" content="telephone=no" />
    <meta name="theme-color" content="#fafafa" />
    <meta property="og:description" content="Web Music Documentation for Web Audio API, Web MIDI API ... etc" />
    <meta property="og:image" content="https://korilakkuma.github.io/Web-Music-Documentation/images/icon.png" />
    <meta property="og:site_name" content="Web Music Documentation" />
    <meta property="og:title" content="Web Music Documentation" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <meta name="twitter:card" content="summary" />
    <link rel="canonical" href="https://korilakkuma.github.io/Web-Music-Documentation/" />
    <link rel="icon" href="images/icon.png" type="image/png" />
    <link rel="apple-touch-icon" href="images/icon.png" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/themes/prism-okaidia.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.css" type="text/css" media="all" />
    <link rel="stylesheet" href="docs.css" type="text/css" media="all" />
    <script type="application/ld+json">
      {
        "@context": "http://schema.org",
        "@type": "TechArticle",
        "name": "Web Music Documentation",
        "image": "https://korilakkuma.github.io/Web-Music-Documentation/image/icon.png",
        "url": "https://korilakkuma.github.io/Web-Music-Documentation/",
        "author": {
          "@type": "Person",
          "name": "Korilakkuma (Tomohiro IKEDA)"
        }
      }
    </script>
  </head>
  <body>
    <header>
      <h1 lang="en">Web Music Documentation</h1>
    </header>
    <main id="document-top">
      <section id="section-about-web-music">
        <h2>Web Music</h2>
        <p>
          <strong>Web Music</strong> とは, Web (ブラウザ) をプラットフォームにした音楽アプリケーション, そして, そのような Web
          アプリケーションを実装するために必要となる, クライアントサイド JavaScript API (ブラウザ API) の総称です. これは, 一般的な技術用語ではなく,
          技術マーケティング的な造語です.
        </p>
        <p>具体的には, 以下のような, クライアントサイド JavaScript API の総称です.</p>
        <ul>
          <li><a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API</a></li>
          <li><a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement (HTMLAudioElement)</a></li>
          <li><a href="https://www.w3.org/TR/webrtc/" target="_blank" rel="noopener noreferrer">WebRTC</a></li>
          <li><a href="https://www.w3.org/TR/webmidi/" target="_blank" rel="noopener noreferrer">Web MIDI API</a></li>
          <li><a href="https://www.w3.org/TR/webcodecs/" target="_blank" rel="noopener noreferrer">WebCodecs API</a></li>
          <li><a href="https://www.w3.org/TR/mediasession/" target="_blank" rel="noopener noreferrer">Media Session</a></li>
        </ul>
        <p>
          本サイト制作開始時点の 2023 年時点で Web Audio API と WebRTC に関しては, <b>W3C recommendation</b>, HTMLMediaElement に関しては,
          <b>HTML Living Standard</b> (2019 年 6 月以降, HTML や DOM に関わる仕様策定は <abbr title="World Wide Web Consortium">W3C</abbr> ではなく
          <abbr title="Web Hypertext Application Technology Working Group">WHATWG</abbr> が仕様策定の主体になることが決定されたので, HTMLMediaElement は HTML
          Living Standard です) となっており, モダンブラウザであれば利用することが可能です (ただし, クライアントサイド JavaScript の宿命ではありますが, OS
          やブラウザによって挙動が異なることは少なからずあるので, 移植性まで考慮すると, そのためのクロスブラウザ対応の問題は必要となります).
          これらのクライアントサイド JavaScript API は 2010 年代前半ごろは, <b>HTML5</b> というバズワード化したカテゴリに分類される API でした. 現在は, HTML5
          という仕様, あるいは, 用語が定着したからか, HTML5 というワードが使われることはほぼなくなりました. したがって, Web Music に関係する API も,
          膨大なクライアントサイド JavaScript API のうちのいくつかです (という認識が一般的と言えます).
        </p>
        <article id="section-web-browser-javascript">
          <h3>クライアントサイド JavaScript とは ?</h3>
          <p>
            クライアントサイド JavaScript とは, JavaScript の仕様の標準である ECMAScript (JavaScript の実行コンテキストに依存しない言語仕様. これに準拠している
            JavaScript のコードであれば, Web ブラウザでも, Node.js でも, ブラウザ拡張でも使うことができます) と, 実行コンテキスト
            (広義な意味でのプラットフォーム) である Web ブラウザで実行する場合にのみアクセス可能な API です (例えば, Web Music の API は Node.js
            で使うことはできません. また, Web ブラウザでも Web Workers が生成したスレッドでは, メインスレッド (UI スレッド)
            と実行コンテキストが異なるので使うことができません).
          </p>
        </article>
      </section>
      <section id="section-web-audio-api-overview">
        <h2>Web Audio API</h2>
        <p>
          Web Music のなかで, もっともコアな API が <strong>Web Audio API</strong> です. 言い換えると, Web
          をプラットフォームとした音楽アプリケーションを制作するほとんどの場合で必要になる API ということです. なぜなら,
          <code>HTMLAudioElement</code> はオーディオファイルを再生するための API で, 高度なオーディオ処理をすることはできず (<a
            href="https://egonelbre.com/project/jsfx/"
            target="_blank"
            rel="noopener noreferrer"
            >jsfx</a>
          のようにハッキーな実装をすることでエフェクトをかけるぐらいは可能ですが, 仕様のユースケースとして想定されている使い方ではありません),
          リアルタイム性やインタラクティブ性も考慮された API ではないからです (厳密には, 考慮された経緯もあって,
          <code>Audio</code> コンストラクタが定義されています). また, Web Music として, Web MIDI API や WebRTC を使う場合, 実際のオーディオ処理は Web Audio API
          が実行することになります.
        </p>
        <section id="section-web-music-history">
          <h3>Web Music の歴史</h3>
          <p>
            古くは, Internet Explorer が独自に
            <a href="https://www.tohoho-web.com/html/bgsound.htm" target="_blank" rel="noopener noreferrer"><code>bgsound</code></a> という HTML
            タグを実装しており, (Internet Explorer のみではありますが) ブラウザでオーディオをファイルを再生することが可能でした (現在の
            <code>HTMLAudioElement</code> に相当する HTML タグと言えます). そのあと, Java アプレットや ActionScript (Flash) によって, Web Audio API
            で実現できているような高度なオーディオ処理が可能となりました.
          </p>
          <p>
            しかし, これらは特定のベンダーに依存していたので, Flash や Silverlight などブラウザの拡張機能 (プラグイン) という位置づけでした. Web 2.0
            (もっと言えば, Ajax) を機にブラウザでも, ネイティブアプリケーションのような Web アプリケーションが実装されてくるようになると, これまで拡張機能
            (オーディオ処理だけでなく, グラフィックス, ストレージ, ローカルファイルへのアクセス, ソケットなど) に依存していたような機能をブラウザ標準で
            (クライアントサイド JavaScript API で) 実現できる流れが 2010 年ごろから活発になりました (このころ, HTML5 という位置づけで仕様策定され,
            モダンブラウザで実装されるようになりました).
          </p>
          <p>
            ドキュメントプラットフォームとしての Web に, アプリケーションプラットフォームが追加されていく転換期に, Web Audio API
            も仕様策定されて現在に至っています.
          </p>
          <ol>
            <li>
              <a href="https://www.w3.org/TR/2011/WD-webaudio-20111215/" target="_blank" rel="noopener noreferrer">草案 (Working Draft)</a> (2011 年 12 月 15
              日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio-1.0/" target="_blank" rel="noopener noreferrer">Web Audio API 1.0 勧告 (W3C recommendation)</a> (2021 年 6
              月 17 日に公開)
            </li>
            <li>
              <a href="https://www.w3.org/TR/webaudio/" target="_blank" rel="noopener noreferrer">Web Audio API 1.1 (最新の W3C Working Draft)</a> (2024 年 11
              月 5 日に公開)
            </li>
          </ol>
        </section>
        <article id="section-about-audio-data-api">
          <h3>Audio Data API</h3>
          <p>
            厳密な歴史を記載すると, Web Audio API よりわずかに先行して Firefox で
            <a href="https://wiki.mozilla.org/Audio_Data_API" target="_blank" rel="noopener noreferrer">Audio Data API</a> というブラウザオーディオ API
            が実装されていました. <code>HTMLAudioElement</code> の拡張という位置づけで, 出力するオーディオデータを直接演算する API がメインでした (Web Audio API
            の <code>ScriptProcessorNode</code> に相当する API). 間もなくして, Web Audio API に統一される方針となり, Firefox も Web Audio API
            のサポートを開始したので現在は削除されています.
          </p>
        </article>
      </section>
      <section id="section-about-document">
        <h2>このサイトに関して</h2>
        <p>
          このサイト (ドキュメント) の目的は, Web Music, その中核となる Web Audio API について解説しますが, W3C
          が公開している仕様のすべてを解説するわけではありません. また, JavaScript の言語仕様の解説は, サイトの目的ではないこともご了承ください (ただし, Web
          Audio API を使う上で, 必要となってくるクライアントサイド JavaScript API に関しては必要に応じて解説をします (例. <code>File API</code>,
          <code>Fetch API</code> など).
        </p>
        <p>このサイトは W3C が公開している仕様にとって代わるものではなく, Web Audio API の仕様の理解を補助するリファレンスサイトと位置づけてください.</p>
        <p>
          デスクトップブラウザでは少なくなりましたが, モバイルブラウザでは仕様とブラウザの実装に差異があり,
          仕様では定義されているのに動作しないということもあります. その場合には, 開発者ツールなどを活用して,
          実装されているプロパティやメソッドを確認してみてください.
        </p>
        <section id="section-about-sample-code">
          <h3>解説の JavaScript コードに関して</h3>
          <p>
            <b>ECMAScript 2015</b> 以降の仕様に準拠したコードで記載します. また, ビルドツールなどを必要としないように, TypeScript
            での記述やモジュール分割などもしません (端的には, コピペすればブラウザコンソールなどで実行できるようなサンプルコード, あるいは,
            コード片を記載します). 具体的には, 以下のような構文を使います.
          </p>
          <ul>
            <li><code>const</code>, <code>let</code> による変数宣言</li>
            <li>Template Strings</li>
            <li>アロー関数</li>
            <li>クラス</li>
            <li><code>Promise</code>, または, <code>async</code>/<code>await</code></li>
          </ul>
          <p>
            Web Audio API のコードも仕様で推奨されているコードを基本的に記載します (例えば, <code>AudioNode</code> インスタンスを生成する場合,
            コンストラクタ形式が推奨されているので, そちらを使います). ただし, 現時点であまりにも実装の乖離が大きい場合は, フォールバック的な解説として,
            実装として動作するコードを記載します.
          </p>
        </section>
        <section id="section-recommendation-browsers">
          <h3>推奨ブラウザ</h3>
          <p>
            閲覧自体は, モダンブラウザであれば特に問題ありませんが, 実際のサンプルコードを動作させることを考慮すると, デスクトップブラウザ, 特に, Web Audio API
            の仕様に準拠している <a href="https://www.google.com/chrome/" target="_blank" rel="noopener noreferrer">Google Chrome</a> もしくは
            <a href="https://www.mozilla.org/ja/firefox/" target="_blank" rel="noopener noreferrer">Mozilla Firefox</a> (いずれも最新バージョン) を推奨します
            (Google Chrome の場合, より高度な
            <a href="https://web.dev/articles/profiling-web-audio-apps-in-chrome" target="_blank" rel="noopener noreferrer">Web Audio API 専用のプロファイラ</a>があるのでおすすめです).
          </p>
        </section>
        <section id="section-prerequisite">
          <h3>前提知識と経験</h3>
          <p>
            前提知識としては, ECMAScript 2015 以降の JavaScript の言語仕様を理解していることと, Web ブラウザを実行環境にした JavaScript による Web
            アプリケーションを実装した経験ぐらいです. Web Audio API は, ユースケースにおいて想定されるオーディオ信号処理を抽象化しているので,
            オーディオ信号処理に対する理解がなくても, それなりのアプリケーションは制作できます (アプリケーションの仕様しだいでは不要になるぐらいです). もちろん,
            オーディオ信号処理の理解や Web 以外のプラットフォームでのオーディオプログラミングの経験 (特に, GUI
            で必要なリアルタイム性のオーディオプログラミングの経験) があれば, それは Web Audio API を理解するうえで活きます. Web Audio API
            が標準でサポートしないようなオーディオ処理を実現したいケースではむしろ必要になります.
          </p>
          <p>
            また, 音楽理論に対する知識も不要です. Web Audio API はユースケースとして, 音楽用途に限定していないからです. したがって, このサイトでは,
            アプリケーションによっては必要になるドメイン知識として位置づけます (もちろん, ユースケースとして, 音楽用途も想定されているので, Web
            をプラットフォームにした音楽アプリケーションを制作する場合には必要となるケースが多いでしょう).
          </p>
          <p>
            このサイトでは, オーディオ信号処理や音楽理論など必要に応じて解説します. Web Audio API が解説の中心ではありますが, Web Music
            アプリケーションを制作するための標準ドキュメントとなることを目指すからです (オーディオ信号処理や音楽理論を深入りする場合は,
            それぞれ最適なドキュメントや書籍がたくさんあるのでそちらを参考にしてください).
          </p>
        </section>
        <article id="section-skeptical">
          <h4>Web Audio API に対する懐疑的な意見</h4>
          <p>
            Web Audio API は, 他のプラットフォームのオーディオ API と比較すると, やや奇怪な API 設計であったり, 仕様策定されたころの JavaScript の事情と, 現代の
            JavaScript の事情が様変わりしたりしたことから, 懐疑的な意見もあります (参考
            <a href="https://zenn.dev/okuoku/articles/13c39882596c92" target="_blank" rel="noopener noreferrer">WebAudioは何故あんな事になっているのか</a>).
            しかしながら, この記事でも述べられているように,
            <q>実はWebAudioはオーディオAPIのオープンスタンダードとしては唯一生き残っている存在と言える。</q> これはたしかで, その点において学ぶ意義はあります.
            音楽アプリケーションとして Web をプラットフォームにする場合は必須となるでしょう.
          </p>
        </article>
        <section id="section-contribution-to-document">
          <h3>Issue と Pull Requests</h3>
          <p>
            プロローグの最後に, このサイト (ドキュメント) はオープンソースとして
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation" target="_blank" rel="noopener noreferrer">GitHub</a> に公開しています.
            このサイトのオーナーも完璧に理解しているわけではないので, 間違いもあるかと思います. その場合には, GitHub に
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/issues" target="_blank" rel="noopener noreferrer">issue</a> を作成したり,
            <a href="https://github.com/Korilakkuma/Web-Music-Documentation/pulls" target="_blank" rel="noopener noreferrer">Pull Requests</a>
            を送っていただいたりすると大変ありがたいです.
          </p>
          <p>それでは, Web Music の未来を一緒に開拓していきましょう !</p>
        </section>
      </section>
      <section id="section-getting-started">
        <h2>Getting Started</h2>
        <section id="section-audio-context">
          <h3>AudioContext</h3>
          <p>
            Web Audio API を使うためには, <code>AudioContext</code> クラスのコンストラクタを呼び出して,
            <code>AudioContext</code> インスタンスを生成する必要があります. <code>AudioContext</code> インスタンスが Web Audio API
            で可能なオーディオ処理の起点になるからです. <code>AudioContext</code> インスタンスを生成することで, Web Audio API
            が定義するプロパティやメソッドにアクセス可能になるわけです.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();</code></pre>
          <p>
            何らかの理由で, レガシーブラウザ (特に, モバイルブラウザ) もサポートしなければならない場合, ベンダープレフィックスつきの
            <code>webkitAudioContext</code> もフォールバックとして設定しておくとよいでしょう (少なくとも, デスクトップブラウザでは不要な処理で,
            これから将来においては確実に不要になる処理ではありますが).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">window.AudioContext = window.AudioContext || window.webkitAudioContext;

const context = new AudioContext();</code></pre>
          <p><code>AudioContext</code> インスタンスをコンソールにダンプしてみます.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

console.dir(context);</code></pre>
          <p>
            <code>AudioContext</code> インスタンスに様々なプロパティやメソッドが実装されていることがわかるかと思います. このドキュメントではこれらを
            (すべてではありませんが) メインに解説していくことになります. また, このように実装を把握することで, 仕様と実装の乖離を調査することにも役立ちます.
          </p>
          <img src="images/audio-context.png" alt="AudioContext" width="1232" height="770" loading="lazy" />
          <p>
            Web Audio API でオーディオ処理を実装するうえで意識することはほとんどありませんが, <code>AudioContext</code> は <code>BaseAudioContext</code> を拡張
            (継承) したクラスであることもわかります.
          </p>
          <img src="images/base-audio-context.png" alt="BaseAudioContext" width="1232" height="770" loading="lazy" />
          <section id="section-autoplay-policy">
            <h4>Autoplay Policy 対策</h4>
            <p>
              Web Audio API に限ったことではないですが, ページが開いたときに, ユーザーが意図しない音を聞かせるのはよくないという観点から (つまり, UX
              上好ましくないという観点から), ブラウザでオーディオを再生する場合,
              <a href="https://developer.chrome.com/blog/autoplay#web_audio" target="_blank" rel="noopener noreferrer">Autoplay Policy</a>
              という制限がかかります. これを解除するためには, <b>ユーザーインタラクティブなイベント</b> 発火後に
              <code>AudioContext</code> インスタンスを生成するか, もしくは, <code>AudioContext</code> インスタンスの <code>resume</code> メソッドを実行して
              <code>AudioContextState</code> を <code>&apos;running&apos;</code> に変更する必要があります. これをしないと, オーディオを鳴らすことができません.
              また, <code>decodeAudioData</code> など一部のメソッドが Autoplay Policy 解除まで実行されなくなります. ユーザーインタラクティブなイベントとは,
              <code>click</code>, <code>mousedown</code> や <code>touchstart</code> などユーザーが明示的に操作することによって発火するイベントのことです.
              したがって, <code>load</code> イベントや <code>mousemove</code> など, 多くのケースにおいてユーザが明示的に操作するわけではないようなイベントでは
              Autoplay Policy の制限を解除することはできません.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">document.addEventListener(&apos;click&apos;, () =&gt; {
  const context = new AudioContext();
});</code></pre>
            <p>
              <code>resume</code> メソッドで解除する場合 (この場合, コンソールには警告メッセージが表示されますが, Autoplay Policy
              は解除できるので無視して問題ありません).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

document.addEventListener(&apos;click&apos;, async () =&gt; {
  await context.resume();
});</code></pre>
            <p>
              <b>これ以降のセクションでは, 本質的なコードを表記したいので, Autoplay Policy は解除されている状態を前提とします.</b>
            </p>
          </section>
        </section>
        <section id="section-audio-node">
          <h3>AudioNode</h3>
          <p>
            Web Audio API におけるオーディオ処理の基本は, <code>AudioNode</code> クラスのインスタンス生成と <code>AudioNode</code> がもつ
            <code>connect</code> メソッドで <code>AudioNode</code> インスタンスを接続していくことです. <code>AudioNode</code> クラスは,
            それ自身のインスタンスを生成することはできず, <code>AudioNode</code> を拡張 (継承) したサブクラスのインスタンスを生成して, オーディオ処理に使います.
            <code>AudioNode</code> はその役割を大きく 3 つに分類することができます.
          </p>
          <ul>
            <li>サウンドの入力点となる <code>AudioNode</code> のサブクラス (<code>OscillatorNode</code>, <code>AudioBufferSourceNode</code> など)</li>
            <li>サウンドの出力点となる <code>AudioNode</code> のサブクラス (<code>AudioDestinationNode</code>)</li>
            <li>
              音響特徴量を変化させる <code>AudioNode</code> のサブクラス (<code>GainNode</code>, <code>DelayNode</code>, <code>BiquadFilterNode</code> など)
            </li>
          </ul>
          <p>
            現実世界のオーディオ機器に例えると, サウンドの入力点に相当する <code>AudioNode</code> のサブクラスが, マイクロフォンや楽器, 楽曲データなどに相当,
            サウンドの出力点に相当する <code>AudioNode</code> のサブクラスが. スピーカーやイヤホンなどに相当, そして, 音響特徴量を変化させる
            <code>AudioNode</code> のサブクラスがエフェクターやボイスチェンジャーなどが相当します.
          </p>
          <p>
            これらの, <code>AudioNode</code> のサブクラスを使うためには, <b>コンストラクタ呼び出し</b>, または,
            <b><code>AudioContext</code> インスタンスに実装されているファクトリメソッド</b> 呼び出す必要があります (ただし, サウンドの出力点となる
            <code>AudioDestinationNode</code> は <code>AudioContext</code> インスタンスの <code>destination</code> プロパティでインスタンスとして使えるので,
            コンストラクタ呼び出しやファクトリメソッドは定義されていません).
          </p>
          <p>例えば, 入力として, オシレーター (<code>OscillatorNode</code>) を使う場合, コンストラクタ呼び出しの実装だと以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);</code></pre>
          <p>
            インスタンス生成時には, その <code>AudioNode</code> のサブクラスに定義されているパラメータ (<code>OscillatorNode</code> の場合,
            <code>OscillatorOptions</code>) を指定することも可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 880 });</code></pre>
          <p>ファクトリメソッドでインスタンス生成する場合, 以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = context.createOscillator();</code></pre>
          <p>
            コンストラクタ呼び出しによる, <code>AudioNode</code> のサブクラスのインスタンス生成は, Web Audio API の初期には仕様策定されておらず,
            <code>AudioContext</code> インスタンスに実装されているファクトリメソッド呼び出す実装のみでした. インスタンス生成時に,
            パラメータを変更可能なことから, どちらかと言えば, コンストラクタ呼び出しによるインスタンス生成が推奨されているぐらいですが,
            ファクトリメソッドが将来非推奨になることはなく, また, 初期の仕様には仕様策定されていなかったことから,
            <b>レガシーブラウザの場合, コンストラクタ呼び出しが実装されていない場合もあります</b>. したがって, サポートするブラウザが多い場合は,
            ファクトリメソッドを, サポートするブラウザが限定的であれば, コンストラクタ呼び出しを使うのが現実解と言えるでしょう.
          </p>
          <section id="section-connect-audio-node">
            <h4>connect メソッド (AudioNode の接続)</h4>
            <p>
              現実世界の音響機器では, 入力と出力, あるいは, 音響変化も接続することで, その機能を果たします. 例えば, エレキギターであれば,
              サウンド入力を担うギターとサウンド出力を担うアンプ (厳密にはスピーカー) は, 単体ではその機能を果たしません.
              シールド線などで接続することによって機能します.
            </p>
            <p>
              このことは, Web Audio API の世界も同じです. (<code>AudioContext</code> インスタンスを生成して,) サウンド入力点となる
              <code>AudioNode</code> のサブクラスのインスタンス (先ほどのコード例だと, <code>OscillatorNode</code> インスタンス) と, サウンド出力点となる
              <code>AudioDestinationNode</code> インスタンスを生成しただけではその機能を果たしません. 少なくとも,
              サウンド入力点と出力点を接続する処理が必要となります (さらに, Web Audio API が定義する様々なノードと接続することで, 高度なオーディオ処理を実現する
              API として真価を発揮します).
            </p>
            <p>
              Web Audio API のアーキテクチャは, 現実世界における音響機器のアーキテクチャと似ています. このことは, Web Audio API
              の理解を進めていくとなんとなく実感できるようになると思います.
            </p>
            <p>
              Web Audio APIにおいて「接続」の役割を担うのが, <code>AudioNode</code> がもつ <b><code>connect</code> メソッド</b>です. 実装としては,
              <code>AudioNode</code> サブクラスのインスタンスの, <code>connect</code> メソッドを呼び出します. このメソッドの第 1 引数には, 接続先となる
              <code>AudioNode</code> のサブクラスのインスタンスを指定します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);</code></pre>
            <p>
              サウンドの入力点と出力点を接続し, 最小の構成を実装できました. しかし, まだ音は出せません. なぜなら,
              サウンドを開始するための音源スイッチをオンにしていないからです. 現実世界の音響機器も同じです. 現実世界がそうであるように, Web Audio API
              においても, 音源のスイッチをオン, オフする必要があります. そのためには, <code>OscillatorNode</code> クラスがもつ
              <b><code>start</code> メソッド</b>, <b><code>stop</code> メソッド</b> を呼び出します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);</code></pre>
            <p>
              <code>start</code> メソッドの引数に <code>0</code> を指定していますが, これはメソッドが呼ばれたら, 即時にサウンドを開始します.
              <code>stop</code> メソッドの引数には, <code>AudioContext</code> インスタンスの <b><code>currentTime</code></b> プロパティに
              <code>2.5</code> を加算した値を指定していますが, これは, <code>stop</code> メソッドを実行してから, 2.5
              秒後に停止することをスケジューリングしています (詳細は, のちほどのセクションで Web Audio API におけるスケジューリングとして解説しますが,
              <code>AudioContext</code> インスタンスの <code>currentTime</code> は,
              <b><code>AudioContext</code> インスタンスが生成されてからの経過時間を秒単位で計測した値</b>が格納されています. より厳密には,
              <b><code>AudioContextState</code> が <code>&apos;running&apos;</code> である状態での経過秒数</b>となります). <code>stop</code> メソッドの引数も
              <code>0</code> を指定すれば即時にサウンドを停止します. ちなみに, <code>start</code> メソッド, <code>stop</code> メソッドもデフォルト値は
              <code>0</code> なので, 引数を省略して呼び出した場合, 即時にサウンドを開始, 停止します.
            </p>
            <p>これで, とりあえず, ブラウザ (Web) で音を鳴らすことができました !</p>
          </section>
        </section>
        <section id="section-audio-param">
          <h3>AudioParam</h3>
          <p>
            サウンドの入力点と出力点を生成して, それらを接続するだけでは, 元の入力音をそのまま出力するだけなので高度なオーディオ処理はできません. むしろ, Web
            Audio API において重要なのは, この入力と出力の間に, 音響変化をさせる <code>AudioNode</code> を接続することです. 音響変化をさせるためには,
            音響変化のためのパラメータを取得・設定したり, 周期的に変化させたり (LFO) できる必要があります. Web Audio API において, その役割を担うのが
            <b><code>AudioParam</code></b> クラスです. <code>AudioNode</code> が現実世界の音響機器と例えをしましたが, それに従うと,
            <code>AudioParam</code> クラスはノブやスライダーなど音響機器のパラメータを設定するコントローラーのようなものです.
          </p>
          <p>
            <code>AudioParam</code> クラスは直接インスタンス化することはありません. <code>AudioNode</code> のプロパティとして,
            <code>AudioNode</code> のサブクラスのインスタンスを生成した時点でインスタンス化されているのでプロパティアクセスで参照することが可能です.
          </p>
          <p>
            <code>AudioParam</code> では, 単純なパラメータの取得や設定だけでなく, そのパラメータを周期的に変化させたり (LFO), スケジューリングによって変化させる
            (エンベロープジェネレーターなど) ことが可能です (ここはオーナーの経験からですが, Web Audio API で高度なオーディオ処理を実装するためには,
            <code>AudioParam</code> を理解して音響パラメータを制御できるようになるかが非常に重要になっていると思います).
          </p>
        </section>
        <section id="section-gain-node">
          <h3>GainNode (AudioNode インタンスの生成と接続, AudioParam の取得と設定)</h3>
          <p>
            <code>AudioNode</code> と <code>AudioParam</code> の具体的な利用例として, このセクションでは, <b><code>GainNode</code></b> を利用して,
            パラメータの取得・設定を実装します. <code>GainNode</code> はその命名のとおり, <b>ゲイン</b> (<b>増幅率</b>), つまり, 入力に対する出力の比率 (入力を
            <code>1</code> としたときに出力の値) を制御するための <code>AudioNode</code> で, Web Audio API におけるオーディオ処理で頻繁に使うことになります.
            このセクションでは, 単純に, <code>GainNode</code> の <b><code>gain</code></b> プロパティ (<code>AudioParam</code> インスタンス) を参照して,
            そのパラメータを取得・設定してみます (この実装例では, 音量の制御と考えても問題ありません).
          </p>
          <p>
            <code>GainNode</code> も <code>AudioNode</code> のサブクラスなので, コンストラクタ呼び出し, または, ファクトリメソッドで
            <code>GainNode</code> インスタンスを生成できます.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context);</code></pre>
          <p>
            コンストラクタ呼び出しで生成する場合, 初期パラメータ (<b><code>GainOptions</code></b> 型) を指定することも可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = new GainNode(context, { gain: 0.5 });</code></pre>
          <p>ファクトリメソッドで生成する場合.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const gain = context.createGain();</code></pre>
          <p><code>GainNode</code> インスタンスを生成したら, <code>OscillatorNode</code> と <code>AudioDestinationNode</code> の間に接続します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context, { gain: 0.5 });

// OscillatorNode (Input) -&gt; GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 2.5 sec
oscillator.stop(context.currentTime + 2.5);</code></pre>
          <p>これで実際にサウンドを発生させると, 音の大きさが小さく聴こえるはずです.</p>
          <p>
            このコードだと, 初期値を変更しているだけなので, 例えば, ユーザー操作によって変更するといったことができないので,
            インスタンス生成時以外でパラメータを設定したり, 取得したりする場合は, <code>GainNode</code> の <code>gain</code> プロパティを参照します. これは,
            先ほども記載したように, <code>AudioParam</code> インスタンスです. パラメータの取得や設定をするには, その
            <b><code>value</code></b> プロパティにアクセスします.
          </p>
          <p>簡単な UI として, 以下の HTML があるとします.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;label for=&quot;range-gain&quot;&gt;gain&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-gain&quot; value=&quot;1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-gain-value&quot;&gt;1&lt;/span&gt;</code></pre>
          <p>
            この <code>input[type=&quot;range&QUOT;]</code> のイベントリスナーで, <code>input[type=&quot;range&QUOT;]</code> で入力された値 (JavaScript の
            <code>number</code> 型) を <code>gain</code> (<code>AudioParam</code> インスタンス) の <code>value</code> プロパティに設定し, また,
            その値を取得して, HTML に動的に表示します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context);

// OscillatorNode (Input) -&gt; GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(context.destination);

// Start immediately
oscillator.start(0);

const spanElement = document.getElementById(&apos;print-gain-value&apos;);

document.getElementById(&apos;range-gain&apos;).addEventListener(&apos;input&apos;, (event) =&gt; {
  gain.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = gain.value;
});</code></pre>
          <p>
            <code>AudioParam</code> のパラメータの取得や設定は, このように, JavaScript のオブジェクトに対するプロパティの getter や setter と同じなので,
            特に違和感なく理解できるのではないでしょうか (設定に関しては,
            あとのセクションで解説するパラメータのオートメーションメソッドを利用する方法もあります).
          </p>
        </section>
        <p>
          <code>AudioNode</code> や <code>AudioParam</code> の接続によって構成されるオーディオデータのルーティングを, Web Audio API の仕様では,
          <b>オーディオグラフ</b> (<b>Audio Graph</b>) と用語定義しています.
        </p>
        <p>
          このセクションでは, Web Audio API の設計の基本となる (Web Audio API のアーキテクチャを決定づけている), <code>AudioContext</code>,
          <code>AudioNode</code>, <code>AudioParam</code> の関係性とそのパラメータの取得・設定の実装を解説しました. 以降のセクションでは, ユースケースに応じて,
          これら 3 つのクラスの詳細についても解説を追加していきます.
        </p>
      </section>
      <section id="section-about-sound">
        <h2>「音」とは ?</h2>
        <p>
          このセクションでは, そもそも「音」とはなにか ? からスタートして, 音の特性について簡単に解説します (いわゆる,
          <b>音響学</b>の基本のほんの一部分を解説します). 網羅的な解説はしないので, Web Audio API を理解するうえで, 最低限の解説をできるだけ簡単に解説します.
          また, そのために, 厳密さは犠牲にしている解説も多くあると思います. 音のスペシャリストの方からすると, ちょっと違う ...
          という部分はたくさんあるかと思いますがご了承ください (ただし, あきらかに間違った解説や誤解を招く可能性のある解説については遠慮なく Issue を作成したり,
          Pull Requests を送ったりしていただければと思います).
        </p>
        <p>
          Web Audio API について解説するセクションではないので, 音の特性 (音響学) に関して学んだことあれば,
          このセクションはスキップしていただくのがよいでしょう.
        </p>
        <section id="section-what-is-sound">
          <h3>音の実体</h3>
          <p>
            そもそも, 「音」って何なのでしょうか？ 結論としては, 音とは媒体の振動が聴覚に伝わったものと定義することができます.
            「媒体」というものが抽象的でよくわからないかもしれませんが, 具体的には, 空気や水です. 日常の多くの音は空気を媒体として,
            空気の振動が聴覚に伝わることで音として知覚するわけですが, 同じことは水中でも起きます. また,
            普段聴いている自分の声は骨を媒体にして伝わっている音です.
          </p>
        </section>
        <section id="section-modeling-sound">
          <h3>音のモデリング</h3>
          <p>
            音をコンピュータで表現するためには, 媒体の振動を数式で表現して, その数式によって導出される数値を 2 進数で表現できる必要があります.
            音の実体は媒体の振動というのを説明しましたが, この振動を表現するのに適した数学的な関数が, <b>sin 関数</b>です (cos 関数は sin
            関数の位相の違いでしかないので本質的に同じと考えてもよいでしょう. また, tan 関数は含まれません. その理由は,
            <span class="math-inline">$\frac{\pi}{2}$</span> や <span class="math-inline">$-\frac{\pi}{2}$</span> で
            <span class="math-inline">$\infty$</span> や
            <span class="math-inline">$-\infty$</span> になるので振動を表現するには都合が悪いからと考えてよいでしょう).
          </p>
          <p>
            Web Audio APIでも, <code>OscillatorNode</code> の <code>type</code> プロパティがとりうる値 (<code>OscillatorType</code>) の 1 つとして
            <code>&#039;sine&#039;</code> が定義されています.
          </p>
          <p>
            音を扱う学問や工学では, この sin 関数が, 音の波 (<b>音波</b>) をモデリングしていることから, <b>正弦波</b> (<b>sin 波</b>) と呼ぶことが多いです.
            とちらであっても, 実体は同じなのですが, このドキュメントではこれ以降, 慣習にしたがって, 正弦波 (sin 波) と記述することにします.
          </p>
          <section id="section-sine-wave">
            <h3>正弦波 (sin 波)</h3>
            <p>ここからは少し数学・物理的な話になってきます. 正弦波 (sin 関数) ってどんな形か覚えてらっしゃいますか？</p>
            <figure>
              <svg id="svg-figure-sin-function" width="720" height="405" data-a="1.0" data-f="1" />
              <figcaption>正弦波 (sin 関数)</figcaption>
            </figure>
            <p>具体的に解説するためにパラメータを設定します.</p>
            <figure>
              <svg
                id="svg-figure-sin-function-with-parameters-1-1Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="1"
                data-f="1"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>パラメータつき正弦波 (sin 関数)</figcaption>
            </figure>
            <section id="section-amplitude-and-frequency">
              <h4>振幅と周波数 (周期)</h4>
              <p>
                まず, 縦軸に着目してみます. 縦軸のパラメータは, <b>振幅</b>と呼ばれ, 単位はありません. ちなみに, 振幅 <code>1</code> の正弦波と表現した場合,
                上記のように振幅の最大値が <code>1</code>, 最小値が <code>-1</code>の 正弦波のことを意味しています. 次に, 横軸に着目してみます.
                横軸のパラメータは, <b>時間</b>を表しています. 縦軸との関係で表現すると, ある時刻における正弦波の振幅値を表した図 (グラフ) と言えます. ここで,
                パラメータつきの正弦波を見てみます. すると, 山 1 つと谷 1 つを最小の構成として, それが繰り返されている, すなわち,
                <b>周期性</b>をもつことがわかります. 数学的には, すべての時間
                <span class="math-inline">$t \left(0 \leqq {t} &lt; \infty \right)$</span> に対して,
                <span class="math-inline">$f\left(t + L\right) = f\left(t\right)$</span> となる定数が存在するとき,
                <span class="math-inline">$f\left(t\right)$</span> は周期 <span class="math-inline">$L$</span> の<b>周期関数</b>と定義されます. そして, sin
                関数は, 周期 <span class="math-inline">$L$</span> としたとき
                <span class="math-inline">$\sin\left(t + L\right) = \sin\left(t\right)$</span> が成立するので, <b>正弦波 (sin 関数) は周期関数</b>です.
              </p>
              <p>
                この波の最小の構成が発生するために要する時間を<b>周期</b>と呼びます. 例として, 上記の正弦波で考えると, 最小の構成の発生までに
                <code>1 sec</code> の時間を要しているので, 周期は <code>1 sec</code> となります. この真逆の概念を表す用語が<b>周波数</b>です. すなわち,
                <code>1 sec</code> の間に, 波の最小の構成が何回発生するか ? ということを表し, 単位は <b>Hz</b> (ヘルツ) です. Hz (ヘルツ) という名前ですが,
                日本語に翻訳すれば, 何回の「回」に相当するでしょう. 上記の正弦波で考えると. この正弦波は, <code>1 sec</code> の間に最小の構成が
                <code>1</code> 回発生しているので, 周波数は, <code>1 Hz</code> ということになります.
              </p>
              <p>
                周期と周波数は互いに真逆の概念ですが, これは数学的には, 互いに<b>逆数</b>の関係にあります. すなわち,
                <b>周期の逆数は周波数を表し, 周波数の逆数は周期を表します</b>. 互いに関係のある値なので, 周期の話をすれば周波数の話も同時にしていることであり,
                周波数の話をすれば周期の話も同時にしていることになります. ただ, 周波数という用語のほうがよく使われる傾向にあると思うので, このドキュメントでは,
                周波数の用語を優先的に利用することにします.
              </p>
              <p>少し慣れるために, パラメータ (振幅や周波数) を変えた正弦波 (sin 波) を見てましょう.</p>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-2Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="0.5"
                  data-f="1"
                  data-t="0.0,0.5,1.0"
                />
                <figcaption>振幅 <code>0.5</code>, 周波数 <code>1 Hz</code> (周期 <code>1 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-0.5-1Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="2"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>2 Hz</code> (周期 <code>0.5 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
              <figure>
                <svg
                  id="svg-figure-sin-function-with-parameters-1-0.5Hz"
                  width="720"
                  height="405"
                  data-parameters="true"
                  data-a="1"
                  data-f="1"
                  data-t="0.0,1.0,2.0"
                />
                <figcaption>振幅 <code>1</code>, 周波数 <code>0.5 Hz</code> (周期 <code>2 sec</code>) の正弦波 (<code>&apos;sine&apos;</code>)</figcaption>
              </figure>
            </section>
            <p>いかがでしたか ? 振幅と周波数は Web Audio API の解説においても頻出する用語なので, ある程度理解しておくと, Web Audio API の理解も進むでしょう.</p>
          </section>
          <section id="section-synthesizer-waveforms">
            <h3>基本波形</h3>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値は, 正弦波を生成する文字列
              <code>&apos;sine&apos;</code> 以外にも, 矩形波を生成する <code>&apos;square&apos;</code> やノコギリ波を生成する <code>&apos;sawtooth&apos;</code>,
              三角波を生成する <code>&apos;triangle&apos;</code> があります. 正弦波の形はわかりましたが, それ以外はどのような形をしているのか見てみましょう.
            </p>
            <figure>
              <svg
                id="svg-figure-square-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の矩形波 (<code>&apos;square&apos;</code>)</figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-sawtooth-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>
                振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) のノコギリ波 (<code>&apos;sawtooth&apos;</code>)
              </figcaption>
            </figure>
            <figure>
              <svg
                id="svg-figure-triangle-function-with-parameters-0.5-4Hz"
                width="720"
                height="405"
                data-parameters="true"
                data-a="0.5"
                data-f="2"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>振幅 <code>0.5</code>, 周波数 <code>4 Hz</code> (周期 <code>0.25 sec</code>) の三角波 (<code>&apos;triangle&apos;</code>)</figcaption>
            </figure>
            <p>
              矩形波・ノコギリ波・三角波のいずれも正弦波と同じように, 周期性をもつ波 (関数) であるということです. 周期性をもつので,
              周波数の概念を適用することができます. そして, 最も重要な点ですが, 周期性をもつ波は周波数の異なる正弦波を合成してできるということです.
              矩形波・ノコギリ波・三角波はいずれも周期性をもちます. 周期性をもつので,
              矩形波・ノコギリ波・三角波はいずれも周波数の異なる正弦波を合成して生成することができます. シンセサイザーでも,
              正弦波・矩形波・ノコギリ波・三角波は基本波形として, サウンド生成のベースとなる波形です. そして, Web Audio API においても, 基本波形はサウンド生成
              (<code>OscillatorNode</code>) のベースになる波形です.
            </p>
          </section>
        </section>
        <section id="section-the-three-components-of-sound">
          <h3>音の 3 要素</h3>
          <p>ここまで, 数学・物理的な話が続いたので, 少し気分を変えて, 感覚視点 (知覚) から音を考えてみましょう.</p>
          <p>
            日常でも, 「音が大きい・小さい」, 音楽を聴いていて「音が高い・低い」,
            楽器を演奏していて「この楽器の音色が好き」などと表現することがあるかと思います. これらは, 音を感覚視点, すなわち, <b>音を知覚するときの視点</b>で,
            どんな音か ? を表現しています. これらの表現にある, <b>音の大きさ</b>・<b>音の高さ</b>・<b>音色</b>を<b>音の 3 要素</b>と呼びます.
          </p>
          <p>音の 3 要素と, 先に解説した振幅・周波数・波形と大きな関わりがあります.</p>
          <dl>
            <dt>音の大きさ (Loudness)</dt>
            <dd>振幅が大きく影響する</dd>
            <dt>音の高さ (Pitch)</dt>
            <dd>周波数が大きく影響する</dd>
            <dt>音色 (Timbre)</dt>
            <dd>波形 (エンベロープ) が大きく影響する</dd>
          </dl>
          <p>
            <b>大きく影響する</b>という表現に注意してください. 例えば, 音の大きさは振幅のみで決定されるわけではないということです. 知覚は主観的な指標であり,
            振幅・周波数・波形は物理量だからです. 物理現象である音と知覚を関連づける指標として, <b>音響特徴量</b> (等ラウドネス曲線や基本周波数, セントロイド,
            ケプストラムなど) が知られています.
          </p>
          <p>簡単な解説を記載しますが, これらの音響特徴量に関する詳細な解説は, 最適なドキュメントや書籍が豊富にあるので, そちらを参考にしてください.</p>
          <dl>
            <dt>等ラウドネス曲線</dt>
            <dd>
              周波数を横軸に, 音圧レベルを縦軸にして, 同じ音の大きさに聴こえる (知覚できる) 点をプロットして曲線で結んだグラフです, 等ラウドネス曲線から,
              音圧レベル (振幅) が同じでも周波数によって音の大きさが異なることが確認できます
            </dd>
            <dt>基本周波数</dt>
            <dd>音信号に含まれる最も低い周波数成分で, 特に, 音楽においては, 音高を決める重要な音響特徴量となります</dd>
            <dt>セントロイド</dt>
            <dd>
              他の工学分野でも利用される用語で「重心」という意味ですが, オーディオ信号処理においては, スペクトルの重心 (偏り) を表す音響特徴量で,
              音楽制作ではミキシング・マスタリングのプロセスで, セントロイドを調整をすることがあります
            </dd>
            <dt>ケプストラム</dt>
            <dd>
              音声分析で利用されることが多く, パワースペクトル (振幅を 2 乗したスペクトル) 対数をとって, 逆フーリエ変換した, 時間領域の信号 (音響特徴量) です.
              ケプストラムによって, 音声を声帯振動と声道フィルタの成分に分離して分析することが可能になります
            </dd>
          </dl>
        </section>
        <section id="section-web-audio-api-relation-to-sound">
          <h3>Web Audio API と音の関係</h3>
          <p></p>
          <section id="section-gain-relation-to-sound">
            <h4>GainNode の gain プロパティと音の大きさ</h4>
            <p>
              <code>GainNode</code>の <code>gain</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の大きさを変えることができます.
              物理的な視点で見ると, 振幅を操作することによって, 音の大きさを変えています.
            </p>
            <img src="images/gain-gain.png" alt="GainNode gain" width="1232" height="770" loading="lazy" />
          </section>
          <section id="section-frequency-relation-to-sound">
            <h4>OscillatorNode の frequency プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>frequency</code> プロパティ (<code>AudioParam</code>) を利用することで, 音の高さを変えることができます.
              物理的な視点で見ると, 周波数を操作することによって, 音の高さを変更しています.
            </p>
            <img src="images/oscillator-frequency.png" alt="OscillatorNode frequency" width="1232" height="770" loading="lazy" />
            <p>
              仕様では, <code>frequency</code> プロパティのとりうる値の範囲は, 負のナイキスト周波数からナイキスト周波数までですが (ナイキスト周波数は,
              <a href="#section-analog-to-digital-conversion-sampling">サンプリング</a>のセクションで解説しています. ナイキスト周波数について理解がなければ,
              おおよそ, <code>-20 kHz</code> ~ <code>20 kHz</code> と大雑把に把握していただいて問題ないです),
              音楽アプリケーションなどで出力する音としてはそこまで設定できてもあまり意味はないでしょう. その理由は,
              <b>人間が聴きとることが可能な音の周波数の範囲は <code>20 Hz</code> ~ <code>20000 Hz</code> (<code>20 kHz</code>) 程度だからです</b>.
            </p>
            <p>
              さらに, <b>音程</b> (音の高さの差) として知覚可能な周波数の上限, 言い換えると, 音楽として有効な音の周波数はもっと低くなります (ピアノ 88
              鍵の音域を参照してください).
            </p>
            <figure>
              <svg id="svg-figure-frequency-and-piano-frequency" width="1196" height="282" data-highlights="0,87" />
              <figcaption>ピアノ 88 鍵と周波数</figcaption>
            </figure>
          </section>
          <section id="section-detune-relation-to-sound">
            <h4>OscillatorNode の detune プロパティと音の高さ</h4>
            <p>
              <code>OscillatorNode</code> の <code>detune</code> プロパティ (<code>AudioParam</code>) を利用することでも, 音の高さを変えることができます.
              物理的な視点も <code>frequency</code> プロパティと同じです. ただし, <code>detune</code> プロパティは, 音楽的な視点で音の高さを変更します.
              <code>detune</code> プロパティの用途は, (音楽で言う) 半音よりも小さい範囲で音の高さを調整したり,
              オクターブ違いの音を生成・合成したりするために利用します. この機能によって, きめ細かいサウンド生成が可能になったり,
              サウンドを合成する場合において厚みをもたせることが可能になったりします. シンセサイザーのファインチューン機能や, エフェクターの 1
              種であるオクターバーを実現するためにあると言えるでしょう.
            </p>
            <img src="images/oscillator-detune.png" alt="OscillatorNode detune" width="1196" height="770" loading="lazy" />
            <p>
              <code>frequency</code> プロパティの単位は Hz (ヘルツ) で, 波が 1 sec の間に何回発生するのかを意味していました. 一方で,
              <code>detune</code> プロパティの単位は <b>cent</b> (セント) です. これは, 音楽の視点から音の高さをとらえた単位で,
              <b>1 オクターブの音程を 1200 で等分した値</b>です.
            </p>
            <p>
              1 つ高いラとか, 1 つ低いラのことを, 1 オクターブ高いラ, 1 オクターブ低いラと表現することがあります.
              音楽的な視点でのオクターブはまさにそういう意味です.
            </p>
            <p>
              オクターブを物理的な視点でみると, <b>周波数比が 1 : 2 の関係にある音程</b>を意味しています. 具体的に説明すると, いわゆる普通のラ (A) (ギターの第 5
              弦の開放弦) の周波数は <code>440 Hz</code> です (キャリブレーションチューニングなどしている場合は別ですが ...). この音を基準に考えると, 1
              オクターブ高いラの周波数は <code>880 Hz</code> です. 周波数比が, 440 : 880 = 1 : 2 になります.
            </p>
            <p>
              話を cent に戻すと, この 1 : 2 の音程を 1200 で割った値が <code>1 cent</code> というわけです. なぜ, 1200 ?
              と疑問に思う方もいらっしゃると思いますが, ピアノをされる方は直感で理解できると思います. ピアノをされない方のために, 1
              オクターブの音程間にピアノの鍵盤がいくつあるか数えてみましょう. 1 オクターブ間であればいいので, 好きな音から始めてください.
            </p>
            <figure>
              <svg id="svg-figure-12-equal-temperament" width="1196" height="162" data-highlights="39,40,41,42,43,44,45,46,47,48,49,50" />
              <figcaption>1 オクターブの鍵盤数</figcaption>
            </figure>
            <p>
              数えてみると, <b>12</b> 個の鍵盤があります. 1 オクターブ間の音程を 1200 で割った (1200 分割した) 値が <code>1 cent</code> でしたので, 1
              オクターブ間の音程を 12 分割すると, <code>100 cent</code> ということになります. つまり, <code>100 cent</code> 値が高くなると,
              右隣の鍵盤の音の高さに変わるということです.
            </p>
            <p>
              例として, <code>440 Hz</code> のラ (A) の音を <code>100 cent</code> 高くすると, 右隣の鍵盤の ラ# (A#) に, さらに <code>100 cent</code> 高くすると,
              シ (B) になります. このように, <code>-100 cent</code> ~ <code>100 cent</code> の間の値を設定することによって,
              半音以下の音の高さの調整が可能になるわけです. また, <code>1200 cent</code>, あるいは, <code>-1200 cent</code> と <code>1200 cent</code>
              ごとに値を設定することにより, オクターブ単位で調整することも可能です.
            </p>
            <p>
              音楽では, 1 オクターブの音程を 12 等分した周波数比の関係を 12 平均音律と呼びます. 12 平均音律においては, 隣り合う音, つまり, 半音の周波数比は,
              およそ, 1 : 1.059463 (正確には, 1 : <span class="math-inline">$2^{\left(1 / 12\right)}$</span>) で, これが <code>100 cent</code> となるわけです.
            </p>
          </section>
          <section id="section-type-relation-to-sound">
            <h4>OscillatorNode の type プロパティと音色</h4>
            <p>
              <code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorType</code>) の値を利用することで, 正弦波だけでなく,
              矩形波やノコギリ波, 三角波を生成することができます. それによって, 音色を変化させることが可能です. ちなみに,
              波形の概形は<b>エンベロープ</b>と呼ばれます. <code>OscillatorNode</code> のみで制御可能な範囲では, この
              <code>type</code> プロパティに応じたエンベロープが音色に大きく影響しています.
            </p>
          </section>
          <p>
            このセクションのまとめとして, 基本波形, 振幅, 周波数を変化させたときの波形を視覚化するデモとなります. 波形の変化とともに, 知覚する音 (音の 3 要素)
            の変化を体感してみてください.
          </p>
          <div class="app-container">
            <svg id="svg-oscillator" class="svg-oscillator" width="720" height="240"></svg>
            <div>
              <button type="button" id="button-oscillator" class="button-oscillator">start</button>
              <form id="form-oscillator-type" class="form-oscillator-type">
                <label><span>sine</span><input type="radio" name="radio-oscillator-type" value="sine" checked /></label>
                <label><span>square</span><input type="radio" name="radio-oscillator-type" value="square" /></label>
                <label><span>sawtooth</span><input type="radio" name="radio-oscillator-type" value="sawtooth" /></label>
                <label><span>triangle</span><input type="radio" name="radio-oscillator-type" value="triangle" /></label>
              </form>
              <div class="ranges-oscillator">
                <label><span>gain</span><input type="range" id="range-gain" value="1" min="0" max="1" step="0.05" /></label>
                <label><span>frequency</span><input type="range" id="range-frequency" value="440" min="27.5" max="8000" step="0.5" /></label>
                <label><span>detune</span><input type="range" id="range-detune" value="0" min="-600" max="600" step="1" /></label>
              </div>
            </div>
          </div>
        </section>
      </section>
      <section id="section-oscillator-node">
        <h2>OscillatorNode</h2>
        <p>
          Web Audio API のアーキテクチャを解説するうえで, <code>OscillatorNode</code> は少し説明しましたが, このセクションでは, Web Audio API
          におけるサウンド生成・合成のベースとなる, <code>OscillatorNode</code> についてその詳細を解説します.
        </p>
        <p>
          シンセサイザーの基本波形の生成・合成, モジュレーション系エフェクターで必須となる LFO (Low-Frequency Oscillator) など, Web Audio API
          において用途の広い, コアとなる <code>AudioNode</code> です. LFO に関しては, エフェクターのセクションで解説するので,
          このセクションでは基本波形の生成・合成に関して解説します.
        </p>
        <section id="section-oscillator-node-type">
          <h3>type プロパティ (OscillatorOptions)</h3>
          <p>
            ただし, <code>&apos;custom&apos;</code> のみは特殊で, 直接値を設定するとエラーが発生します. これは, <code>OscillatorNode</code> の
            <code>setPeriodicWave</code> メソッドによって, 自動的に <code>&apos;custom&apos;</code> に設定されます. また, その引数として,
            <code>AudioContext</code> の <code>createPeriodicWave</code> メソッドで波形テーブルを生成する必要があります. 波形テーブルの生成は,
            スペクトルや倍音などオーディオ信号処理の知識が必要になるので, 別のセクションで解説します.
          </p>
        </section>
        <section id="section-oscillator-node-frequency-and-detune">
          <h3>frequency プロパティ (AudioParam) / detune プロパティ (AudioParam)</h3>
          <p>
            周波数を制御して音の高さを変更します. <code>frequency プロパティ</code> と <code>detune</code> プロパティを合わせて算出される周波数 (<span
              class="math-inline"
              >$f_{\mathrm{computed}}$</span>) は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$f_{\mathrm{computed}} = \mathrm{frequency} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
          <p>
            この数式は, <code>frequency</code> は物理的な視点 (Hz) で周波数を制御, <code>detune</code> は音楽的な視点 (cent)
            で周波数を制御することを意味しています.
          </p>
        </section>
        <section id="section-oscillator-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>OscillatorNode</code> のプロパティを設定して音の高さや音色を制御することはそれほど難しくないかと思います. また, 発音し続けるか, 1 度だけ発音
            (<code>start</code> メソッド)・停止 (<code>stop</code> メソッド) する場合も直感的に実装可能です. おそらく, 多くの場合, ハマってしまうのが,
            <code>OscillatorNode</code> の発音と停止を繰り返す場合です.
          </p>
          <p>
            <code>OscillatorNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>OscillatorNode</code> インスタンスは再度, 発音 (停止)
            することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合, <code>OscillatorNode</code> インスタンスを再生成して,
            再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code>stop</code> メソッド) を実行する必要があります.
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 発音・停止することを期待していますが, 2 回目のクリック以降は, 発音されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();
const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  // Start immediately
  // But, cannot start since the second times ...
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  // Stop immediately
  oscillator.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>
            期待する動作, つまり, 発音・停止を繰り返すするには, 一度 <code>start</code>・<code>stop</code>した
            <code>OscillatorNode</code> インスタンスは破棄して, 再度 <code>OscillatorNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // Start immediately
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (oscillator === null) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);

  // GC (Garbage Collection)
  oscillator = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
          <p>このような仕様なので, <code>start</code> メソッドを続けて呼んだり, <code>stop</code> メソッドを続けて呼んだりしても, エラーが発生します.</p>
          <p>
            <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は, さまざまなプラットフォームのオーディオ API のなかでも Web Audio
            API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web ではないプラットフォームのオーディオ API はここまで抽象化されている API
            すら少ないと思います).
          </p>
        </section>
        <section id="section-oscillator-node-synthesize">
          <h3>基本波形の合成</h3>
          <p>
            基本波形の合成, すなわち, Web Audio API における <code>OscillatorNode</code> の合成は直感的で, 必要なだけ
            <code>OscillatorNode</code> インスタンスを生成して, (最後の) 接続先として <code>AudioDestinationNode</code> を指定するだけです.
          </p>
          <p>
            ただし, そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています
            (逆に, この音割れ (クリッピング) をエフェクトとして使うのが歪み系エフェクターです). もしくは,
            <code>DynamicsCompressorNode</code> を接続して振幅を制御して, 意図しない音割れを防ぐこともできます (ただし, 厳密には,
            コンプレッサーは振幅の小さい音も相対的に大きくするので, 物理的にはまったく同じではありません).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let oscillatorC = null;
let oscillatorE = null;
let oscillatorG = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillatorC !== null) || (oscillatorE !== null) || (oscillatorG !== null)) {
    return;
  }

  oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
  oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
  oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

  const gain = new GainNode(context, { gain: 0.25 });

  // OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  oscillatorC.connect(gain);
  oscillatorE.connect(gain);
  oscillatorG.connect(gain);
  gain.connect(context.destination);

  // Start immediately
  oscillatorC.start(0);
  oscillatorE.start(0);
  oscillatorG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillatorC === null) || (oscillatorE === null) || (oscillatorG === null)) {
    return;
  }

  // Stop immediately
  oscillatorC.stop(0);
  oscillatorE.stop(0);
  oscillatorG.stop(0);

  // GC (Garbage Collection)
  oscillatorC = null;
  oscillatorE = null;
  oscillatorG = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
        </section>
      </section>
      <section id="section-audio-buffer-source-node">
        <h2>AudioBufferSourceNode</h2>
        <p>
          <b><code>AudioBufferSourceNode</code></b> は, <b>ワンショットオーディオの再生</b>を目的に利用します. ワンショットオーディオとは,
          ピアノやギターなど実際の楽器の音源を収録した WAVE ファイルや MP3 ファイルのことです. Web Audio API の仕様では, ユースケースとして,
          楽曲データに関しては, <b><code>MediaElementAudioSourceNode</code></b> を利用することを想定しているので, この点は注意が必要です. ただし,
          <code>AudioBufferSourceNode</code> を楽曲データの再生に使うこともできます. 現実解としてユースケースに反した利用をすることも多いです (これは,
          <code>AudioBufferSourceNode</code> がオーディオデータの実体である <code>AudioBuffer</code> インスタンスをもつので,
          オーディオ信号処理が適用しやすいことが理由として考えられます).
        </p>
        <p>このセクションでは, 仕様上のユースケースであるワンショットオーディオの再生を目的に, <code>AudioBufferSourceNode</code> を解説します.</p>
        <p>
          ところで, ワンショットオーディオの再生であれば, 同じことは <code>HTMLAudioElement</code> (<code>audio</code> タグ) でも可能な場合もあります. 事実, Web
          Audio API が仕様策定される以前は, そのようなユースケースも想定して, <code>Audio</code> コンストラクタが定義されています. しかしながら,
          <code>HTMLAudioElement</code> (<code>Audio</code> コンストラクタ) によるワンショットオーディオの再生は以下のような問題があります.
        </p>
        <ul>
          <li>JavaScript のタイマー (<code>setInterval</code> や <code>setTimeout</code>) では, 正確なスケジュールングが難しい</li>
          <li><code>HTMLAudioElement</code> のイベントハンドラでも精度が粗く, 正確なスケジュールングが難しい</li>
          <li>同時発音数の制限</li>
          <li>ワンショットオーディオに対して, さらにオーディオ処理を付加したいユースケース</li>
        </ul>
        <p>
          これらの問題を, ある程度容易に解決してくれるのが <code>AudioBufferSourceNode</code> です (もっとも, <code>AudioBufferSourceNode</code> を利用しても,
          コンピュータのリソースは有限なので, 計算量が多い場合や他のプロセスがリソースを多く消費している場合などは,
          少なからずスケジューリングも正確でなくなります).
        </p>
        <img src="images/audio-buffer-source-node.png" alt="AudioBufferSourceNode" width="1232" height="770" loading="lazy" />
        <section id="section-audio-buffer-source-node-buffer">
          <h3>buffer プロパティ</h3>
          <p>
            <code>AudioBufferSourceNode</code> において, 最も重要と言えるのが, <b><code>buffer</code></b> プロパティであり, これは,
            <b><code>AudioBuffer</code></b> インスタンスを参照します. <code>AudioBuffer</code> とは, オーディオデータの実体 (を抽象化するクラス) です.
          </p>
          <img src="images/audio-buffer.png" alt="AudioBuffer" width="1232" height="770" loading="lazy" />
          <section id="section-audio-buffer">
            <h4>AudioBuffer</h4>
            <p>
              <code>AudioBuffer</code> クラスは, オーディオデータの実体ですが, 直接的にアクセスすることはできません. そのためのメソッドや,
              デジタル化されたオーディオデータに必要なパラメータ (サンプリングレートやチャンネル数, オーディオデータ全体のサイズなど) を定義しています.
            </p>
            <section id="section-audio-buffer-sample-rate">
              <h5>sampleRate プロパティ</h5>
              <p>
                オーディオデータのサンプリング周波数です. これは, <code>createBuffer</code> メソッドで利用して
                <code>AudioBuffer</code> インスタンスを生成する場合, 実質的に意味のあるプロパティとなります. <code>decodeAudioData</code> メソッドで取得した
                <code>AudioBuffer</code> インスタンスは, <code>AudioContext</code> インスタンスの
                <code>sampleRate</code> プロパティの値にリサンプリングされるからです (つまり, その場合, <code>AudioContext</code> インスタンスの
                <code>sampleRate</code> プロパティを参照しても同じ値なので).
              </p>
            </section>
            <section id="section-audio-buffer-length">
              <h5>length プロパティ</h5>
              <p>
                1 チャネルにおける, オーディオデータのサイズです. つまり, <code>sampleRate</code> プロパティの逆数である<code>サンプリング周期</code>と
                <code>length</code> プロパティを乗算した値が, オーディオデータの再生時間となります (次に解説する,
                <code>duration</code> プロパティの値と同じになります).
              </p>
              <div class="math-block">$\mathrm{duration} = \frac{\mathrm{length}}{\mathrm{sampleRate}}$</div>
            </section>
            <section id="section-audio-buffer-duration">
              <h5>duration プロパティ</h5>
              <p>
                オーディオデータの再生時間 (単位は <code>sec</code>) です. 先ほど解説したように, <code>sampleRate</code> プロパティと
                <code>length</code> プロパティと関連している値となります.
              </p>
            </section>
            <section id="section-audio-buffer-number-of-channels">
              <h5>numberOfChannels プロパティ</h5>
              <p>
                オーディオデータのチャンネル数です. 例えば, モノラルであれば <code>1</code>, ステレオであれば <code>2</code>, 5.1 チャンネルであれば
                <code>6</code> になります. 次に解説する, <code>getChannelData</code> メソッドの引数の上限を決めている値になっています.
              </p>
            </section>
            <section id="section-audio-buffer-get-channel-data">
              <h5>getChannelData メソッド</h5>
              <p>
                <code>getChannelData</code> メソッドで引数で指定したチャンネルのオーディオデータを <code>Float32Array</code> として取得することが可能です.
                引数となるチャンネルの指定は <code>0</code> から <code>numberOfChannels - 1</code> までです. 例えば, ステレオ (<code>numberOfChannels</code> が
                <code>2</code>)であれば, <code>getChannelData(0)</code> で左チャンネルのオーディオをデータを <code>Float32Array</code> で取得し,
                <code>getChannelData(1)</code> で右チャンネルのオーディオデータを<code>Float32Array</code> で取得することができます.
              </p>
            </section>
            <section id="section-audio-buffer-copy">
              <h5>copyFromChannel メソッド / copyToChannel メソッド</h5>
              <p>
                他に, <code>AudioBuffer</code> をコピーするためのメソッドがあります. ワンショットオーディオの再生においてはおそらく使うことはないので,
                必要であれば, 仕様や MDN などを参考にしてください.
              </p>
            </section>
          </section>
          <section id="section-create-audio-buffer">
            <h4>AudioBuffer の生成</h4>
            <p>
              <code>AudioBuffer</code> クラスに関して簡単に解説しましたが, 肝心なのは
              <code>AudioBuffer</code> インスタンスをどうやって生成するのかということだと思います. Web Audio API では,
              <b><code>decodeAudioData</code></b> メソッドを利用するか, <b><code>createBuffer</code></b> メソッドを利用することによって,
              <code>AudioBuffer</code> インスタンスを生成可能です.
            </p>
            <p>
              もっとも, ワンショットオーディオ再生目的であれば, <code>createBuffer</code> メソッドを利用することはおそらくなく,
              <code>ArrayBuffer</code> インスタンスから <code>AudioBuffer</code> インスタンスを生成する
              <code>decodeAudioData</code> メソッドを利用することになると思います. したがって, まずは,
              <code>ArrayBuffer</code> インスタンスの取得に関して解説します (これは Web Audio API の解説というよりは, JavaScript で
              <code>ArrayBuffer</code> インスタンスを取得する方法なので, すでにご存知の場合はスキップして問題ないです).
            </p>
            <section id="section-array-buffer-and-decode-audio-data">
              <h5>ArrayBuffer の取得と decodeAudioData メソッド</h5>
              <p>
                クライアントサイド JavaScript で <code>ArrayBuffer</code> を取得するには, Web にあるリソースであれば, <code>Fetch API</code> (もしくは,
                <code>XMLHttpRequest</code>), ユーザーのファイルシステムから選択するのであれば <code>File API</code> と
                <code>FileReader API</code> を使うことになります.
              </p>
              <p>
                ワンショットオーディオ再生の場合, アプリケーション側であらかじめオーディオデータを Web にアップロードしているケースがほとんどなので,
                このセクションでは, <code>Fetch API</code> で <code>ArrayBuffer</code> を取得する実装を解説します.
              </p>
              <p>
                <code>Fetch API</code> は, <code>fetch</code> 関数, <code>Headers</code> オブジェクト, <code>Request</code> オブジェクト,
                <code>Response</code> オブジェクトの総称ですが, ほとんどのケースで明示的に利用するのは, <code>fetch</code> 関数の呼び出しです.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    // TODO: Create instance of `ArrayBuffer` by calling `decodeAudioData`
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                <code>fetch</code> 関数のデフォルトの HTTP メソッドは GET なので, ワンショットオーディオの取得であれば, そのリソースの URL
                を指定すればよいでしょう. あとは, 取得した <code>Response</code> オブジェクトの <code>arrayBuffer</code> メソッドを呼び出して,
                <code>ArrayBuffer</code> インスタンスを取得するだけです. いずれの関数・メソッドも, <code>Promise</code> を返します. 可読性重視などであれば,
                <code>async</code>/<code>await</code> で実装してもよいでしょう.
              </p>
              <p>
                <code>ArrayBuffer</code> インスタンスが取得できたら, <code>AudioContext</code> インスタンスの <b><code>decodeAudioData</code></b> メソッドの第 1
                引数に, <code>ArrayBuffer</code> インスタンスを指定して, 第 2 引数に, 成功時のコールバック関数を指定します. このコールバック関数の引数に,
                <code>AudioBuffer</code> インスタンスが渡されます. 失敗した場合, 第 3 引数のコールバック関数が実行されます. このコールバック関数の引数には,
                <code>DOMException</code> インスタンスが渡されます.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      // Create instance of `AudioBufferSourceNode`
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                初期の頃は上記のような仕様でしたが, 最新の仕様では, 成功時は <code>Promise&lt;AudioBuffer&gt;</code> を返すので, 戻り値から
                <code>AudioBuffer</code> インスタンスを取得することも可能です.
              </p>
              <p>
                <code>decodeAudioData</code> メソッドの実行で 1 つ注意しなければならないのは, <code>decodeAudioData</code> メソッドも
                <b><a href="#section-autoplay-policy">Autoplay Policy</a></b> の影響を受けるということです. したがって,
                ユーザーインタラクティブなイベント発生後に実行する必要があります.
              </p>
            </section>
            <section id="section-create-buffer">
              <h5>createBuffer メソッド</h5>
              <p>
                <code>AudioBuffer</code> インスタンスを生成するには, <code>AudioContext</code> インスタンスの
                <b><code>createBuffer</code></b> メソッドを利用することでも可能です. 引数は, 第 1 引数にチャンネル数, 第 2 引数に 1
                チャンネルのオーディオデータのサイズ, 第 3 引数にサンプリング周波数を指定します. しかしながら, インスタンスは生成できるものの,
                オーディオデータをもっているわけではないので, ワンショットオーディオの再生において利用することはないでしょう. ユースケースとしては,
                オーディオデータから生成した <code>AudioBuffer</code> インスタンスからコピー (<code>copyFromChannel</code> メソッドや
                <code>copyToChannel</code> メソッドが必要なケース) が考えられます.
              </p>
            </section>
            <p>
              これで, ワンショットオーディオを再生する最低限の処理ができているので, あとは <code>AudioBufferSourceNode</code> のインスタンスを生成します
              (ファクトリメソッドで生成する場合, <code>createBufferSource</code> メソッドを利用します).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

      // If use `createBufferSource`
      // const source = context.createBufferSource();
      //
      // source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          </section>
        </section>
        <section id="section-audio-buffer-source-node-playback-rate-and-detune">
          <h3>playbackRate プロパティ / detune プロパティ</h3>
          <p>
            音楽用途でワンショットオーディオを使う場合, 対応するピッチの数だけ, ワンショットオーディオデータを作成するのは大変です. また, HTTP
            リクエストの送受信や <code>decodeAudioData</code> メソッドの実行も多くなってしまうのでパフォーマンス的にもよくありません. それを解決するのが,
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティです. これらは, 音の物理的な性質, つまり,
            <b>再生速度を変化させるとピッチも変化する</b>という性質を利用して, ピッチ (と再生時間) を変更します. 例えば, <code>playbackRate</code> を
            <code>2</code> に設定すれば, ピッチも 2 倍, つまり, 1 オクターブ高いピッチのオーディオデータの再生を同一の
            <code>AudioBuffer</code> インスタンスから可能です. <code>detune</code> は, cent 単位でピッチを変更します. ピッチを変更すると,
            再生時間も変わりますが, ワンショットオーディオは再生時間が短時間なので, この点が問題になることはほとんどないでしょう. いずれも,
            <code>AudioParam</code> インスタンスなので, 値を取得したり, 設定する場合は, <code>value</code> プロパティにアクセスします.
          </p>
          <p>
            <code>playbackRate</code> プロパティと <code>detune</code> プロパティを考慮した, 実際の再生速度
            <span class="math-inline">$p_{\mathrm{computed}}$</span> は, 仕様では以下のように決定されます.
          </p>
          <div class="math-block">$p_{\mathrm{computed}} = \mathrm{playbackRate} \cdot \mathrm{pow}\left(2, \left(\mathrm{detune} / 1200 \right)\right)$</div>
        </section>
        <section id="section-audio-buffer-source-node-loop">
          <h3>loop プロパティ / loopStart プロパティ / loopEnd プロパティ</h3>
          <p>
            ワンショットオーディオをループ再生させたい場合, <code>loop</code> プロパティを <code>true</code> に設定します. また, <code>loop</code> プロパティを
            <code>true</code> に設定することで, <code>loopStart</code> プロパティと <code>loopEnd</code> プロパティが有効になります. これらのプロパティは,
            ループ再生するオーディオデータの開始位置, 終了位置を秒単位で指定します.
          </p>
        </section>
        <section id="section-audio-buffer-source-node-start-and-stop">
          <h3>start メソッド / stop メソッド</h3>
          <p>
            <code>AudioBufferSourceNode</code> インスタンスは, 言わば使い捨てなので, 一度発音・停止した <code>AudioBufferSourceNode</code> インスタンスは再度,
            発音 (停止) することはできません. 例えば, ユーザーインタラクティブな操作で発音・停止を繰り返すような場合,
            <code>AudioBufferSourceNode</code> インスタンスを再生成して, 再度 <code>AudioDestinationNode</code> に接続して, <code>start</code> メソッド (<code
              >stop</code>
            メソッド) を実行する必要があります. この仕様は, <code>OscillatorNode</code> とまったく同じです (ただし,
            <code>AudioBuffer</code> インスタンスは使い回すことが可能です).
          </p>
          <p>例えば, 以下のコードはボタンをクリックするたびに, 再生・停止することを期待していますが, 2 回目のクリック以降は, 再生されずエラーが発生します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const source = new AudioBufferSourceNode(context);

// AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Start immediately
  // But, cannot start since the second times ...
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if (source.buffer === null) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      source.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            期待する動作, つまり, 再生・停止を繰り返すには, 一度 <code>start</code>・<code>stop</code> した (あるいは, <code>duration</code> まで再生した)
            <code>AudioBufferSourceNode</code> インスタンスは破棄して, 再度 <code>AudioBufferSourceNode</code> インスタンスを生成します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let source = null;
let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  source = new AudioBufferSourceNode(context, { buffer });

  // AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);

  // Start immediately
  source.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (source === null)) {
    return;
  }

  // Stop immediately
  source.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            ワンショットオーディオも, 複数の <code>AudioBufferSourceNode</code> インスタンスを <code>AudioDestinationNode</code> に接続することで合成が可能です
            (そのまま合成 (接続) してしまうと, 振幅が大きくなりすぎて, 音割れが発生してしまうので, <code>GainNode</code> を接続して振幅を調整しています).
          </p>
          <p>
            また, 3 つの <code>AudioBufferSourceNode</code> インスタンスで, それぞれ <code>detune</code> プロパティの値を調整して, C
            メジャーコードを再生しています.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let sourceC = null;
let sourceE = null;
let sourceG = null;

let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  sourceC = new AudioBufferSourceNode(context, { buffer });
  sourceE = new AudioBufferSourceNode(context, { buffer });
  sourceG = new AudioBufferSourceNode(context, { buffer });

  sourceC.detune.value = 0;
  sourceE.detune.value = 400;
  sourceG.detune.value = 700;

  const gain = new GainNode(context, { gain: 0.25 });

  // AudioBufferSourceNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  sourceC.connect(gain);
  sourceE.connect(gain);
  sourceG.connect(gain);

  gain.connect(context.destination);

  // Start immediately
  sourceC.start(0);
  sourceE.start(0);
  sourceG.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (sourceC === null) || (sourceE === null) || (sourceG === null)) {
    return;
  }

  // Stop immediately
  sourceC.stop(0);
  sourceE.stop(0);
  sourceG.stop(0);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
          <p>
            <code>AudioBufferSourceNode</code> でも, <b><code>start</code> メソッドと <code>stop</code> メソッドは一対</b>という仕様は,
            さまざまなプラットフォームのオーディオ API のなかでも Web Audio API 独自の仕様で, ハマりやすい仕様なので注意してください (そもそも, Web
            ではないプラットフォームのオーディオ API はここまで抽象化されている API すら少ないと思います).
          </p>
        </section>
      </section>
      <section id="section-media-element-audio-source-node">
        <h2>MediaElementAudioSourceNode</h2>
        <p>
          Web Audio API において, 楽曲データに対してなんらかのオーディオ信号処理を適用したい場合に利用するのが
          <b><code>MediaElementAudioSourceNode</code></b> です. 具体的には, <code>HTMLMediaElement</code> (<code>HTMLAudioElement</code> や
          <code>HTMLVideoElement</code>) のオーディオデータに対するオーディオ信号処理を適用する場合に利用します.
        </p>
        <img src="images/media-element-audio-source-node.png" alt="MediaElementAudioSourceNode" width="1232" height="770" loading="lazy" />
        <p>
          <code>HTMLMediaElement</code> を音源にするので, <code>MediaElementAudioSourceNode</code> コンストラクタの第 2 引数 (<b
            ><code>MediaElementAudioSourceOptions</code></b>
          型) の <b><code>mediaElement</code></b> プロパティ (もしくは, ファクトリメソッドの <code>createMediaElementSource</code> の引数) に,
          <code>HTMLMediaElement</code> を指定します.
        </p>
        <p>
          また, コンストラクタやファクトリメソッドに指定する <code>HTMLMediaElement</code> が HTML パース時点で,
          <code>src</code> 属性に指定しているメディアファイルが同一オリジンでない場合, クロスオリジン制限にかかってしまうので,
          <b><code>crossorigin</code> 属性に <code>&apos;anonymous&apos;</code> を設定</b>しておく必要あります. この属性と値の設定によって,
          <b>オリジン間リソース共有</b> (<b>CORS</b>: <b>Cross-Origin Resources Sharing</b>) が可能となります (<b
            ><code>HTMLMediaElement</code> のみで再生する場合は不要です</b>).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;!-- シューベルト 交響曲 第8番 ロ短調 D759 「未完成」 第1楽章 (余談ですが, X JAPAN の「ART OF LIFE」のモチーフになっている楽曲です) --&gt;
&lt;audio src=&quot;https://korilakkuma.github.io/Web-Music-Documentation/assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&quot; crossorigin="anonymous" controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const audioElement = document.querySelector(&apos;audio&apos;);

const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

// If use `createMediaElementSource`
// const source = context.createMediaElementSource(audioElement);

// MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
source.connect(context.destination);</code></pre>
        <p>
          <code>MediaElementAudioSourceNode</code> インスタンス生成には 2 点注意すべき点があります. 上記のサンプルコードのように,
          <code>HTMLMediaElement</code> に HTML パース時点で, <code>src</code> 属性にメディアファイルが指定されている場合は, 特に問題ありませんが,
          インタラクティブに, 例えば, ユーザーのファイルシステムからメディアファイルを選択するような場合,
          <b><code>HTMLMediaElement</code> の <code>loadstart</code> イベント発火以降にインスタンスを生成する必要があります</b> (逆に, HTML パース時点で
          <code>src</code> 属性にメディアファイルを指定している場合, <code>loadstart</code> イベントは発火しないので注意が必要です).
          <code>loadstart</code> イベント以降に発火するイベントであればよいので, <code>canplaythrough</code> イベントハンドラなどで
          <code>MediaElementAudioSourceNode</code> インスタンスを生成してもよいでしょう.
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <p>
          もう 1 点は, 1 つの <code>HTMLMediaElement</code> に対して 1 つの <code>MediaElementAudioSourceNode</code> インスタンスが対応しているという点です.
          例えば, <code>HTMLMediaElement</code> の <code>src</code> 属性のみを変更する場合,
          <code>MediaElementAudioSourceNode</code> インスタンスを再度生成するとエラーが発生します (逆に, 別のオブジェクトとなる
          <code>HTMLMediaElement</code> を指定する場合, <code>MediaElementAudioSourceNode</code> インスタンスを生成する必要があります).
        </p>
        <p>
          したがって, 先ほどのサンプルコードだと, 2 回以上, ファイルを選択してしまうと, 同じ <code>HTMLAudioElement</code> に対して, 複数回
          <code>MediaElementAudioSourceNode</code> インスタンスが生成されてエラーが発生してしまうので, 以下のように変更します.
        </p>
        <p>
          また, <code>File API</code> から選択した楽曲データを, <code>HTMLMediaElement</code> の <code>src</code> 属性に指定する場合, Object URL を利用します
          (<code>FileReader API</code> を使って Data URL を利用しても可能ですが, 実装が増えるだけなので, なんらかの理由がなければ
          <code>createObjectURL</code> を利用して Object URL を設定するのがよいでしょう).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

let source = null;

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
  source.connect(context.destination);
});</code></pre>
        <section id="section-media-element-audio-source-node-start-and-stop">
          <h3>再生と停止</h3>
          <p>
            <code>MediaElementAudioSourceNode</code> に楽曲データを再生・停止するためのメソッドはありません. 再生や一時停止は, コンストラクタの引数に指定した
            <code>HTMLMediaElement</code> の <code>play</code> / <code>pause</code> メソッドを実行します. したがって, <code>OscillatorNode</code> や
            <code>AudioBufferSourceNode</code> のように使い捨てのノードではない, つまり, インスタンスを再度生成して
            <code>AudioDestinationNode</code> に再度接続する必要もないので, この点は直感的な仕様と言えます.
          </p>
          <p>
            あとは, <code>AudioDestinationNode</code> に接続すれば, 再生・停止することは簡単ですが, これでは
            <code>HTMLMediaElement</code> をそのまま利用するほうが合理的なので, 簡易例として, オーディオ信号処理を適用していることがわかるように,
            <code>BiquadFilterNode</code> を利用して Low-Pass Filter (低域通過フィルタ) を使ったサンプルコードです. カットオフ周波数を変更すると,
            音の輪郭が変わることを確認してみてください (<code>BiquadFilterNode</code> に関しては,
            <a href="#section-effectors-filter-biquad-filter-node">フィルタのセクション</a>で詳細を解説します).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;audio controls /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement = document.querySelector(&apos;audio&apos;);

const inputCutoffElement = document.getElementById(&apos;range-cutoff&apos;);
const spanElement        = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanElement.textContent = `${lowpass.frequency.value} Hz`;
});

// UI (by `controls` attribute) plays and pauses media
audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});</code></pre>
        </section>
        <section id="section-html-media-element-and-media-element-audio-source-node">
          <h3>HTMLMediaElement と MediaElementAudioSourceNode</h3>
          <p>
            すでにサンプルコードを実行して, お気づきになったかもしれませんが,
            <code>HTMLMediaElement</code> のプロパティやイベントハンドラはすべて利用することが可能です. <code>volume</code> や <code>muted</code>,
            <code>playbackRate</code> は再生する楽曲データそのものに影響します. <code>autoplay</code> や <code>loop</code> は再生における UX に影響します. また,
            実際のプロダクトでは, <code>loadedmetadata</code> イベント, <code>canplaythrough</code>イベント, <code>timeupdate</code> イベント,
            <code>ended</code> イベントなどで, UI を更新するイベントハンドラを実行することも多いでしょう. このドキュメントですべてを解説することはできないので,
            <a href="https://html.spec.whatwg.org/multipage/media.html" target="_blank" rel="noopener noreferrer">HTMLMediaElement</a>
            の仕様などを参考にしてください.
          </p>
          <p>
            よくある実装として, <code>loadedmetadata</code> イベントで <code>duration</code> プロパティ (トータルの再生時間秒数) を取得,
            <code>timeupdate</code> イベントで <code>currentTime</code> プロパティ (現在の再生位置) を更新,
            <code>ended</code> イベントで初期表示に戻すというのは Web Audio API に直接関係はありませんが, メディアデータをあつかう Web
            アプリケーションでは必須になるような実装なので理解しておいて損はないでしょう. また, <code>MediaElementAudioSourceNode</code> の解説に着目するために
            <code>HTMLMediaElement</code> の <code>controls</code> 属性での UI で再生・一時停止を実装していましたが,
            再生・停止ボタンも実装したサンプルコードです. コードをご覧になると理解できるかもしれませんが, Web Audio API
            のコードは変更されていないことにも着目してみてください.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;play&lt;/button&gt;
&lt;span id=&quot;print-current-time&quot;&gt;00 : 00&lt;/span&gt; / &lt;span id=&quot;print-duration&quot;&gt;00 : 00&lt;/span&gt;
&lt;input type=&quot;file&quot; /&gt;
&lt;label for=&quot;range-cutoff&quot;&gt;cutoff&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-cutoff&quot; value=&quot;4000&quot; min=&quot;350&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-cutoff-value&quot;&gt;4000 Hz&lt;/span&gt;
&lt;audio /&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const inputElement  = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
const audioElement  = document.querySelector(&apos;audio&apos;);

const spanCurrentTimeElement = document.getElementById(&apos;print-current-time&apos;);
const spanDurationElement    = document.getElementById(&apos;print-duration&apos;);
const inputCutoffElement     = document.getElementById(&apos;range-cutoff&apos;);
const spanCutoffElement      = document.getElementById(&apos;print-cutoff-value&apos;);

let source = null;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);

  // If use Data URL,
  //
  // const reader = new FileReader();
  //
  // reader.onload = () =&gt; {
  //   audioElement.src = reader.result;
  // };
  //
  // reader.readAsDataURL(file);
});

inputCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  lowpass.frequency.value = event.currentTarget.valueAsNumber;

  spanCutoffElement.textContent = `${lowpass.frequency.value} Hz`;
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
  }

  // MediaElementAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  source.connect(lowpass);
  lowpass.connect(context.destination);
});

audioElement.addEventListener(&apos;loadedmetadata&apos;, () =&gt; {
  spanDurationElement.textContent = `${Math.trunc(audioElement.duration / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.duration) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;timeupdate&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = `${Math.trunc(audioElement.currentTime / 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)} : ${(Math.trunc(audioElement.currentTime) % 60).toString(10).slice(0, 2).padStart(2, &apos;0&apos;)}`;
});

audioElement.addEventListener(&apos;ended&apos;, () =&gt; {
  spanCurrentTimeElement.textContent = &apos;00 : 00&apos;;
});

buttonElement.addEventListener(&apos;click&apos;, async () =&gt; {
  if (audioElement.paused) {
    await audioElement.play();

    buttonElement.textContent = &apos;pause&apos;;
  } else {
    audioElement.pause();

    buttonElement.textContent = &apos;play&apos;;
  }
});</code></pre>
        </section>
      </section>
      <section id="section-media-stream-audio-source-node">
        <h2>MediaStreamAudioSourceNode</h2>
        <p>
          Web Audio API において, マイクロフォンやオーディオインターフェースに入力されたサウンドデータに対して,
          なんらかのオーディオ信号処理を適用したい場合に利用するのが <b><code>MediaStreamAudioSourceNode</code></b> です. もっと言ってしまえば,
          <b>WebRTC</b> (<b><code>MediaDevices</code></b> の <b><code>getUserMedia</code></b> メソッドで取得できる <b><code>MediaStream</code></b> インスタンス)
          で取得したサウンドデータに対するオーディオ信号処理を適用する場合に利用します.
        </p>
        <img src="images/media-stream-audio-source-node.png" alt="MediaStreamAudioSourceNode" width="1232" height="770" loading="lazy" />
        <p>
          WebRTC の仕様は Web Audio API と同等かそれ以上に膨大ですが, Web Audio API との関係で言えば, <code>MediaDevices</code> の
          <code>getUserMedia</code> メソッドを理解すれば問題ないでしょう.
        </p>
        <p>
          <code>getUserMedia</code> メソッドの引数には
          <a href="https://www.w3.org/TR/mediacapture-streams/#dom-mediastreamconstraints" target="_blank" rel="noopener noreferrer">MediaStreamConstraints</a>
          を指定します (少なくとも, Web Audio API で利用することを想定するので, <code>audio</code> は <code>true</code> にしておきます). 初回実行時は,
          マイクロフォン (もしくは, 選択したオーディオインターフェース) に対するアクセス許可を求めるダイアログが表示されます. アクセスを許可して問題なければ,
          戻り値の <code>Promise</code> が <code>fulfilled</code> 状態になります. 成功時の <code>Promise</code> のコールバック関数の引数に
          <code>MediaStream</code> インスタンスが渡されるので, そのインスタンスを <code>MediaElementAudioSourceNode</code> コンストラクタ
          (もしくはファクトリメソッドの <code>createMediaStreamSource</code> の引数) に指定します
        </p>
        <p>
          あとは, <code>MediaStreamAudioSourceNode</code> インスタンスを <code>AudioDestinationNode</code> に接続すれば, WebRTC
          からのサウンドデータを出力することが可能です.
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    // MediaStreamAudioSourceNode (Input) -&gt; AudioDestinationNode (Output)
    source.connect(context.destination);

  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        <p>
          もちろん, オーディオ信号処理を適用しないのであれば, WebRTC だけを利用するほうが合理的なので, 簡易例として,
          オーディオ信号処理を適用していることがわかるように, <code>BiquadFilterNode</code> を利用して Low-Pass Filter (低域通過フィルタ)
          を使ったサンプルコードです. カットオフ周波数を変更すると, 音の輪郭が変わることを確認してみてください (<code>BiquadFilterNode</code> に関しては,
          別のセクションで詳細を解説します).
        </p>
        <pre
          data-prismjs-copy="クリップボードにコピー"
          data-prismjs-copy-success="コピーしました"
        ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        <section id="section-media-stream-audio-source-node-enumerate-devices">
          <h3>デバイス情報の列挙</h3>
          <p>
            <code>MediaStreamAudioSourceNode</code> で, デフォルトの入力オーディオデバイス (多くの場合, 内臓のマイクロフォン)
            を使う場合には特に必要ありませんが, 指定のデバイスを入力オーディオデバイスとしたり, Web Audio API 1.1 以降では,
            <code>AudioContext</code> インスタンスの <b><code>setSinkId</code></b> メソッドで出力オーディオデバイスを指定できたりします.
          </p>
          <p>
            そのために, 利用可能なデバイス情報を取得する必要があります. <b><code>MediaDeviceInfo</code></b> はデバイスの情報を定義するクラス (<code>kind</code>
            プロパティでデバイスの種類 (<code>MediaDeviceKind</code> 列挙型) や, <code>label</code> プロパティでデバイス名,
            <code>deviceId</code> プロパティでデバイスの識別子などが定義されています) で, <code>MediaDevices</code> の
            <b><code>enumerateDevices</code></b> メソッドを呼び出すと, 成功時の <code>Promise</code> のコールバック関数の引数に,
            <b><code>MediaDeviceInfo</code></b> インスタンスの配列が渡されます. この配列にアクセスすることで,
            利用可能な入出力オーディオデバイスや映像デバイスの情報を取得できます.
          </p>
          <p>
            <b><code>MediaDeviceKind</code></b> 列挙型は, <b><code>&apos;audioinput&apos;</code></b>, <b><code>&apos;audiooutput&apos;</code></b>, <b><code>&apos;videoinput&apos;</code></b> のいずれかの文字列になります. したがって, 例えば, 入力オーディオデバイスの情報のみ必要であれば,
            <code>MediaDeviceInfo</code> の <code>kind</code> プロパティが, <code>audioinput</code> のインスタンスだけフィルタリングすればよいでしょう.
          </p>
          <p>
            注意点として, <code>MediaDevices</code> の <code>enumerateDevices</code> メソッドで利用可能なデバイスを取得するためには,
            デバイスへのアクセスを許可している必要があるので, 先に <code>getUserMedia</code> メソッドを実行しておきます.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;select id=&quot;select-input-devices&quot;&gt;&lt;/select&gt;
&lt;select id=&quot;select-output-devices&quot;&gt;&lt;/select&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then(() =&gt; {
    navigator.mediaDevices.enumerateDevices()
      .then((deviceInfos) =&gt; {
        // `deviceInfos` is `MediaDeviceInfo` list (`MediaDeviceInfo[]`)

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as input audio devices
        const inputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audioinput&apos;;
        });

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as output audio devices
        const outputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audiooutput&apos;;
        });

        const fragmentInputDevices  = document.createDocumentFragment();
        const fragmentOutputDevices = document.createDocumentFragment();

        inputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentInputDevices.appendChild(optionElement);
        });

        outputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentOutputDevices.appendChild(optionElement);
        });

        const selectInputDevicesElement  = document.getElementById(&apos;select-input-devices&apos;);
        const selectOutputDevicesElement = document.getElementById(&apos;select-output-devices&apos;);

        selectInputDevicesElement.appendChild(fragmentInputDevices);
        selectOutputDevicesElement.appendChild(fragmentOutputDevices);
      })
      .catch((error) =&gt; {
        // error handling
      });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-media-stream-audio-source-node-designate-input-audio-device">
          <h3>入力オーディオデバイスの指定</h3>
          <p>
            入力オーディオデバイスを指定するには, <code>MediaDevices</code> の <code>enumerateDevices</code> メソッドで取得した,
            <code>MediaDeviceInfo</code> インスタンスのうち, <code>kind</code> プロパティが <code>&apos;audioinput&apos;</code> のインスタンスの,
            <b
              ><code>deviceId</code> プロパティを <code>MediaStreamConstraints</code> の <code>audio</code> プロパティの
              <code>deviceId</code> プロパティに指定します</b>. そして, <code>MediaStreamConstraints</code> が更新されるので, 再度, <code>MediaDeviceInfo</code> の
            <code>getUserMedia</code> メソッドを呼び出すことで, 指定した入力オーディオデバイスの <code>MediaStream</code> インスタンスを取得することができます.
            1 つ注意点としては, オーディオデバイスを切り替えるたびに, 次のセクションで解説する<b>デバイスの破棄を実行する必要があります</b> (これを実行しないと,
            デバイスが接続されたままになるので, 多くの場合, それは意図する動作ではないと考えられるので).
          </p>
          <img src="images/media-device-info.png" alt="MediaDeviceInfo" width="1232" height="770" loading="lazy" />
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;select id=&quot;select-input-devices&quot;&gt;&lt;/select&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

let mediaStream = null;

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    mediaStream = stream;

    const source = new MediaStreamAudioSourceNode(context, { mediaStream });

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);

    navigator.mediaDevices.enumerateDevices()
      .then((deviceInfos) =&gt; {
        // `deviceInfos` is `MediaDeviceInfo` list (`MediaDeviceInfo[]`)

        // `MediaDeviceInfo` list (`MediaDeviceInfo[]`) as audio inputs
        const inputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
          return deviceInfo.kind === &apos;audioinput&apos;;
        });

        const fragmentInputDevices = document.createDocumentFragment();

        inputDeviceInfos.forEach((deviceInfo) =&gt; {
          const optionElement = document.createElement(&apos;option&apos;);

          optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

          const textNode = document.createTextNode(deviceInfo.label);

          optionElement.appendChild(textNode);

          fragmentInputDevices.appendChild(optionElement);
        });

        const selectInputDevicesElement = document.getElementById(&apos;select-input-devices&apos;);

        selectInputDevicesElement.appendChild(fragmentInputDevices);

        selectInputDevicesElement.addEventListener(&apos;change&apos;, async (event) =&gt; {
          // Stop previous selected input audio device
          const audioTracks = mediaStream.getAudioTracks();

          for (const audioTrack of audioTracks) {
            audioTrack.stop();
          }

          const deviceId = event.currentTarget.value;

          mediaStream = await navigator.mediaDevices.getUserMedia({ audio: { deviceId } });

          const source = new MediaStreamAudioSourceNode(context, { mediaStream });

          // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
          source.connect(lowpass);
          lowpass.connect(context.destination);
        });
      })
      .catch((error) =&gt; {
        // error handling
      });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-media-stream-audio-source-node-media-stream-track-stop">
          <h3>デバイスの破棄</h3>
          <p>
            <code>MediaStreamAudioSourceNode</code> には, オーディオデバイスがらの入力を停止するためのメソッドはありません. デバイスを破棄するには,
            <code>MediaStream</code> インスタンスの <b><code>getAudioTracks</code></b> メソッドで, オーディオデバイスの実体である
            <b><code>MediaStreamTrack</code></b> インスタンスの配列を取得します. <code>MediaStreamTrack</code> には, デバイスを破棄するための
            <b><code>stop</code></b> メソッドが実装されているので, 対象の <code>MediaStreamTrack</code> インスタンスで <code>stop</code> メソッドを実行します
            (同様に, ビデオデバイスを停止するには <b><code>getVideoTracks</code></b> メソッドで <code>MediaStreamTrack</code> インスタンスの配列を取得します).
          </p>
          <img src="images/media-stream.png" alt="MediaStream" width="1232" height="770" loading="lazy" />
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

navigator.mediaDevices.getUserMedia(constraints)
  .then((stream) =&gt; {
    const source = new MediaStreamAudioSourceNode(context, { mediaStream: stream });

    // If use `createMediaStreamSource`
    // const source = context.createMediaStreamSource(stream);

    const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000 });

    // MediaStreamAudioSourceNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    source.connect(lowpass);
    lowpass.connect(context.destination);

    window.setTimeout(() =&gt; {
      const audioTracks = stream.getAudioTracks();

      for (const audioTrack of audioTracks) {
        audioTrack.stop();
      }
    }, 10000);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
      </section>
      <section id="section-audio-worklet">
        <h2>AudioWorklet</h2>
        <p>
          Web Audio API には, 基本波形のサウンド生成やエフェクター, サウンドの視覚化など高度なサウンド処理をより簡単に実装するために, 様々な
          <code>AudioNode</code> が定義されています. これらの <code>AudioNode</code> があるおかげで, 内部で実行されているオーディオ信号処理の詳細を知らなくても,
          高度なサウンド機能の実装が簡単にできるわけです. 例えば, 正弦波の数式を知らなくても,
          <code>OscillatorNode</code> によって正弦波を生成することができました.
        </p>
        <p>
          Web Audio API が定義する多くの <code>AudioNode</code> は, サウンドデータの実体にアクセスする機能をもちません. なぜなら, <code>AudioNode</code> (と,
          <code>AudioNode</code> がもつ <code>AudioParam</code>) は, サウンド処理を抽象化する, つまり, 抽象度の高い API として定義されているからです.
          しかしながら, その代償として, <code>AudioNode</code> の接続と <code>AudioParam</code> の制御では不可能なオーディオ処理も存在してしまいます. 具体的に,
          現状の仕様では, ノイズ生成, ノイズサプレッサー, ボーカルキャンセラ, ピッチシフターなどは <code>AudioNode</code> の接続と
          <code>AudioParam</code> の制御のみでは実装できないので, 直接サウンドデータにアクセスできる必要があります.
        </p>
        <p>
          直接サウンドデータにアクセスすることを可能にするのが, (広義の) <b><code>AudioWorklet</code></b> です (狭義には
          <code>AudioWorklet</code> クラスを意味するので). <code>AudioWorklet</code> は複数の API で構成されており, メインスレッドで
          <code>AudioNode</code> を継承する <b><code>AudioWorkletNode</code></b>, オーディオスレッド (<b><code>AudioWorkletGlobalScope</code></b>) で直接サウンドデータにアクセスすることを可能にする <b><code>AudioWorkletProcessor</code></b>, メインスレッドからオーディオスレッドのファイルをロードする <code>AudioContext</code> インスタンスがもつ
          <b><code>AudioWorklet</code></b> インスタンスです.
        </p>
        <section id="section-audio-worklet-node">
          <h3>AudioWorkletNode</h3>
          <p>
            メインスレッドで定義されていて, <code>AudioNode</code> クラスを継承しています. <code>AudioWorkletNode</code> を
            <code>AudioNode</code> に接続することで, <code>AudioWorkletProcessor</code> (の <code>process</code> メソッド)
            で実装したオーディオ信号処理が適用されて, 次に接続している <code>AudioNode</code> への入力として出力されます.また, <code>AudioNode</code> を
            <code>AudioWorkletNode</code> に接続することで, <code>AudioWorkletProcessor</code> に入力サウンドデータとして渡して,
            オーディオ信号処理を適用することも可能です.
          </p>
          <p>
            <code>AudioWorkletNode</code> コンストラクタの第 1 引数には, <code>AudioContext</code> インスタンスを指定し, 第 2 引数には,
            <code>AudioWorkletGlobalScope</code> (オーディオスレッド) で <code>registerProcessor</code> メソッドで指定した文字列を指定します.
            <code>AudioWorkletNode</code> インスタンスを生成するのは, <code>AudioWorklet</code> インスタンスの
            <b><code>addModule</code> メソッド成功後に実行する必要があります</b> (また, 後発な API であるので,
            ファクトリメソッドによるインスタンス生成も仕様定義されていないことに注意してください).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/processor.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/processor.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-audio-worklet-global-scope">
          <h3>AudioWorkletGlobalScope</h3>
          <p>
            AudioWorklet はその API の仕様設計上, メインスレッドとは別のオーディオスレッドを専用に生成することになります.
            このオーディオスレッドのグローバルスコープが <b><code>AudioWorkletGlobalScope</code></b> です. つまり, メインスレッドにおける
            <code>Window</code> に相当します. メインスレッドとは別の世界なので, 直接 DOM にはアクセスできなかったり,
            メインスレッドで使えるようなクライアントサイド JavaScript API が利用できなかったりします. <code>AudioWorkletGlobalScope</code> には,
            <code>sampleRate</code> プロパティや <code>currentTime</code> プロパティが定義されていますが, これはメインスレッドの
            <code>AudioContext</code> インスタンスと同値です.
          </p>
          <section id="section-audio-worklet-global-scope-register-processor">
            <h4>registerProcessor メソッド</h4>
            <p>
              <code>AudioWorkletGlobalScope</code> で定義されている, 最も重要なメソッドが <b>registerProcessor</b> メソッドです. メインスレッドの
              <code>AudioWorkletNode</code> と, オーディオスレッドの <code>AudioWorkletProcessor</code> を継承したクラスを関連づける役割をもっているからです. 第
              1 引数に, <code>AudioWorkletNode</code> のコンストラクタに関連づける文字列を, 第 2 引数に,
              <code>AudioWorkletProcessor</code> を継承したクラスを指定します (インスタンスではないので注意してください).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    // TODO: Audio Signal Processing

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
          </section>
        </section>
        <section id="section-audio-worklet-processor">
          <h3>AudioWorkletProcessor</h3>
          <section id="section-audio-worklet-processor-subclass">
            <h4>AudioWorkletProcessor の継承クラス</h4>
            <p>
              <code>AudioWorklet</code> を構成する API で, 実際にオーディオ信号処理を実行するのが,
              <b><code>AudioWorkletProcessor</code></b> クラスを継承したサブクラスです (<code>AudioWorkletProcessor</code> を継承したサブクラスを).
              <code>AudioWorkletProcessor</code> クラスで最も重要な API が <b><code>process</code></b> メソッドです.
              <code>AudioWorkletProcessor</code> を継承するサブクラスは <b><code>process</code> メソッドを必ずオーバライドする必要があります</b>.
            </p>
            <p>
              また, 実用的なことを考慮すると, <b><code>MessagePort</code></b> インスタンスである<b><code>port</code></b> プロパティも事実上, 必須と言えます.
              <code>AudioWorkletGlobalScope</code> に定義されている <code>AudioWorkletProcessor</code> (を継承したサブクラス) は, メインスレッドで設定された値
              (例えば, <code>input[type=&quot;range&quot;]</code> で設定された値) を直接的に取得することができません. そこで,
              <code>MessagePort</code> インススタンスの <b><code>messssage</code></b> イベントハンドラや <b><code>postMessage</code></b> メソッドを利用して,
              いわゆる <b>メッセージパッシング</b> (<b>Message Passing</b>) でメインスレッドとデータを送受信する必要があります.
            </p>
            <section id="section-audio-worklet-processor-process">
              <h5>process メソッド (AudioWorkletProcessCallback)</h5>
              <p>
                <code>process</code> メソッドの 第 1 引数には入力サウンドデータとなる <code>Float32Array</code>, 第 2 引数には出力サウンドデータとなる
                <code>Float32Array</code>, 第 3 引数には, 独自に <code>AudioParam</code> を定義する場合にパラメータとなる
                <code>Float32Array</code> がそれぞれ渡されます. これらの <code>Float32Array</code> のサイズは, すべて <b><code>128</code></b> (サンプル) です
                (しかしながら,
                <a href="#section-audio-worklet-processor-render-quantum-size"
                  ><b>Web Audio API 1.1 以降では必ずしも <code>128</code> サンプルではなくなる可能性があります</b></a>). また, 第 1 引数と第 2 引数 (入力サウンドデータと出力サウンドデータ) は, 実際には,
                <code>FrozenArray&lt;FrozenArray&lt;Float32Array&gt;&gt;</code> と配列の入れ子になっています (仕様上の定義として
                <code>FrozenArray</code> ですが, 実装上は <code>Array</code> です. 1 つ内側の <code>Array</code> がチャンネルごとの
                <code>Float32Array</code> を格納するためです).
              </p>
              <p>
                <code>AudioWorkletNode</code> に接続している <code>AudioNode</code> がなければ第 1 引数は不要です. よって, 必須となるのは,
                出力サウンドデータである, <code>128</code> サンプルの <code>Float32Array</code> にオーディオ信号処理を適用した値を格納していくことです.
                そのミニマムな実装例として, ホワイトノイズ (白色雑音) を生成する <code>process</code> メソッドのサンプルコードを記載します.
              </p>
              <p>
                ちなみに, 2 つの <code>Array</code> (<code>FrozenArray</code>) の入れ子になっているので, 形式的に, 入力サウンドデータ, 出力サウンドデータともに,
                0 番目の <code>Array&lt;Float32Array&gt;</code> (<code>FrozenArray&lt;Float32Array&gt;</code>) を取得すると理解していただいて問題ないでしょう
                (なぜこのような仕様になっているのかは, オーナーは理解できていません).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = (2 * Math.random()) - 1;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                ちなみに, <code>process</code> メソッドの戻り値は <code>boolean</code> ですが, ここは形式的に,
                <b><code>true</code> を返す</b>と理解していただいて問題ないでしょう.
                <b><code>true</code> を返すことで, <code>128</code> サンプルごとに <code>process</code> メソッドが繰り返し実行されるからです</b>. 1 度でも
                <code>false</code> を返した <code>AudioWorkletProcessor</code> は破棄されるような仕様になっているので, 戻り値を切り替える (<code>true</code> or
                <code>false</code>) ユースケースがほとんどないからです.
              </p>
            </section>
            <article id="section-audio-worklet-processor-render-quantum-size">
              <h5>Web Audio API 1.1 以降における render quantum size</h5>
              <p>
                Web Audio API 1.0 まで render quantum size は <code>128</code> サンプルの固定値でしたが, Web Audio API 1.1 の仕様では,
                <a href="https://www.w3.org/TR/webaudio/#enumdef-audiocontextrendersizecategory" target="_blank" rel="noopener noreferrer"
                  ><b><code>AudioContextRenderSizeCategory</code></b> 列挙型の値</a>
                (<code>AudioContextOptions</code> の <b><code>renderSizeHint</code></b> オプションの値) を,
                <b><code>&apos;hardware&apos;</code></b> に設定した場合, ユーザーの環境によって最適な render quantum size が選択されて, 必ずしも
                <code>128</code> サンプルでなくなる可能性があります. したがって, 実装においても,
                <code>128</code> サンプルをマジックナンバーとして定義するよりは, 上記のコード例のように, チャンネルごとの <code>Float32Array</code> の
                <code>length</code> プロパティから render quantum size を取得するほうが, 今後の仕様に合わせてコードを変更する必要がなくなります.
              </p>
              <p>
                もっとも, <code>AudioContextRenderSizeCategory</code> を <b><code>&apos;default&apos;</code></b> に設定している場合 (デフォルト値も
                <code>&apos;default&apos;</code> なので, <code>renderSizeHint</code> オプションの値を変更しなければ,
                <code>128</code> サンプルと決められているので, 制作する Web Music アプリケーションに応じて, マジックナンバーとして定義しても問題ないでしょう
                (ライブラリやフレームワークのような汎用的なコードの場合は, render quantum size は, チャンネルごとの <code>Float32Array</code> の
                <code>length</code> プロパティから render quantum size を取得するか, <code>AudioContext</code> インスタンスの
                <a href="https://www.w3.org/TR/webaudio/#dom-baseaudiocontext-renderquantumsize" target="_blank" rel="noopener noreferrer"
                  ><b><code>renderQuantumSize</code></b></a>
                (ただし, <b>Web Audio API 1.1 公開日時点で実装されているブラウザはありません</b>) を参照するのが安全と言えます).
              </p>
              <p>このあたりは, 制作するプロダクトの仕様に応じた<b>判断の問題</b>と言えそうです.</p>
            </article>
            <section id="section-audio-worklet-processor-message-port">
              <h5>port プロパティ (MessagePort インスタンス)</h5>
              <p>
                <code>MessagePort</code> の仕様は Web Audio API の仕様とは別にある (Web Audio API に依存している API ではない) のですが,
                実用上必須となるので簡単に解説しておきます (理解されている場合はスキップしてください).
              </p>
              <p>
                メインスレッドから <code>postMessage</code> されたデータを受信するには <code>messssage</code> イベントハンドラの
                <code>MessageEvent</code> イベントオブジェクトにアクセスする必要がありますが, <code>AudioWorkletProcessor</code> (を継承したサブクラス)
                で呼び出されるのは, コンストラクタと <code>process</code> メソッドのみです. イベントハンドラは, 一度設定してしまえばいいので,
                コンストラクタで設定するという実装が定石となります.
              </p>
              <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;select&gt;
  &lt;option value=&quot;whitenoise&quot; selected&gt;White Noise&lt;/option&gt;
  &lt;option value=&quot;pinknoise&quot;&gt;Pink Noise&lt;/option&gt;
  &lt;option value=&quot;browniannoise&quot;&gt;Brownian Noise&lt;/option&gt;
&lt;/select&gt;
</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/processor.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/processor.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/processor.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.type = &apos;whitenoise&apos;;

    this.b0 = 0;
    this.b1 = 0;
    this.b2 = 0;
    this.b3 = 0;
    this.b4 = 0;
    this.b5 = 0;
    this.b6 = 0;

    this.lastOut = 0;

    this.port.onmessage = (event) =&gt; {
      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    switch (this.type) {
      case &apos;whitenoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            output[channelNumber][n] = (2 * Math.random()) - 1;
          }
        }

        break;
      }

      case &apos;pinknoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            this.b0 = (0.99886 * this.b0) + (white * 0.0555179);
            this.b1 = (0.99332 * this.b1) + (white * 0.0750759);
            this.b2 = (0.96900 * this.b2) + (white * 0.1538520);
            this.b3 = (0.86650 * this.b3) + (white * 0.3104856);
            this.b4 = (0.55000 * this.b4) + (white * 0.5329522);
            this.b5 = (-0.7616 * this.b5) - (white * 0.0168980);

            output[channelNumber][n] = this.b0 + this.b1 + this.b2 + this.b3 + this.b4 + this.b5 + this.b6 + (white * 0.5362);
            output[channelNumber][n] *= 0.11;

            this.b6 = white * 0.115926;
          }
        }

        break;
      }

      case &apos;browniannoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            output[channelNumber][n] = (this.lastOut + (0.02 * white)) / 1.02;

            this.lastOut = output[channelNumber][n];

            output[channelNumber][n] *= 3.5;
          }
        }

        break;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                また, <code>MessagePort</code> は相互に送受信することができるので, オーディオスレッド (<code>AudioWorkletGlobalScope</code>)
                からメインスレッドにデータを送信することも可能です. その場合, <code>AudioWorkletNode</code> の <code>port</code> プロパティが
                <code>MessagePort</code> インスタンスとなるので, 同様に <code>messssage</code> イベントハンドラをメインスレッドで実装すれば,
                <code>MessageEvent</code> イベントオブジェクトから, <code>postMessage</code> されたデータを受信することが可能になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/message.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/message.js&apos;)
  .then(() =&gt; {
    const oscillator = new OscillatorNode(context);
    const processor  = new AudioWorkletNode(context, &apos;MessageProcessor&apos;);

    // OscillatorNode (Input) -&gt; AudioWorkletNode (Bypass) -&gt; AudioDestinationNode (Output)
    oscillator.connect(processor);
    processor.connect(context.destination);

    oscillator.start(0);

    processor.port.onmessage = (event) =&gt; {
      if (event.data) {
        console.log(event.data);
      }
    };
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/message.js&apos;

class MessageProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = input.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      output[channelNumber].set(input[channelNumber]);
    }

    this.port.postMessage({ messaage: &apos;Bypass samples&apos; });

    return true;
  }
}

registerProcessor(&apos;MessageProcessor&apos;, MessageProcessor);</code></pre>
            </section>
            <section id="section-audio-worklet-processor-parameter-descriptors">
              <h5>parameterDescriptors メソッド</h5>
              <p>
                <b><code>parameterDescriptors</code></b> メソッドは, <code>AudioWorkletProcessor</code> で独自の
                <code>AudioParam</code> を実装したい場合に使います. <code>process</code> メソッドや <code>port</code> プロパティのように (事実上)
                必須の実装というわけではないので, ユースケースとして不要であればスキップしてください.
              </p>
              <p>
                <code>parameterDescriptors</code> メソッドは Getter です. <b><code>AudioParamDescriptor</code></b> の配列を返すように実装します.
                <code>AudioParamDescriptor</code> で定義できるプロパティは, <code>name</code>, <code>defaultValue</code>, <code>minValue</code>,
                <code>maxValue</code>, <code>automationRate</code> の 5 つで, このなかで, <b><code>name</code> プロパティのみは必須で定義する必要があります</b>.
                これは, メインスレッドで <b><code>AudioParamMap</code></b> インスタンスである <code>AudioWorkletNode</code> の
                <b><code>parameters</code></b> プロパティから, キーとして対象の <code>AudioParam</code> (<code>AudioParamDescriptor</code>)
                を取得するのに必要となるからです. それ以外のプロパティについてはデフォルト値が設定されています (<code>defaultValue</code> は <code>0</code>,
                <code>minValue</code> と <code>maxValue</code> がそれぞれ, <code>-3.4028235e38</code> と <code>3.4028235e38</code>,
                <code>automationRate</code> は <code>&apos;a-rate&apos;</code> です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/a-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;a-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = (2 * Math.random()) - 1;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                <code>parameterDescriptors</code> メソッドを実装すると, <code>process</code> メソッドの第 3 引数が渡されるようになります.
                <code>name</code> で指定した文字列をプロパティにして, パラメータの <code>Float32Array</code> を取得します. <code>automation</code> が
                <code>&apos;a-rate&apos;</code> の場合, <code>128</code> サンプル (render quantum size) ごとに異なる値が格納されているので,
                インデックスを走査します. <code>&apos;k-rate&apos;</code> の場合, <code>128</code> サンプル (render quantum size) ごとに固定値なので,
                <code>Float32Array</code> のインデックス <code>0</code> の値を利用します.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/a-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;a-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    const gain = parameters.noiseGain;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = ((gain.length &gt; 1) ? gain[n] : gain[0]) * ((2 * Math.random()) - 1);
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p><code>automationRate</code> が &apos;k-rate&apos; の場合,</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/k-rate.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  static get parameterDescriptors() {
    return [{
      name          : &apos;noiseGain&apos;,
      defaultValue  : 1,
      minValue      : 0,
      maxValue      : 1,
      automationRate: &apos;k-rate&apos;
    }];
  }

  constructor() {
    super();
  }

  process(inputs, outputs, parameters) {
    const output = outputs[0];

    const gain = parameters.noiseGain;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = gain[0] * ((2 * Math.random()) - 1);
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
              <p>
                メインスレッドで, 定義した <code>AudioParamDescriptor</code> を <code>AudioParam</code> インスタンスとして取得するには,
                <code>parameters</code> プロパティで <code>AudioParamMap</code> を取得して, <code>name</code> プロパティをキーにして取得します. これは
                <code>AudioParam</code> インスタンスなので,
                <a href="#section-audio-param-scheduling">オートメーションメソッド</a>を利用することでパラメータをスケジューリングすることが可能になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/a-rate.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/a-rate.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    const audioParamMap = processor.parameters;
    const noiseGain = audioParamMap.get(&apos;noiseGain&apos;);

    // do something for scheduling parameter ...
    const t0 = context.currentTime;
    const t1 = t0 + 2.5;

    noiseGain.setValueAtTime(0, t0);
    noiseGain.linearRampToValueAtTime(1, t1);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            </section>
          </section>
        </section>
        <section id="section-audio-worklet-add-module">
          <h3>AudioWorklet</h3>
          <p>
            すでに, サンプルコードでは利用していますが, <code>AudioContext</code> には <b><code>audioWorklet</code></b> プロパティが定義されており, これは
            (狭義の) <b><code>AudioWorklet</code></b> インスタンスです. 責務としては, <b><code>addModule</code></b> メソッドを呼び出して,
            <code>AudioWorkletProcessor</code> のサブクラスを定義したスクリプト (Worklet ファイル) をロードすることです.
          </p>
          <p>
            <code>addModule</code> メソッドは, Web Audio API に依存した仕様ではなく, JavaScript で使える Worklet (例えば, <code>CSS.paintWorklet</code>) が,
            指定したスクリプト (Worklet ファイル) をロードするために定義されています. <code>addModule</code> メソッドの第 1 引数は スクリプト (Worklet ファイル)
            の URL 文字列で必須です. また, 第 2 引数は任意で, <code>Fetch API</code> の
            <a href="https://fetch.spec.whatwg.org/#requests" target="_blank" rel="noopener noreferrer">Request</a> の
            <a href="https://fetch.spec.whatwg.org/#concept-request-credentials-mode" target="_blank" rel="noopener noreferrer">credentials mode</a>
            のオブジェクトを指定できます. 戻り値は <code>Promise</code> です. 成功時のコールバック関数の引数はありません.
          </p>
        </section>
        <section id="section-audio-worklet-examples">
          <h3>AudioWorklet によるオーディオ信号処理</h3>
          <p>
            <code>AudioWorklet</code> は複数の API で構成されているので, 抽象的な解説だけだと理解が難しいかもしれません. このセクションでは,
            <code>AudioWorklet</code> のユースケースとして想定されるオーディオ信号処理の実装例を紹介して理解のサポートになることを目指します.
          </p>
          <section id="section-audio-worklet-examples-noise-generator">
            <h4>ノイズ生成</h4>
            <p>
              すでに, 解説のサンプルコードとして記載していますが, Web Audio API でホワイトノイズやピンクノイズを生成する場合,
              <code>AudioWorklet</code> を利用する必要があります.
            </p>
            <p>
              ノイズの生成は, 乱数生成がベースになっています. 特に, ホワイトノイズは乱数そのままであり (振幅の調整だけしている),
              その振幅スペクトルはすべての周波数成分を一様に含んでいます (ノイズ生成の詳細に関しては,
              <a href="https://noisehack.com/generate-noise-web-audio-api/" target="_blank" rel="noopener noreferrer"
                >How to Generate Noise with the Web Audio API</a>
              を参考にしてください).
            </p>
            <p>
              実際に, Web アプリケーションとして実装する場合, ユーザーインタラクティブな操作によって, 発音・停止をさせるケースが多くなるでしょう. そのために,
              <code>AudioWorkletProcessor</code> を継承したサブクラスで, 発音・停止のフラグを実装しておき, メインスレッドからの
              <code>postMessage</code> で切り替えるようにします. すでに解説しましたが, <code>process</code> メソッドで, <code>false</code> を返してしまうと,
              その <code>AudioWorkletProcessor</code> は破棄されるような仕様になっているので, 常に <code>true</code> を返し, 停止中の場合, 出力となる
              <code>Float32Array</code> にデータを格納しないという実装で停止を実装しています.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;select&gt;
  &lt;option value=&quot;whitenoise&quot; selected&gt;White Noise&lt;/option&gt;
  &lt;option value=&quot;pinknoise&quot;&gt;Pink Noise&lt;/option&gt;
  &lt;option value=&quot;browniannoise&quot;&gt;Brownian Noise&lt;/option&gt;
&lt;/select&gt;
</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/noise-generator.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/noise-generator.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;NoiseGeneratorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mousedown&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: true });

      event.currentTarget.textContent = &apos;stop&apos;;
    });

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mouseup&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: false });

      event.currentTarget.textContent = &apos;start&apos;;
    });

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/noise-generator.js&apos;

class NoiseGeneratorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.processing = false;

    this.type = &apos;whitenoise&apos;;

    this.b0 = 0;
    this.b1 = 0;
    this.b2 = 0;
    this.b3 = 0;
    this.b4 = 0;
    this.b5 = 0;
    this.b6 = 0;

    this.lastOut = 0;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.processing === &apos;boolean&apos;) {
        this.processing = event.data.processing;
      }

      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    if (!this.processing) {
      return true;
    }

    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    switch (this.type) {
      case &apos;whitenoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            output[channelNumber][n] = (2 * Math.random()) - 1;
          }
        }

        break;
      }

      case &apos;pinknoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            this.b0 = (0.99886 * this.b0) + (white * 0.0555179);
            this.b1 = (0.99332 * this.b1) + (white * 0.0750759);
            this.b2 = (0.96900 * this.b2) + (white * 0.1538520);
            this.b3 = (0.86650 * this.b3) + (white * 0.3104856);
            this.b4 = (0.55000 * this.b4) + (white * 0.5329522);
            this.b5 = (-0.7616 * this.b5) - (white * 0.0168980);

            output[channelNumber][n] = this.b0 + this.b1 + this.b2 + this.b3 + this.b4 + this.b5 + this.b6 + (white * 0.5362);
            output[channelNumber][n] *= 0.11;

            this.b6 = white * 0.115926;
          }
        }

        break;
      }

      case &apos;browniannoise&apos;: {
        for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
          const bufferSize = output[channelNumber].length;

          for (let n = 0; n &lt; bufferSize; n++) {
            const white = (2 * Math.random()) - 1;

            output[channelNumber][n] = (this.lastOut + (0.02 * white)) / 1.02;

            this.lastOut = output[channelNumber][n];

            output[channelNumber][n] *= 3.5;
          }
        }

        break;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGeneratorProcessor&apos;, NoiseGeneratorProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-oscillator">
            <h4>基本波形生成</h4>
            <p>
              基本波形の生成は, <code>OscillatorNode</code> を利用すれば可能ですが, <code>OscillatorNode</code> による波形生成は, いわゆる,
              <b>フーリエ級数展開</b>にもとづいた波形生成, つまり, 周波数の異なる sin 波の合成によって, 矩形波やノコギリ波, 三角波が生成されています
              (その証拠として, <a href="#svg-oscillator">波形</a>を確認すると <b>Gibbs の現象</b>が発生していることが確認できます).
            </p>
            <p>実は, 基本波形は, 直線を組み合わせることによって (近似した波形を) 生成することも可能です (その場合, Gibbs の現象は発生しなくなります).</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;select&gt;
  &lt;option value=&quot;sine&quot; selected&gt;Sine&lt;/option&gt;
  &lt;option value=&quot;square&quot;&gt;Square&lt;/option&gt;
  &lt;option value=&quot;sawtooth&quot;&gt;Sawtooth&lt;/option&gt;
  &lt;option value=&quot;triangle&quot;&gt;Triangle&lt;/option&gt;
&lt;/select&gt;
&lt;label for=&quot;range-frequency&quot;&gt;frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-frequency&quot; value=&quot;440&quot; min=&quot;20&quot; max=&quot;8000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-frequency-value&quot;&gt;440 Hz&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/oscillator.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/oscillator.js&apos;)
  .then(() =&gt; {
    const processor = new AudioWorkletNode(context, &apos;OscillatorProcessor&apos;);

    // AudioWorkletNode (Input) -&gt; AudioDestinationNode (Output)
    processor.connect(context.destination);

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mousedown&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: true });

      event.currentTarget.textContent = &apos;stop&apos;;
    });

    document.querySelector(&apos;button[type=&quot;button&quot;]&apos;).addEventListener(&apos;mouseup&apos;, (event) =&gt; {
      processor.port.postMessage({ processing: false });

      event.currentTarget.textContent = &apos;start&apos;;
    });

    document.querySelector(&apos;select&apos;).addEventListener(&apos;change&apos;, (event) =&gt; {
      processor.port.postMessage({ type: event.currentTarget.value });
    });

    document.querySelector(&apos;input[type=&quot;range&quot;]&apos;).addEventListener(&apos;input&apos;, (event) =&gt; {
      processor.port.postMessage({ frequency: event.currentTarget.valueAsNumber });

      document.getElementById(&apos;print-frequency-value&apos;).textContent = `${event.currentTarget.value} Hz`;
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/oscillator.js&apos;

class OscillatorProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.processing = false;

    this.numberOfProcessedSamples = 0;

    this.type      = &apos;sine&apos;;
    this.frequency = 440;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.processing === &apos;boolean&apos;) {
        this.processing = event.data.processing;
      }

      if ((typeof event.data.frequency === &apos;number&apos;) &amp;&amp; (event.data.frequency &gt; 0)) {
        this.frequency = event.data.frequency;
      }

      if (event.data.type) {
        this.type = event.data.type;
      }
    };
  }

  process(inputs, outputs, parameters) {
    if (!this.processing) {
      return true;
    }

    // channel number is 0, 1, 2 ...
    // `output` is `[Float32Array, Float32Array, Float32Array ...]`
    const output = outputs[0];

    const numberOfSamplesPer1Hz = sampleRate / this.frequency;

    for (let channelNumber = 0, numberOfChannels = output.length; channelNumber &lt; numberOfChannels; channelNumber++) {
      const bufferSize = output[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        switch (this.type) {
          case &apos;sine&apos;: {
            output[channelNumber][n] = Math.sin((2 * Math.PI * this.frequency * this.numberOfProcessedSamples) / sampleRate);
            break;
          }

          case &apos;square&apos;: {
            output[channelNumber][n] = (this.numberOfProcessedSamples &lt; (numberOfSamplesPer1Hz / 2)) ? 1 : -1;
            break;
          }

          case &apos;sawtooth&apos;: {
            const a = (2 * this.numberOfProcessedSamples) / numberOfSamplesPer1Hz;

            output[channelNumber][n] = a - 1;
            break;
          }

          case &apos;triangle&apos;: {
            const a = (4 * this.numberOfProcessedSamples) / numberOfSamplesPer1Hz;

            output[channelNumber][n] = (this.numberOfProcessedSamples &lt; (numberOfSamplesPer1Hz / 2)) ? (-1 + a) : (3 - a);
            break;
          }
        }

        if (++this.numberOfProcessedSamples &gt;= numberOfSamplesPer1Hz) {
          this.numberOfProcessedSamples = 0;
        }
      }
    }

    return true;
  }
}

registerProcessor(&apos;OscillatorProcessor&apos;, OscillatorProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-reverse-channel">
            <h4>リバースチャンネル</h4>
            <p>
              左チャンネルからのサウンド出力と右チャンネルからのサウンド出力を反転する単純なエフェクターです (ただし, その原理から,
              左右チャンネルのサウンドデータが異なる場合にしか効果がありません). 一般的には, オーディオデータに対して適用するので, オーディオデータの準備として
              <code>AudioBufferSourceNode</code> か <code>MediaElementAudioSourceNode</code> を入力ノードとして,
              <code>AudioWorkletNode</code> に接続しておきます.
            </p>
            <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;div&gt;
  &lt;input type=&quot;file&quot; /&gt;
  &lt;label&gt;Reverse &lt;input type=&quot;checkbox&quot; /&gt;&lt;/label&gt;
&lt;/div&gt;
&lt;audio controls /&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/channel-reverser.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/channel-reverser.js&apos;)
  .then(() =&gt; {
    const reverser = new AudioWorkletNode(context, &apos;ChannelReverserProcessor&apos;);

    const inputElement    = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
    const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);
    const audioElement    = document.querySelector(&apos;audio&apos;);

    let source = null;

    inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
      const file = event.currentTarget.files[0];

      audioElement.src = window.URL.createObjectURL(file);
    });

    checkboxElement.addEventListener(&apos;click&apos;, (event) =&gt; {
      reverser.port.postMessage({ reversing: event.currentTarget.checked });
    });

    audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
      if (source === null) {
        source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
      }

      // MediaElementAudioSourceNode (Input) -&gt; AudioWorkletNode (Channel Reverser) -&gt; AudioDestinationNode (Output)
      source.connect(reverser);
      reverser.connect(context.destination);
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/channel-reverser.js&apos;

class ChannelReverserProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.reversing = false;

    this.port.onmessage = (event) =&gt; {
      if (typeof event.data.reversing === &apos;boolean&apos;) {
        this.reversing = event.data.reversing;
      }
    };
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    if ((input.length === 0) || (output.length === 0)) {
      return true;
    }

    if ((input.length !== 2) || (output.length !== 2)) {
      output[0].set(input[0]);
      return true;
    }

    if (this.reversing) {
      output[0].set(input[1]);
      output[1].set(input[0]);
    } else {
      output[0].set(input[0]);
      output[1].set(input[1]);
    }

    return true;
  }
}

registerProcessor(&apos;ChannelReverserProcessor&apos;, ChannelReverserProcessor);</code></pre>
          </section>
          <section id="section-audio-worklet-examples-vocal-canceler">
            <h4>ボーカルキャンセラ</h4>
            <p>
              一般的に, 音楽のオーディオデータはステレオで, 臨場感あるサウンドになるように, 左チャンネルと右チャンネルの音の大きさを調整しています.
              ボーカルの聴こえる位置は中央になるように左右のチャンネルを同じ音の大きさで録音し, ギターなどのボーカル以外の楽器音は,
              左右どちらかの音の大きさを大きくして, 聴こえる位置が左右のどちらかになるように録音されています. このように録音されたオーディオデータであれば,
              左右のチャンネルのサウンドデータの差分をとることにより, 左右均等に録音されているボーカルの音を取り除くことが可能になります. これが,
              ボーカルキャンセラです (ただし, その原理から, ドラムなど中央に位置する楽器音も取り除かれてしまいます).
            </p>
            <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;div&gt;
  &lt;input type=&quot;file&quot; /&gt;
  &lt;label&gt;Depth &lt;input type=&quot;range&quot; id=&quot;range-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;&lt;label&gt;
&lt;/div&gt;
&lt;audio controls /&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/vocal-canceler.js&apos; is URL that has subclass that extends `AudioWorkletProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/vocal-canceler.js&apos;)
  .then(() =&gt; {
    const canceler = new AudioWorkletNode(context, &apos;VocalCancelerProcessor&apos;);

    const inputElement = document.querySelector(&apos;input[type=&quot;file&quot;]&apos;);
    const rangeElement = document.querySelector(&apos;input[type=&quot;range&quot;]&apos;);
    const audioElement = document.querySelector(&apos;audio&apos;);

    let source = null;

    inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
      const file = event.currentTarget.files[0];

      audioElement.src = window.URL.createObjectURL(file);
    });

    rangeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      canceler.port.postMessage({ depth: event.currentTarget.valueAsNumber });
    });

    audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
      if (source === null) {
        source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });
      }

      // MediaElementAudioSourceNode (Input) -&gt; AudioWorkletNode (Vocal Canceler) -&gt; AudioDestinationNode (Output)
      source.connect(canceler);
      canceler.connect(context.destination);
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/vocal-canceler.js&apos;

class VocalCancelerProcessor extends AudioWorkletProcessor {
  constructor() {
    super();

    this.depth = 0;

    this.port.onmessage = (event) =&gt; {
      if ((typeof event.data.depth === &apos;number&apos;) &amp;&amp; (event.data.depth &gt;= 0)) {
        this.depth = event.data.depth;
      }
    };
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    if ((input.length === 0) || (output.length === 0)) {
      return true;
    }

    if ((input.length !== 2) || (output.length !== 2)) {
      output[0].set(input[0]);

      return true;
    }

    const bufferSize = input[0].length;

    for (let n = 0; n &lt; bufferSize; n++) {
      output[0][n] = input[0][n] - (this.depth * input[1][n]);
      output[1][n] = input[1][n] - (this.depth * input[0][n]);
    }

    return true;
  }
}

registerProcessor(&apos;VocalCancelerProcessor&apos;, VocalCancelerProcessor);</code></pre>
          </section>
        </section>
        <article id="section-script-processor-node">
          <h3>ScriptProcessorNode</h3>
          <p>
            <code>AudioWorklet</code> は初期の Web Audio API の仕様定義にはなく, <code>ScriptProcessorNode</code> が仕様定義されていました.
            <b>メインスレッド</b>で実行される<b>イベントハンドラ</b>によって, サウンド入出力する以外は <code>AudioWorklet</code> と考え方は同じです. しかし,
            メインスレッドで実行されるので, 不自然な音切れ (いわゆる, プチプチ音) が発生する<b>グリッチ</b> (<b>glitch</b>) や UI
            がスムーズに動作しなくなる現象である<b>ジャンク</b> (<b>jank</b>), イベントハンドラで実行されることによる<b>遅延</b> (<b>latency</b>) など API
            自体の根本的な問題がありました.
          </p>
          <p>
            Web Audio API の歴史的には, <code>ScriptProcessorNode</code> の問題を API 設計から解決するために, 後発的に仕様策定されて実装されているのが
            <code>AudioWorklet</code> です. <code>ScriptProcessorNode</code> は将来的に仕様からも削除される予定なので, 新規に実装するのであればわざわざ
            <code>ScriptProcessorNode</code> を利用する必要はありませんが, 既存のコードを読む必要があるかもしれないので,
            正弦波とホワイトノイズをミックスするサンプルコードを記載します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const processor  = context.createScriptProcessor(0, 2, 2);

oscillator.connect(processor);
processor.connect(context.destination);

oscillator.start(0);

const bufferSize = processor.bufferSize;

processor.onaudioprocess = (event) =&gt; {
  const inputLs  = event.inputBuffer.getChannelData(0);
  const inputRs  = event.inputBuffer.getChannelData(1);
  const outputLs = event.outputBuffer.getChannelData(0);
  const outputRs = event.outputBuffer.getChannelData(1);

  for (let n = 0; n &lt; bufferSize; n++) {
    outputLs[n] = (0.5 * inputLs[n]) + (0.25 * ((2 * Math.random()) - 1));
    outputRs[n] = (0.5 * inputRs[n]) + (0.25 * ((2 * Math.random()) - 1));
  }
};
</code></pre>
        </article>
      </section>
      <section id="section-scheduling">
        <h2>スケジューリング</h2>
        <section id="section-audio-context-current-time">
          <h3>AudioContext の currentTime プロパティ</h3>
          <p>
            Web Audio API におけるスケジューリング (<code>AudioScheduledSourceNode</code> の <code>start</code> / <code>stop</code> メソッドのスケジューリングや
            <code>AudioParam</code> のスケジューリング) において, 基本となる時間が
            <b><code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ</b>です. <code>currentTime</code> プロパティには,
            <b><code>AudioContextState</code> が <code>&apos;running&apos;</code> である状態での経過時間が秒単位で格納されています</b> (参照するだけの
            <code>readonly</code> プロパティです). <code>OscillatorNode</code> や <code>AudioBufferSourceNode</code> を即時に発音・停止するために,
            <code>start</code> / <code>stop</code> メソッドの第 1 引数に <code>0</code> を指定していましたが, 即時に発音・停止するであれば
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティを指定することでも可能です
            (即時に発音・停止するのは頻繁にあることなので, デフォルト値 <code>0</code> で即時に発音・停止するように仕様定義されています).
          </p>
          <p>
            つまり, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティを基準に, 未来の時間を指定すれば (加算すれば),
            スケジューリングが可能になるということです.
          </p>
          <p>
            仕様上の詳細を解説をすると, <code>OscillatorNode</code> や <code>AudioBufferSourceNode</code> は <code>AudioNode</code> クラスを継承した
            <code>AudioScheduledSourceNode</code> クラスを継承しており, <code>start</code> / <code>stop</code> メソッドはこのクラスに定義されているメソッドです
          </p>
          <article id="section-audio-context-current-time-definition">
            <h4>currentTime プロパティの時間の定義</h4>
            <p>
              Web Audio API が仕様策定された初期のころは, Autoplay Policy の制約もなく, <code>AudioContextState</code> 型もなかったので,
              <code>currentTime</code> プロパティには, <code>AudioContext</code> インスタンスが生成されてからの経過時間が秒単位で格納されていました. そのあと
              (2017 年ごろから), モダンブラウザ全般で, Autoplay Policy の導入が始まったことにより, <code>AudioContextState</code> 型も仕様に追加されて,
              <code>currentTime</code> プロパティには, <code>AudioContextState</code> が
              <code>&apos;running&apos;</code> である状態での経過時間が秒単位で格納されるように仕様も変わりました.
            </p>
          </article>
          <article id="section-javascript-time">
            <h4>JavaScript における時間</h4>
            <dl>
              <dt><code>Date.now</code></dt>
              <dd>
                UNIX 時間 (タイムスタンプとも呼ばれます). 1970 年 1 月 1 日 00 : 00 からの経過時間をミリ秒単位で表します. 音楽のような,
                時間管理の精度が高く要求されるようなユースケースでは不向きと言えます.
              </dd>
              <dt><code>performance.now</code></dt>
              <dd>
                <code>DOMHighResTimeStamp</code> 型の時間 (<code>64 bit</code> 浮動小数点数なので, 実体は <code>number</code> 型です) を表現します.
                詳細な仕様は他のドキュメントを参考にするほうがよいですが (実行コンテキストによって計測時刻が異なるので), ざっくり言うと, 対象の Web
                ページにアクセスしてからの経過時間をミリ秒単位で表します. hls.js など動画ストリーミングライブラリでは使われているなど (例
                <a href="https://github.com/video-dev/hls.js/blob/master/src/controller/abr-controller.ts#L179" target="_blank" rel="noopener noreferrer"
                  >ABR: Adaptive BitRate streaming</a>), <code>Date.now</code> と比較すると, 精度の高い時間と言えます.
              </dd>
            </dl>
          </article>
        </section>
        <section id="section-oscillator-node-scheduling">
          <h3>OscillatorNode のスケジューリング</h3>
          <p>
            <code>OscillatorNode</code> のスケジューリングを設定することで, 和音をアルペジオ (分散和音) のように発音・停止するサンプルコードにしてみました.
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティの値を基準に, スケジュールしたい時間を加算した値を引数に渡すことで,
            <code>AudioContext</code> インスタンスが生成されてからの経過時間が指定した時間に達すると, 発音・停止します.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let oscillatorC = null;
let oscillatorE = null;
let oscillatorG = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillatorC !== null) || (oscillatorE !== null) || (oscillatorG !== null)) {
    return;
  }

  oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
  oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
  oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

  const gain = new GainNode(context, { gain: 0.25 });

  // OscillatorNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  oscillatorC.connect(gain);
  oscillatorE.connect(gain);
  oscillatorG.connect(gain);
  gain.connect(context.destination);

  // Schedule oscillator start
  oscillatorC.start(context.currentTime + 0.0);
  oscillatorE.start(context.currentTime + 0.1);
  oscillatorG.start(context.currentTime + 0.2);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillatorC === null) || (oscillatorE === null) || (oscillatorG === null)) {
    return;
  }

  // Schedule oscillator stop
  oscillatorC.stop(context.currentTime + 0.0);
  oscillatorE.stop(context.currentTime + 0.1);
  oscillatorG.stop(context.currentTime + 0.2);

  // GC (Garbage Collection)
  oscillatorC = null;
  oscillatorE = null;
  oscillatorG = null;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
        </section>
        <section id="section-audio-buffer-source-node-scheduling">
          <h3>AudioBufferSourceNode のスケジューリング</h3>
          <p>
            <code>OscillatorNode</code> の場合と同様に, スケジューリングを設定することで, 和音をアルペジオのように発音・停止するサンプルコードにしてみました.
            <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティの値を基準に, スケジュールしたい時間を加算した値を引数に渡すことで,
            <code>AudioContext</code> インスタンスが生成されてからの経過時間が指定した時間に達すると, 発音・停止します. 1 点異なるのは,
            <b><code>AudioBufferSourceNode</code> の <code>start</code> メソッドはオーバライドされています</b>. もし,
            オーディオデータの再生開始位置を指定したい場合は, 第 2 引数に再生開始位置を秒単位で指定, 再生時間を指定する場合は, 第 3 引数に秒単位で指定します (<b
              ><a href="#section-audio-buffer-source-node-playback-rate-and-detune">再生速度</a>を変更している場合でも影響は受けない</b>ので, 第 2 引数, 第 3 引数は再生速度を <code>1</code> とした場合の値を指定します). どちらも, 任意の引数なので不要であれば省略可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
let sourceC = null;
let sourceE = null;
let sourceG = null;

let buffer = null;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if (buffer === null) {
    return;
  }

  sourceC = new AudioBufferSourceNode(context, { buffer });
  sourceE = new AudioBufferSourceNode(context, { buffer });
  sourceG = new AudioBufferSourceNode(context, { buffer });

  sourceC.detune.value = 0;
  sourceE.detune.value = 400;
  sourceG.detune.value = 700;

  const gain = new GainNode(context, { gain: 0.25 });

  // AudioBufferSourceNode (Input) -&gt; GainNode -&gt; AudioDestinationNode (Output)
  sourceC.connect(gain);
  sourceE.connect(gain);
  sourceG.connect(gain);

  gain.connect(context.destination);

  // Schedule one-shot audio start
  sourceC.start((context.currentTime + 0.0), 0, sourceC.buffer.duration);
  sourceE.start((context.currentTime + 0.1), 0, sourceE.buffer.duration);
  sourceG.start((context.currentTime + 0.2), 0, sourceG.buffer.duration);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((buffer === null) || (sourceC === null) || (sourceE === null) || (sourceG === null)) {
    return;
  }

  // Schedule one-shot audio stop
  sourceC.stop(context.currentTime + 0.0);
  sourceE.stop(context.currentTime + 0.1);
  sourceG.stop(context.currentTime + 0.2);

  buttonElement.textContent = &apos;start&apos;;
});

fetch(&apos;./assets/one-shots/piano-C.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
        </section>
        <section id="section-audio-param-scheduling">
          <h3>AudioParam のスケジューリング (パラメータのオートメーション)</h3>
          <p>
            <code>AudioParam</code> には, パラメータをスケジュールするための, <b>パラメータのオートメーションメソッド</b>が仕様定義されています.
            このセクションでは, その最適なユースケースと言える, エンペロープジェネレーターを例にそれらを解説します.
          </p>
          <p><code>AudioParam</code> で定義されている, パラメータのオートメーションメソッドを以下のリストに記載します.</p>
          <section id="section-audio-param-set-value-at-time">
            <h4>setValueAtTime(value, startTime)</h4>
            <p><code>startTime</code> にパラメータを <code>value</code> の値に設定する</p>
          </section>
          <section id="section-audio-param-linear-ramp-to-value-at-time">
            <h4>linearRampToValueAtTime(value, endTime)</h4>
            <p><code>endTime</code> にパラメータが <code>value</code> の値になるように線形的に (直線的に) 変化させる</p>
          </section>
          <section id="section-audio-param-exponential-ramp-to-value-at-time">
            <h4>exponentialRampToValueAtTime(value, endTime)</h4>
            <p><code>endTime</code> にパラメータが <code>value</code> の値になるように指数関数的に変化させる</p>
          </section>
          <section id="section-audio-param-set-target-at-time">
            <h4>setTargetAtTime(target, startTime, timeConstant)</h4>
            <p>
              <code>startTime</code> になったら, パラメータを <code>target</code> の値に向けて, <code>timeConstant</code> の時間をかけて変化させる
              (より正確には, パラメータの現在の値と <code>target</code> の値の差分の約 <code>63.2%</code> (<span class="math-inline"
                >$1 - \frac{1}{\mathrm{exp}} = 0.632120 \cdots$</span>) まで変化するのに, <code>timeConstant</code>の時間を要する)
            </p>
          </section>
          <section id="section-audio-param-set-value-curve-at-time">
            <h4>setValueCurveAtTime(values, startTime, duration)</h4>
            <p>
              <code>startTime</code> になったら, <code>Float32Array</code> の <code>values</code> の値にしたがって,
              <code>duration</code> の時間をかけて変化させる
            </p>
          </section>
          <section id="section-audio-param-cancel-scheduled-values">
            <h4>cancelScheduledValues(cancelTime)</h4>
            <p><code>cancelTime</code> 以降のスケジューリングを解除する</p>
          </section>
          <section id="section-audio-param-cancel-and-hold-at-time">
            <h4>cancelAndHoldAtTime(cancelTime)</h4>
            <p><code>cancelTime</code> 以降のスケジューリングを解除する (<code>cancelTime</code> 時点の値を保持する)</p>
          </section>
        </section>
        <section id="section-envelope-generator">
          <h3>エンベロープジェネレーター</h3>
          <section id="section-envelope">
            <h4>エンベロープとは ?</h4>
            <p>
              <b>エンベロープ</b>とは, 波形の概形のことです. テキストによる解説よりは, イラストによる解説が一目瞭然なので, 例として以下のような波形で説明します.
            </p>
            <figure>
              <svg id="svg-figure-career" width="720" height="405" data-parameters="false" data-a="1" data-f="1" />
              <figcaption>時間領域の波形</figcaption>
            </figure>
            <figure>
              <svg id="svg-figure-envelope" width="720" height="405" data-parameters="false" data-a="1" data-f="1" />
              <figcaption>振幅エンベロープ</figcaption>
            </figure>
            <p>
              上記のエンベロープは, 振幅に対する波形の概形なので, <b>振幅エンベロープ</b>と呼びます. 振幅エンベロープを,
              時間的に制御するオーディオ処理を<b>エンベロープジェネレーター</b>と呼びます.
            </p>
            <p>
              エンベロープジェネレーターの実装において, パラメータのスケジュールの対象になるの <code>GainNode</code> インスタンスの
              <code>gain</code> プロパティです. <code>gain</code> プロパティは, <code>AudioParam</code> インスタンスであり,
              <code>AudioParam</code> で仕様定義されているパラメータのオートメーションメソッドを利用することで, パラメータをスケジュールすることが可能です
              (同様に, <code>OscillatorNode</code> インスタンスの <code>frequency</code> プロパティや, <code>DelayNode</code> インスタンスの
              <code>delayTime</code> プロパティ, <code>BiquadFilterNode</code> の <code>frequency</code> プロパティ ... など,
              <code>AudioParam</code> インスタンスであれば利用可能です. したがって, パラメータのオートメーションメソッドを理解することで,
              さまざまなエフェクトが試行錯誤可能になります).
            </p>
          </section>
          <section id="section-envelope-generator-initialization">
            <h4>gain プロパティの初期化</h4>
            <p>
              まず, 初期化処理として, <a href="#section-audio-param-set-value-at-time"><code>setValueAtTime</code></a> メソッドを実行します. 初期値として, 値を
              <code>0</code> に, また即時に初期化するように, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ (<code>t0</code>)
              を指定します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;

  envelopegenerator.gain.setValueAtTime(0, t0);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
})</code></pre>
          </section>
          <section id="section-envelope-generator-attack">
            <h4>attack</h4>
            <p>
              <b>attack</b> (アタック) は, ゲインが最大値, すなわち, <code>1</code> になるまでに要する時間です. そこで, attack の実装には,
              <a href="#section-audio-param-linear-ramp-to-value-at-time"><code>linearRampToValueAtTime</code></a> メソッドを利用します (一般的には,
              線形的に変化させますが, 指数関数的に変化させたい場合,
              <a href="#section-audio-param-exponential-ramp-to-value-at-time"><code>exponentialRampToValueAtTime</code></a>
              メソッドを使ってみてもよいでしょう). 注意が必要なのは, 第 2 引数です. attack time の値をそのまま指定してしまうとうまくいきません. なぜなら,
              <b>時間ではなく時刻</b>を指定する必要があるからです. したがって, サウンド開始時刻 (変数 <code>t0</code>) に attack を加算した値 (変数
              <code>t1</code>) を第 2 引数に指定します.
            </p>
            <p>
              attack は, もう少しくだいて表現すれば, 音の立ち上がりの速さを決定するパラメータと言えます. 楽器で具体例をあげると,
              ピアノやギターは比較的音の立ち上がりが速い楽器で, バイオリンやフルートなどは比較的音の立ち上がりが遅い楽器です. すなわち,
              アタックを短くするとピアノやギターのように音の立ち上がりが速くなり, アタックを長くするとバイオリンやフルートのように音の立ち上がりが遅くなります.
              ちなみに, 音の立ち上がりが比較的速い (アタックが短い) エレキギターでは, バイオリン奏法と呼ばれる奏法があります. これは, ピッキング時に,
              ギターのボリュームを0にすることによって, ピッキングした瞬間の音 (アタック音) を消し, そのあとに, ボリュームを増加させるという奏法です.
              エレキギターであるのに, まるでバイオリンのような音色を奏でることができます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack = 0.01;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});</code></pre>
          </section>
          <section id="section-envelope-generator-decay-and-sustain">
            <h4>decay / sustain</h4>
            <p>
              <b>decay</b> (ディケイ) は, ゲインが最大値 <code>1</code> から <b>sustain</b> (サステイン) にまで減衰する時間です.
              <a href="#section-audio-param-set-target-at-time"><code>setTargetAtTime</code></a> メソッドを利用することで実装できます. 注意が必要なのは, 第 2
              引数と第 3 引数です. 第 2 引数にはパラメータが変化を開始する時刻を指定し, 第 3 引数にはパラメータが, 現在の値と第 1 引数で指定した値の差分 (の約
              <code>63.2%</code>) まで変化するのに要する時間を指定します. したがって, 第 2 引数は <code>gain</code> プロパティが <code>1</code> となる時刻
              (減衰開始時刻) である変数 <code>t1</code> を指定し, 第 3 引数は decay time (減衰時間) である変数 <code>t2</code>を指定します. そして, 第 1 引数は
              <code>gain</code> プロパティが収束する値である sustain level (持続レベル) を指定します.
            </p>
            <p>attack, decay, release は物理量が<b>時間</b>なのに対して, sustain のみ<code>ゲイン</code>なので注意してください.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});</code></pre>
          </section>
          <section id="section-envelope-generator-release">
            <h4>release</h4>
            <p>
              <b>release</b> (リリース) は, ゲインが sustain から最小値 <code>0</code> に変化するまでの時間です. decay / sustain と同じく,
              <a href="#section-audio-param-set-target-at-time"><code>setTargetAtTime</code></a> メソッドを利用することで実装できます.
              <code>gain</code> プロパティを <code>0</code> に近づけていくので, 第 1 引数には <code>0</code> を指定します. 第 2
              引数に指定するリリースの開始時刻は, <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティ値です. また, 第 3
              引数には変化に要する時間, すなわち, release time (リリースタイム) を指定します.
            </p>
            <p>
              ドラムのような音の余韻が短い楽器をシミュレートしたり, スタッカート (音を短く切って演奏する楽譜の記号) を実現したりする場合は release を短く, 逆に,
              ダンパーペダルを踏んだピアノの音や, フェルマータ (音を長く伸ばして演奏する楽譜の記号) を実現したりする場合は release を長くします.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
            <p>
              リリースを実装する場合は, <code>OscillatorNode</code> インスタンスの <code>stop</code> メソッドの即時実行は不要です. その理由は,
              <code>stop</code> メソッドを即時実行すると, その時点で音が停止してしまうので, 音に余韻が生まれません. といっても, このままでは,
              <code>start</code> メソッドの多重呼び出しになります. すなわち, <code>start</code> メソッドと
              <code>stop</code> メソッドは一対ということが順守できていません.
            </p>
            <p>
              そこで, タイマー処理で <code>gain</code> プロパティをチェックして, <b>停止とみなせる値</b>になれば, <code>stop</code> メソッドを実行します.
              ここで, 最小値である <code>0</code> と表現しなかったのは理由があります. 確かに, 理論上は, 停止とみなせる値は <code>0</code> ですが, 実装上では,
              (原因はわかりませんが) 半永久的に <code>0</code> にはなりません. したがって, 停止とみなせる値を <code>0.001</code> 未満と設定しています.
            </p>
            <p>また, 停止とみなせる値になる前に, 再度, <code>mousedown</code> した場合は, <code>OscillatorNode</code> を即時停止します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;
let intervalid = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    oscillator.stop(0);
    oscillator = null;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;

  intervalid = window.setInterval(() =&gt; {
    if (envelopegenerator.gain.value &gt;= 1e-3) {
      return;
    }

    // Stop sound (If use `OscillatorNode`)
    oscillator.stop(0);
    oscillator = null;

    if (intervalid !== null) {
      window.clearInterval(intervalid);
      intervalid = null;
    }
  }, 0);
});</code></pre>
            <p>
              これで, 完成しました ... と言いたいところですが, 1 つ問題点があります. もし, attack time もしくは decay time が経過する前に,
              <code>mouseup</code> イベントが発生するとどうなるでしょう ? attack, decay のゲイン変化のスケジューリングと,
              release　におけるゲイン変化のスケジューリングが混在してしまいますね. つまり, 上記のコードだと,
              意図したスケジューリングにならない可能性があるという問題点があります. これを解決するには,
              イベント発生時にスケジューリングをすべて解除すれば解決します. そして, スケジューリングの解除には,
              <a href="#section-audio-param-cancel-scheduled-values"><b>cancelScheduledValues</b></a>メソッド, もしくは, 値をそのまま保持しておきたい場合は,
              <a href="#section-audio-param-cancel-and-hold-at-time"><b>cancelAndHoldAtTime</b></a> メソッドを利用します.
            </p>
            <p>
              具体的には, <code>mouseup</code> 時は, 値を保持しておきたいので, <code>cancelAndHoldAtTime</code> メソッドでスケジューリングを解除します. また,
              ボタンが連打された場合に不要なスケジューリングが解除されるように, <code>mousedown</code> 時は,
              <code>cancelScheduledValues</code> メソッドでスケジューリングを解除します (そのあと <code>setValueAtTime</code> メソッドで
              <code>0</code> に初期化されるので値を保持する必要がないので).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const envelopegenerator = new GainNode(context);

const attack  = 0.01;
const decay   = 0.3;
const sustain = 0.5;
const release = 1.0;

let oscillator = null;
let intervalid = null;

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (oscillator !== null) {
    oscillator.stop(0);
    oscillator = null;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = sustain;

  envelopegenerator.gain.cancelScheduledValues(t0);
  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(t2Level, t1, t2);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (oscillator === null) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  envelopegenerator.gain.cancelAndHoldAtTime(t3);
  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  buttonElement.textContent = &apos;start&apos;;

  intervalid = window.setInterval(() =&gt; {
    if (envelopegenerator.gain.value &gt;= 1e-3) {
      return;
    }

    // Stop sound (If use `OscillatorNode`)
    oscillator.stop(0);
    oscillator = null;

    if (intervalid !== null) {
      window.clearInterval(intervalid);
      intervalid = null;
    }
  }, 0);
});</code></pre>
            <article id="section-audio-param-cancel-and-hold-at-time-if-firefox">
              <h5>Firefox での <code>cancelAndHoldAtTime</code> の実装状況とポリフィル</h5>
              <p>
                Firefox 125 の時点では, <code>cancelAndHoldAtTime</code> が実装されていません. しかしながら, <code>cancelScheduledValues</code> と
                <code>setValueAtTime</code> を使うことでポリフィルを実装することは可能です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">if (typeof envelopegenerator.gain.cancelAndHoldAtTime === &apos;function&apos;) {
  envelopegenerator.gain.cancelAndHoldAtTime(t3);
} else {
  const value = envelopegenerator.gain.value;

  envelopegenerator.gain.cancelScheduledValues(t3);
  envelopegenerator.gain.setValueAtTime(value, t3);
}</code></pre>
            </article>
          </section>
          <section id="section-apply-envelope-generator">
            <h4>エンベロープジェネレーターの応用</h4>
            <p>
              エンベロープジェネレーターの実装, すなわち, <code>gain</code> プロパティのスケジューリングは, オーディオソースに依存したことではないので,
              <code>AudioBufferSourceNode</code> をオーディオソースとして利用することで,
              ワンショットオーディオにもエンベロープジェネレーターを適用することが可能になります. また, attack と release を楽曲データの再生に適用することで,
              フェードイン・フェードアウトの実装も可能になります.
            </p>
          </section>
          <p>
            このセクションのまとめとして, エンベロープジェネレーターの制御となる <code>gain</code> プロパティの値を視覚化するデモとなります. attack, decay,
            sustain, release の値を変えてみて, <code>gain</code> プロパティの値の変化や, それにともなう音色の変化を体感してみてください.
          </p>
          <div class="app-container">
            <svg id="svg-envelopegenerator" class="svg-envelopegenerator" width="720" height="240"></svg>
            <div>
              <button type="button" id="button-envelopegenerator" class="button-envelopegenerator">start</button>
              <div class="ranges-envelopegenerator">
                <label><span>attack</span><input type="range" id="range-attack" value="0.01" min="0" max="1" step="0.01" /></label>
                <label><span>decay</span><input type="range" id="range-decay" value="0.3" min="0" max="1" step="0.01" /></label>
                <label><span>sustain</span><input type="range" id="range-sustain" value="0.5" min="0" max="1" step="0.01" /></label>
                <label><span>release</span><input type="range" id="range-release" value="1" min="0" max="1" step="0.01" /></label>
              </div>
            </div>
          </div>
        </section>
        <article id="section-audio-param-automation-rate">
          <h3><code>a-rate</code> と <code>k-rate</code> (<code>AutomationRate</code>)</h3>
          <p>
            <code>AudioParam</code> には, <b><code>automationRate</code></b> プロパティがあり, これは <b><code>&apos;a-rate&apos;</code></b> か
            <b><code>&apos;k-rate&apos;</code></b> の <code>AutomationRate</code> 型で列挙されるどちらかの値が設定されています.
            <code>&apos;a-rate&apos;</code> は, <b><code>1</code> サンプルごとに値を適用することができる <code>AudioParam</code> です</b>.
            <code>&apos;k-rate&apos;</code> は,
            <b><code>128</code> サンプル単位 (render quantum size) で値を適用することができる <code>AudioParam</code> です</b>. <code>AudioParam</code> ごとに,
            <code>AutomationRate</code> が仕様設定されているので, 重要度としては低くなりますが,
            <code>&apos;a-rate&apos;</code> のほうがパラメータを変化させるコストはやや高いぐらいに認識しておくとよいかもしれません (<a
              href="https://developer.mozilla.org/en-US/docs/Web/API/AudioWorkletProcessor/process#examples"
              target="_blank"
              rel="noopener noreferrer"
              >実装イメージ</a>. <code>&apos;k-rate&apos;</code> の場合, <code>128</code> サンプルのパラメータの <code>0</code> 番目だけ適用すればよいので最適化しやすい). また,
            <code>AudioWorkletProcessor</code> クラスで, <code>AudioParam</code> を定義する場合 (<code>parameterDescriptors</code> プロパティ),
            適切に選択する必要がある場合もあります (デフォルトは, <code>&apos;a-rate&apos;</code>).
          </p>
          <p>
            もっとも, <code>AudioParam</code> のほとんどは <code>&apos;a-rate&apos;</code> です. 現在の仕様では, 以下のリストにある <code>AudioParam</code> が
            <code>&apos;k-rate&apos;</code> です.
          </p>
          <dl>
            <dt>AudioBufferSourceNode</dt>
            <dd><code>playbackRate</code>, <code>detune</code></dd>
            <dt>DynamicsCompressorNode</dt>
            <dd><code>threshold</code>, <code>knee</code>, <code>ratio</code>, <code>attack</code>, <code>release</code></dd>
            <dt>PannerNode</dt>
            <dd><code>panningModel</code> が <code>&apos;HRTF&apos;</code> の場合, <code>&apos;k-rate&apos;</code> のようにふるまう</dd>
          </dl>
          <p>
            <code>128</code> サンプルというのは, Web Audio API における, オーディオ処理のバッファ単位です (仕様では,
            <b>render quantum size</b> という用語が使われています). 例えば, <code>AudioWorkletProcessor</code> では, <code>128</code> サンプルごとの入力に対して
            (必要があれば, オーディオ処理を適用して), <code>128</code> サンプル出力します. リアルタイム性が要求されるようなオーディオ API では, 多くは,
            このような, 仕様で定義されているバッファサイズごとにオーディオ処理を適用する (そして, それを繰り返す) という API になっています (<a
              href="#section-audio-worklet-processor-render-quantum-size"
              ><b>Web Audio API 1.1 以降では, 必ずしも <code>128</code> サンプルではないことに注意してください</b></a>).
          </p>
        </article>
        <section id="section-garbage-collection">
          <h3>Web Audio API におけるガベージコレクション</h3>
          <p>
            Web Audio API においてはこのセクションで解説したようなスケジューリングや,
            <code>DelayNode</code> などを利用した時に発生する遅延オーディオデータなどがあるので, JavaScript
            の仕様上のガベージコレクションの対象となるオブジェクトに追加して, いくつかの条件があります.
          </p>
          <ul>
            <li>参照が残っていない</li>
            <li>処理すべきサウンドデータが残っていない</li>
            <li>ノードが接続されていない</li>
            <li>サウンドが停止している</li>
            <li>スケジューリングが設定されていない</li>
          </ul>
          <p>
            上記 5 つの条件すべてを満たすオブジェクトが, ガベージコレクションの対象となります. ざっくり説明すれば,
            なにかしらで利用されているオブジェクトはガベージコレクションの対象にならないということです.
          </p>
          <section id="section-garbage-collection-no-reference-counting">
            <h4>参照が残っていない</h4>
            <p>これに関しては, Web Audio API に限らず, JavaScript, あるいは, ガベージコレクションが実装されているあらゆるプログラミング言語一般的なことです.</p>
          </section>
          <section id="section-garbage-collection-no-sound-data">
            <h4>処理すべきサウンドデータが残っていない</h4>
            <p>
              処理すべきサウンドデータが意図せずに残るケースとして, <code>DelayNode</code> や <code>ConvolverNode</code> を利用して,
              エフェクターであるディレイやリバーブを実装した場合が考えられますが, 実装的には対処する必要はありません. 処理すべきサウンドデータがある場合に,
              サウンドデータを完了状態にするのは, <code>DelayNode</code> や <code>ConvolverNode</code> の役割であるのと, そもそも,
              このような場合に処理が残っているサウンドデータを破棄するなどの手段が現状の仕様では存在しないからです.
            </p>
          </section>
          <section id="section-garbage-collection-no-connections">
            <h4>ノードが接続されていない</h4>
            <p>
              不要になった <code>AudioNode</code> インスタンスは, <b><code>disconnect</code></b> メソッドでノードの接続を解除しておくのが律儀ではありますが,
              参照を破棄することで, 同時にノードの接続も解除されるので, 明示的に実装する必要はありません. ちなみに,
              <code>disconnect</code> メソッドのユースケースとしては, 例えば,
              ユーザーインタラクティブな操作などで動的にノードの接続を解除する必要がある場合ぐらいです.
            </p>
            <p>以下のコードは, ノード接続状態のまま, 参照を破棄していますが, 同時にノード接続も解除されるのでメモリリークに陥ることはありません.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);
}, 10);</code></pre>
          </section>
          <section id="section-garbage-collection-stop-sound">
            <h4>サウンドが停止している</h4>
            <p>
              以下のコードは, サウンドが発音状態なので, ガベージコレクションが実行されず, メモリがしだいに不足していく例です. その理由は,
              コールバック関数実行のたびに, 以前のインスタンスへの参照は破棄されますが, それに対応するサウンドが停止していないからです.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  oscillator.start(0);
}, 10);</code></pre>
          </section>
          <section id="section-garbage-collection-no-schedulings">
            <h4>スケジューリングが設定されていない</h4>
            <p>
              以下のコードは, 参照を破棄して, サウンドを停止状態にしていますが, 時間が経過するほど,
              サウンドの開始が少しずつ遅延するようにサウンドスケジューリングしているので, ガベージコレクションの実行もそれにともなって遅れるので,
              メモリが不足していきます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;

let counter = 0;

window.setInterval(() =&gt; {
  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  const startTime = context.currentTime + counter;
  const stopTime  = startTime + 10;

  oscillator.start(startTime);
  oscillator.stop(stopTime);

  ++counter;
}, 10);</code></pre>
          </section>
        </section>
      </section>
      <section id="section-audio-signal-processing">
        <h2>デジタルオーディオ信号処理</h2>
        <section id="section-analog-to-digital-conversion">
          <h3>A/D 変換</h3>
          <p>
            アナログ信号である音 (媒体の振動) をコンピュータで処理するためには, <code>0</code> と <code>1</code> のみの情報, つまり,
            デジタル信号に変換する必要があります. この変換処理のことを, <b>A/D変換</b> (<b>Analog to Digital Conversion</b>) と呼びます. A/D 変換は, 大きく 3
            つの処理があります.
          </p>
          <ol>
            <li>サンプリング (標本化)</li>
            <li>量子化</li>
            <li>符号化</li>
          </ol>
          <p>
            サンプリング (標本化) と量子化の処理に共通することは, <b>連続した信号を離散した信号に変換することです</b>. コンピュータでは,
            <b>連続した値や無限大となる値を扱うことが不可能</b>だからです.
          </p>
          <p>
            <a href="#section-about-sound">「音」</a>のセクションでは, いくつか音の波形のイラストを記載しましたが, それらは常に 2 つの連続した物理量 (次元)
            をもっていました. <b>時間</b>と<b>振幅</b>です. サンプリング (標本化) と量子化は, これら 2 つの連続した物理量を離散信号に変換する処理となります.
          </p>
          <section id="section-analog-to-digital-conversion-sampling">
            <h4>サンプリング (標本化)</h4>
            <p>
              <b>サンプリング</b> (<b>標本化</b>)は, 時間を離散した値に変換する処理です. 離散信号, すなわち, とびとびの値をとっていくためには,
              その間隔を決定するパラメータが必要になります. それが, <b>サンプリング周期</b> (<b>標本化周期</b>) です. サンプリング周期の逆数となるパラメータは,
              <b>標本化周波数</b> (<b>サンプリング周波数</b>) です. 簡単に解説すれば, サンプリング周波数は, <code>1 sec</code> の間に, いくつのサンプル (離散点)
              をとるか ? ということを意味しています. 例えば, サンプリング周波数が <code>48000 Hz</code> の場合, <code>1 sec</code> の間に
              <code>48000</code> サンプル (離散点) をとることになります.
            </p>
            <figure>
              <svg id="svg-figure-sampling" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>
                <span>サンプリングとサンプリング周波数 (サンプリング周期)</span>
                <span>1 sec に 8 サンプルあるので, <code>8 Hz</code> (<code>0.125 sec</code>)</span>
              </figcaption>
            </figure>
            <p>
              サンプリング (標本化) では重要な定理があります. それは, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> 以上の周波数は元のアナログ信号に復元できないという定理です. この定理は,
              <b>サンプリング定理</b> (<b>標本化定理</b>, <b>シャノンの定理</b>) と呼ばれます. 逆の視点で表現すれば, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> より低い周波数は元のアナログ信号に復元可能ということです. また, サンプリング周波数の
              <span class="math-inline">$\frac{1}{2}$</span> は, <b>ナイキスト周波数</b>と呼ばれます. サンプリング定理から, 原信号に含まれる最大の周波数成分の
              <b>2 倍より大きい</b>サンプリング周波数に設定すれば, 元のアナログ信号に復元可能ということになります (実際には, 低域通過フィルタ (Low-Pass Filter)
              を利用して, 高い周波数成分を除去するプリプロセス処理を施します).
            </p>
            <p>
              サンプリング定理を満たさないサンプリング周波数, すなわち, 原信号に含まれる最大の周波数成分の 2 倍以下のサンプリング周波数でサンプリングすると,
              <b>折り返し歪み</b> (<b>エイリアス歪み</b>) が発生して, ノイズとして復元されてしまいます.
            </p>
            <p>
              例えば, <code>1 Hz</code> の信号に対して, 2 サンプル (サンプリング周波数 <code>2 Hz</code>, ナイキスト周波数 <code>1 Hz</code>)
              では原信号に復元できません.
            </p>
            <figure>
              <svg id="svg-figure-sampling-theorem-with-aliasing" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>
                <span>サンプリング定理 (折り返し歪みが発生する)</span>
              </figcaption>
            </figure>
            <p>
              <code>1 Hz</code> の信号に対して, 3 サンプル (サンプリング周波数 <code>3 hz</code>, ナイキスト周波数 <code>1.5 Hz</code>) だと, 精度は低いですが,
              原信号に復元できます
            </p>
            <figure>
              <svg
                id="svg-figure-sampling-theorem-without-aliasing"
                width="720"
                height="405"
                data-parameters="true"
                data-a="1"
                data-f="1"
                data-t="0.0,0.5,1.0"
              />
              <figcaption>サンプリング定理 (定理を満たす場合)</figcaption>
            </figure>
            <p>
              サンプリングの精度を高くするほど, すなわち,
              サンプリング周波数を高くするほど元のアナログ信号に対してより精度の高いデジタル信号に変換可能となります. 一方で,
              データサイズはサンプリング周波数に比例して大きくなってしまいます.
            </p>
            <p>
              以下の図は, 充分なサンプル数 (サンプリング周波数) だと原信号により精度高く復元できること表しています. そのトレードオフとして,
              サンプル数が多くなるので, データサイズはより大きくなることも表しています.
            </p>
            <figure>
              <svg id="svg-figure-sampling-theorem" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>サンプリング定理</figcaption>
            </figure>
            <p>
              サンプリング周波数の具体例として, 音楽 CD は <code>44.1 kHz</code> に設定されています. 人間の聴覚が知覚可能な周波数はおよそ
              <code>20 kHz</code>であることを考慮してサンプリング定理を適用しているからです. さらに音質の高いものだと
              <code>96 kHz</code> 以上に設定されている音楽データもあります (ハイレゾオーディオのサンプリング周波数). 電話では
              <code>8 kHz</code> に設定されています. 音声の場合は, 多少音質が損なわれても相手の音声を聴きとることが可能なこと,
              楽器音ほど高い周波数成分が含まれないこと, リアルタイムに通信するので可能な限りデータサイズを減らす必要があることなどが理由としてあげられます.
            </p>
            <p>
              Web Audio API では, <code>AudioContext</code> インスタンス生成時の引数として, <b><code>AudioContextOptions</code></b> の
              <b><code>sampleRate</code></b> プロパティで明示的に指定することが可能です. 明示的に指定しない場合, デバイスのサンプリング周波数
              (<code>44100</code>, <code>48000</code> など) に設定されています.
            </p>
            <p>
              サウンドの視覚化の実装では, サンプリング周波数 (<code>AudioContext</code> インスタンス, または, <code>AudioBuffer</code> インスタンスの
              <code>sampleRate</code> プロパティ) にアクセスすることはよくあります. したがって, サンプリング周波数が何を意味しているのか ? ということと,
              サンプリング定理に関して理解しておくと役に立つでしょう.
            </p>
          </section>
          <section id="section-analog-to-digital-conversion-quantization">
            <h4>量子化</h4>
            <p>
              <b>量子化</b>は, <b>振幅を離散した値に変換する処理です</b>. サンプリングと同じく, とびとびの値をとっていくためには,
              その間隔を決定づけるパラメータが必要になります. それが, <b>量子化ビット</b> (<b>量子化精度</b>) です.
            </p>
            <p>
              サンプリングされたアナログ信号は時間軸方向は, 離散化されていますが, 振幅軸の方向は連続したままです. 量子化では,
              量子化ビットで指定された精度にしたがって, 振幅を整数値に丸める処理を実行します. 例えば, 量子化ビットが <code>2 bit</code> の場合, 4
              つのステップの値 (<span class="math-inline">$2^{2} = 4$</span>) のいずれかに, <code>3 bit</code> の場合, 8 つのステップの値 (<span
                class="math-inline"
                >$2^{3} = 8$</span>) のいずれかに振幅が丸められます.
            </p>
            <figure>
              <svg id="svg-figure-quantization" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>量子化 (量子化ビット <code>3 bit</code>)</figcaption>
            </figure>
            <p>
              量子化の丸め処理によって生じる誤差を, <code>量子化雑音</code>と呼びます. 量子化ビットが小さいほど, 丸め処理による誤差が大きくなり,
              原信号への復元も精度が低くなってしまいます. 逆に, 量子化の精度を高くするほど, すなわち, 量子化ビットを大きくするほど, 量子化雑音は少なくなり
              (誤差が小さくなり), 原信号への復元の精度も高くなりますが, データサイズは量子化ビットに比例して大きくなります.
            </p>
            <figure>
              <svg id="svg-figure-quantization-bits" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>量子化ビット</figcaption>
            </figure>
            <p>音楽 CD での量子化ビットは 16 bit に設定されています. ハイレゾオーディオの量子化ビットは <code>24 bit</code> 以上が必要条件となっています.</p>
          </section>
          <section id="section-analog-to-digital-conversion-coding">
            <h4>符号化</h4>
            <p>
              サンプリングによって, 時間軸方向に離散化し, それぞれのサンプル点を, 量子化によって丸めた整数値に 2 進数を割り当てていきます. 量子化した
              (整数値に丸めた) 振幅を 2 進数に符号化すると, コンピュータの内部で処理することが可能なデジタル信号となります.
            </p>
            <p>サンプリング周波数 <code>16 Hz</code>, 量子化ビット <code>4 bit</code>, 2 の補数方式で符号化した例です.</p>
            <figure>
              <svg id="svg-figure-coding" width="720" height="405" data-parameters="true" data-a="1" data-f="1" data-t="0.0,0.5,1.0" />
              <figcaption>符号化</figcaption>
            </figure>
          </section>
          <article id="section-analog-to-digital-conversion-pcm">
            <h4>PCM (Pulse Code Modulation)</h4>
            <p>
              <b>PCM</b> (<b>Pulse Code Modulation</b>) とは, このセクションで解説したように, アナログ信号をデジタル信号に変換する変調方式のことです.
              厳密に表現すると, このセクションで解説した PCM は, 量子化の幅を均等 (線形的) に取得しているので, <b>Linear PCM</b> です. 量子化の方式によって,
              <b>log-PCM</b>, <b>DPCM</b> (<b>Differential PCM</b>), <b>ADPCM</b> (<b>Adaptive Differential PCM</b>) などがあります.
              どれが優れた方式というのはなく, ケースによって使い分けますが, 多くの場合, Linear PCM が使われているので, 単純に PCM と言った場合, Linear PCM
              を意味することが多いです. Web Audio API でもオーディオデータの実体である <code>AudioBuffer</code> では, <code>32 bit</code> (浮動小数点数) の
              Linear PCM による値を格納しています.
            </p>
          </article>
        </section>
        <section id="section-fourier-analysis">
          <h3>フーリエ解析</h3>
          <p>
            このセクションでは, デジタルオーディオ信号処理において, 中核となる数学的処理である, <b>フーリエ解析</b> (フーリエ級数とフーリエ級数を一般化した
            (非周期関数に拡張した) フーリエ変換, コンピュータでフーリエ変換を実行するための離散フーリエ変換, そして, 回転因子の性質を利用して,
            離散フーリエ変換の (時間) 計算量を <span class="math-inline">$O\left(N^{2}\right)$</span> から
            <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> に減らして実行する高速フーリエ変換) について解説します. もっとも,
            数式による厳密な解説や証明は, 最適なドキュメントや書籍がすでにたくさんあるので, できるだけ, Web Audio API での仕様を把握したり,
            <code>AudioWorklet</code> でオーディオ信号処理を実装したりする場合を想定して, 数式による (厳密な) 解説は最小限にとどめて,
            イラストやコードをベースに, 概念を理解するために役に立つ内容になればと思います.
          </p>
          <section id="section-fourier-series">
            <h4>フーリエ級数</h4>
            <p>
              <a href="#section-amplitude-and-frequency">周期関数</a>は, 周波数の異なる余弦波と正弦波の級数で近似することができます. この級数が,
              <b>フーリエ級数</b>であり, 周期関数をフーリエ級数で表現する場合, <b>フーリエ級数展開</b> と呼ばれます.
              <span class="math-inline">$x\left(t\right)$</span>が, 周期 <span class="math-inline">$T$</span> の場合, フーリエ級数は以下の数式で定義されます.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &f\left(t\right) = \frac{a_{0}}{2} + \sum_{n=1}^{\infty}\left(a_{n}\cos\left(n\frac{2 \pi}{T}t\right) + b_{n}\sin\left(n\frac{2 \pi}{T}t\right)\right) \\
                  &a_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\cos\left(n\frac{2 \pi}{T}t\right)dt \\
                  &b_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\sin\left(n\frac{2 \pi}{T}t\right)dt \\
                \end{flalign}
              $
            </div>
            <p>
              <span class="math-inline">$a_{n}$</span>, <span class="math-inline">$b_{n}$</span> は <b>フーリエ係数</b>で,
              物理的には各周波数成分の<b>振幅</b>を表しています. また, <span class="math-inline">$\frac{2 \pi}{T} = 2 \pi f$</span> は, <b>角速度</b>
              <span class="math-inline">$\omega$</span> で定義される場合もあります.
            </p>
            <p>
              厳密には, フーリエ級数が成立する条件は, 周期関数であるだけでは不十分で, <b>ディリクレの条件</b>と合わせて十分条件となります.
              これを理解するためには, 三角関数の基本的な性質 (高校レベルの数学) や三角関数の直交性などをもとに,
              <b>リーマン・ルベーグの補助定理</b>や<b>パーセバルの等式</b>などを理解する必要があるので, 数学的な厳密性を理解したい場合は,
              それぞれ最適なドキュメントを参考にしてください.
            </p>
            <p>
              ここでは, 視覚的に理解するために, 周波数の異なる正弦波の級数で, 矩形波やノコギリ波, 三角波を生成してみます. また, 級数を大きくする
              (項数を大きくする) ほど, 実際の波形により近似することもわかります.
            </p>
            <div class="app-container">
              <svg id="svg-fourier-series" class="svg-fourier-series" width="720" height="240"></svg>
              <div class="forms-fourier-series">
                <button type="button" id="button-plot-fourier-series">Plot</button>
                <button type="button" id="button-clear-fourier-series">Clear</button>
                <button type="button" id="button-animation-fourier-series">Animation</button>
                <label for="select-interval-fourier-series">Interval</label>
                <select id="select-interval-fourier-series">
                  <option value="500" selected>500 msec</option>
                  <option value="60 fps">60 fps</option>
                  <option value="50">50 msec</option>
                  <option value="100">100 msec</option>
                  <option value="250">250 msec</option>
                </select>
              </div>
              <div class="forms-fourier-series">
                <label for="select-function-fourier-series">Wave Type</label>
                <select id="select-function-fourier-series">
                  <option value="square" selected>Square</option>
                  <option value="sawtooth">Sawtooth</option>
                  <option value="triangle">Triangle</option>
                </select>
                <label>
                  <span id="output-sum-box-fourier-series">&Sigma; <sub>k</sub> = <span id="output-sum-fourier-series">1</span></span>
                  <input type="range" id="range-sum-fourier-series" value="1" min="1" max="100" step="1" />
                </label>
              </div>
            </div>
            <p>
              余弦波と正弦波で表現されるフーリエ級数に, <b>オイラーの公式</b> (<span class="math-inline">$j$</span> は
              <span class="math-inline">$j^{2} = -1$</span> となる虚数単位) を適用すると, <b>複素フーリエ級数</b>を導出可能です (つまり, 複素フーリエ級数は,
              より一般化したフーリエ級数と言えます. さらに一般化を進めると, フーリエ変換となります).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{align}
                  &e^{j\theta} = \cos\left(\theta\right) + j\sin\left(\theta\right) \quad (Euler's formula) \\
                  &f\left(t\right) = \sum_{n=-\infty}^{\infty}c_{n}e^{jn\frac{2 \pi}{T}t} \\
                  &c_{n} = \frac{1}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)e^{-jn\frac{2 \pi}{T}t}dt \\
                \end{align}
              $
            </div>
            <p>
              物理的な観点で理解すると, 複素フーリエ級数は, 余弦波と正弦波の 2 次元の<b>振動現象</b>であるフーリエ級数を, 3 次元の<b>回転</b>へと拡張します.
              また, 複素フーリエ級数によって, フーリエ級数の問題点, すなわち, 位相をシフトするとフーリエ係数の値が変化する問題
              (余弦波と正弦波は位相の違いでしかないので, 原信号が同じでもフーリエ係数が異なることが起きうる) を発展的に解決します.
            </p>
          </section>
          <section id="section-fourier-transform">
            <h4>フーリエ変換</h4>
            <p>
              フーリエ級数が周期関数のみ適用可能だったのを, 非周期関数にも適用できるように, さらに拡張したフーリエ級数が<b>フーリエ変換</b>です.
              フーリエ級数から, フーリエ変換を導出するには, 非周期, つまり, <b>周期を <span class="math-inline">$\infty$</span></b> に拡張して導出します.
              具体的には, 複素フーリエ級数の係数 <span class="math-inline">$c_{n}$</span> の周期 <span class="math-inline">$T$</span> を
              <span class="math-inline">$\infty$</span> に拡張すると, 角速度
              <span class="math-inline">$\omega (= \frac{2 \pi}{T} = 2 \pi f)$</span> が連続的な角速度になることで導出できます (以下は,
              フーリエ変換と逆フーリエ変換の定義式です).
            </p>
            <p>
              フーリエ変換の厳密な導出を理解するには, <b>デルタ関数</b>や<b>単位階段関数</b>の理解が必要になります (さらに, フーリエ変換では,
              <b>絶対可積分</b> (<span class="math-inline">$\int_{-\infty}^{\infty}\left|f\left(t\right)\right|dt \lt \infty$</span> ) が必要条件となります.
              フーリエ級数からフーリエ変換の数学的な導出の詳細に関しては, それぞれ最適なドキュメントを参考にしてください).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &F\left(f\right) = \int_{-\infty}^{\infty}f\left(t\right)e^{-j2 \pi ft}dt \\
                  &f\left(t\right) = \frac{1}{2 \pi}\int_{-\infty}^{\infty}F\left(f\right)e^{j2 \pi tf}df \\
                \end{flalign}
              $
            </div>
            <section id="section-spectrum">
              <h5>スペクトル</h5>
              <p>
                フーリエ変換後の関数は, 物理的には<b>スペクトル</b>となります. スペクトルとは, 各周波数成分の振幅や位相を表す波形です (したがって,
                周波数領域の波形, 周波数ドメインの波形などと表現することもあります). つまり, 2 次元のグラフで考えると, 横軸の次元が周波数となり,
                縦軸が振幅であれば, <b>振幅スペクトル</b>, 位相であれば, <b>位相スペクトル</b>となります. もう少し厳密に説明すると, フーリエ変換後の関数は,
                複素数の関数となるので, <b>絶対値</b>を取得すれば振幅スペクトル, <b>偏角</b>を取得すれば位相スペクトルとなります (以下に, 複素数
                <span class="math-inline">$z = x + jy$</span> を定義した場合の絶対値 <span class="math-inline">$\left|z\right|$</span> と偏角
                <span class="math-inline">$\theta$</span> を記載します). フーリエ変換後の関数を逆フーリエ変換すると, 元の横軸を時間とした波形となります.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $\left|z\right| = \sqrt{x^{2} + y^{2}} \quad \cos\theta = \frac{x}{\sqrt{x^{2} + y^{2}}} \quad \sin\theta = \frac{y}{\sqrt{x^{2} + y^{2}}} \quad \tan\theta = \left(\frac{\sin\theta}{\cos\theta}\right) \quad \theta = \tan^{-1}\left(\frac{\sin\theta}{\cos\theta}\right) = \tan^{-1}\left(\frac{y}{x}\right)$
              </div>
              <p>
                ちなみに, <b>人間の聴覚は位相スペクトルの違いに鈍感</b>という特性があるので, 一般的に, スペクトルと表現した場合,
                振幅スペクトルを意味することがほとんどです.
              </p>
              <p>
                音響特徴量は振幅スペクトルにあらわれることが多く, したがって, オーディオ信号処理を適用する場合,
                周波数領域にて演算を実行することが頻繁にあります. このことが, デジタルオーディオ信号処理において, フーリエ解析 (コンピュータでは,
                高速フーリエ変換) が中核となる理由です.
              </p>
            </section>
          </section>
          <section id="section-discrete-fourier-transform">
            <h4>離散フーリエ変換 (DFT)</h4>
            <p>
              コンピュータで実現する場合, 無限区間の積分は原理上できないので, ある区間で和分を算出する必要があります. これが,
              <b>離散フーリエ変換</b> (<b>DFT</b>: <b>Discrete Fourier Transform</b>)です (余談ですが, コンピュータにおいて, 積分は和分,
              微分は差分で実装します).
            </p>
            <p>
              フーリエ変換から, 離散フーリエ変換を導出するには, 周波数 (周期) と, サンプリング周波数 (サンプリング周期) を数列 (離散値) で対応づけます.
              <span class="math-inline">$f_{s}$</span> は, サンプリング周波数 (<span class="math-inline">$T_{s}$</span> は, サンプリング周期). また,
              離散フーリエ変換は, 一定のサイズで変換する必要があるので <span class="math-inline">$N$</span> は, 離散フーリエ変換のサンプル数です (Web Audio API
              では, <code>AnalyserNode</code> の <b><code>fftSize</code></b> プロパティの値に相当します).
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &t = nT_{s} = \frac{n}{f_{s}} \quad (n = 0. 1, 2, \cdots N - 1) \\
                  &f = k\frac{f_{s}}{N} \quad (k = 0, 1, 2, \cdots N - 1) \\
                \end{flalign}
              $
            </div>
            <p>
              そして, 積分は和分になるので, これらをフーリエ変換の式に適用して, 変形すると, 離散フーリエ変換と逆離散フーリエ変換の定義式が導出できます.
              <span class="math-inline">$x\left(n\right)$</span>, および, <span class="math-inline">$X\left(k\right)$</span> は, サンプリングした信号です.
              数学的には数列, プログラミング的には配列のような順序性をもつ数値のコレクションと考えると理解しやすいかもしれません.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)e^{-j\frac{2 \pi kn}{N}} \\
                  &x\left(n\right) = \frac{1}{N}\sum_{k = 0}^{N - 1}X\left(k\right)e^{j\frac{2 \pi nk}{N}} \\
                \end{flalign}
              $
            </div>
            <p>
              多くのプログラミング言語において, 配列のようなコレクションのインデックスは <code>0</code> から開始するので, 離散フーリエ変換の積和演算の範囲も,
              <code>0</code> から開始している点と有界となっている点に着目してください.
            </p>
            <p>また, <span class="math-inline">$e^{-j\frac{2 \pi n}{N}}$</span> は, <b>回転因子</b> (<b>回転子</b>) と呼ばれ, 以下のように定義されます.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $W^{n} = e^{-j\frac{2 \pi n}{N}} = \cos\left(\frac{2 \pi n}{N}\right) - j\sin\left(\frac{2 \pi n}{N}\right)$
            </div>
            <p>回転因子は, 例えば, <span class="math-inline">$N$</span> を <code>8</code> とした場合, 複素平面上の単位円を 8 分割するような回転を表現します.</p>
            <figure>
              <svg id="svg-figure-rotation-factors" width="720" height="405" />
              <figcaption><span class="math-inline">$N = 8$</span> の場合の回転因子</figcaption>
            </figure>
            <p>
              このことから, 回転因子は以下のような性質をもっています (また, 離散フーリエ変換のサイズ <span class="math-inline">$\frac{N}{2}$</span> は,
              ナイキスト周波数成分のインデックスに相当します).
            </p>
            <ul>
              <li><span class="math-inline">$W^{n + N} = W^{n}$</span></li>
              <li><span class="math-inline">$W^{n + \frac{N}{2}} = -W^{n}$</span></li>
            </ul>
            <p>
              以下は, 回転因子で定義した離散フーリエ変換と逆離散フーリエ変換です. 高速フーリエ変換では, 回転因子の性質
              (周期性による対称性や半周期性の負の対称性) を利用して, 各要素の計算量を減らして演算の高速化を実現しています.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)W^{n} \\
                  &x\left(n\right) = \frac{1}{N}\sum_{k = 0}^{N - 1}X\left(k\right)W^{-k} \\
                \end{flalign}
              $
            </div>
          </section>
          <section id="section-fast-fourier-transform">
            <h4>高速フーリエ変換 (FFT)</h4>
            <p>
              <b>高速フーリエ変換</b> (<b>FFT</b>: <b>Fast Fourier Transform</b>) は, 回転因子の性質を利用して, 離散フーリエ変換では (時間) 計算量を
              <span class="math-inline">$O\left(N^{2}\right)$</span> 要するのを,
              <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> にまで減らして, コンピュータでのフーリエ変換を高速化するアルゴリズムです.
              ただし, 回転因子の性質を利用する関係上, <b>フーリエ変換のサイズが 2 の冪乗</b>の場合のみ高速化できるという制約がつきます (この制約に関しては,
              <code>0</code> 埋め処理などによって, 強制的にフーリエ変換のサイズを 2 の冪乗にするなどして解決できます). Web Audio API においても,
              <code>AnalyserNode</code> の <code>fftSize</code> プロパティがとりうる値は, すべて 2 の冪乗です.
            </p>
            <p>実際に, どのように計算量を削減しているのかを解説していきます.</p>
            <section id="section-fast-fourier-transform-algorithm">
              <h5>高速フーリエ変換の導出</h5>
              <p>離散フーリエ変換の式を行列演算に書き換えます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &X\left(k\right) = \sum_{n = 0}^{N - 1}x\left(n\right)W^{n} \\
                  \end{flalign}
                $
              </div>
              <p><span class="math-inline">$N = 4$</span> として, 行列演算に変換します.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{3} & W^{6} & W^{9} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>ここで, 回転因子の性質を利用すると, <span class="math-inline">$W^{4} = W^{0}, W^{6} = W^{2}, W^{9} = W^{1}$</span> となるので,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{2} & W^{0} & W^{2} \\
                    W^{0} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>ここまでで, 離散フーリエ変換の演算を行列演算に変換することができました.</p>
              <p>
                行列演算に変換できたら, 行を偶奇で分割します. 偶数行を行列の上部に入れ替えて, 奇数行を行列の下部に入れ替えます. 変換行列の行を入れ替えるので,
                出力となる <span class="math-inline">$X_{k}$</span> も行が入れ替わることに注意してください.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{0} & W^{2} \\
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <p>これを考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{2}) \\
                    (x_{1} + x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & W^{3} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{2}) \\
                    (x_{1} - x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <p>この変形によって, 乗算と加算の回数がおよそ半分まで減らすことができました. ここでさらに回転因子の性質を利用すると, 以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & -W^{0} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{2}) \\
                    (x_{1} + x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & -W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{2}) \\
                    (x_{1} - x_{3}) \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ところで, 行の偶奇を入れ替えたので, 出力となる <span class="math-inline">$X_{k}$</span> の順序が, 入力の順序と一致しなくなります (つまり,
                ある時間領域の値のスペクトルが一致しなくなります). 実は, 一見するとランダムに並びますが, 規則性があり, 各インデックスを 2 進数で表現した場合の,
                ビットを上下反転させた関係になっています. この関係を利用して, インデックスの並びを整列するアルゴリズムを<b>ビットリバース</b>と呼びます.
              </p>
              <table>
                <caption>
                  ビットリバースの対応表 2 ビット (<span class="math-inline">$N = 4$</span>)
                </caption>
                <thead>
                  <tr>
                    <th scope="col">Index</th>
                    <th scope="col"><span class="math-inline">$x_{n}$</span></th>
                    <th scope="col"><span class="math-inline">$X_{k}$</span></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>0</td>
                    <td>00</td>
                    <td>00</td>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>01</td>
                    <td>10</td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>10</td>
                    <td>01</td>
                  </tr>
                  <tr>
                    <td>3</td>
                    <td>11</td>
                    <td>11</td>
                  </tr>
                </tbody>
              </table>
              <p>同じように, <span class="math-inline">$N = 8$</span> として, 高速フーリエ変換になるように行列演算します.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                    X_{4} \\
                    X_{5} \\
                    X_{6} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0}  & W^{0}  & W^{0}  & W^{0}  & W^{0}  & W^{0}  \\
                    W^{0} & W^{1} & W^{2}  & W^{3}  & W^{4}  & W^{5}  & W^{6}  & W^{7}  \\
                    W^{0} & W^{2} & W^{4}  & W^{6}  & W^{8}  & W^{10} & W^{12} & W^{14} \\
                    W^{0} & W^{3} & W^{6}  & W^{9}  & W^{12} & W^{15} & W^{18} & W^{21} \\
                    W^{0} & W^{4} & W^{8}  & W^{12} & W^{16} & W^{20} & W^{24} & W^{29} \\
                    W^{0} & W^{5} & W^{10} & W^{15} & W^{20} & W^{25} & W^{30} & W^{35} \\
                    W^{0} & W^{6} & W^{12} & W^{18} & W^{24} & W^{30} & W^{36} & W^{42} \\
                    W^{0} & W^{7} & W^{14} & W^{21} & W^{28} & W^{35} & W^{42} & W^{49} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>回転因子の性質を利用すると,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{1} \\
                    X_{2} \\
                    X_{3} \\
                    X_{4} \\
                    X_{5} \\
                    X_{6} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{1} & W^{2} & W^{3} & W^{4} & W^{5} & W^{6} & W^{7} \\
                    W^{0} & W^{2} & W^{4} & W^{6} & W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{3} & W^{6} & W^{1} & W^{4} & W^{7} & W^{2} & W^{5} \\
                    W^{0} & W^{4} & W^{0} & W^{4} & W^{8} & W^{5} & W^{0} & W^{5} \\
                    W^{0} & W^{5} & W^{2} & W^{7} & W^{4} & W^{1} & W^{6} & W^{3} \\
                    W^{0} & W^{6} & W^{4} & W^{2} & W^{0} & W^{6} & W^{4} & W^{2} \\
                    W^{0} & W^{7} & W^{6} & W^{5} & W^{4} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                行列演算に変換できたら, 行を偶奇で分割します. 偶数行を行列の上部に入れ替えて, 奇数教を行列の下部に入れ替えます. 変換行列の行を入れ替えるので,
                出力となる <span class="math-inline">$X_{k}$</span> も行が入れ替わることに注意してください.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{4} \\
                    X_{6} \\
                    X_{1} \\
                    X_{3} \\
                    X_{5} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{4} & W^{6} & W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{4} & W^{0} & W^{4} & W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{6} & W^{4} & W^{2} & W^{0} & W^{6} & W^{4} & W^{2} \\
                    W^{0} & W^{1} & W^{2} & W^{3} & W^{4} & W^{5} & W^{6} & W^{7} \\
                    W^{0} & W^{3} & W^{6} & W^{1} & W^{4} & W^{7} & W^{2} & W^{5} \\
                    W^{0} & W^{5} & W^{2} & W^{7} & W^{4} & W^{1} & W^{6} & W^{3} \\
                    W^{0} & W^{7} & W^{6} & W^{5} & W^{4} & W^{3} & W^{2} & W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} \\
                    x_{1} \\
                    x_{2} \\
                    x_{3} \\
                    x_{4} \\
                    x_{5} \\
                    x_{6} \\
                    x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 4 行 4 列の行列と, 右上 4 行 4 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 4 行 4 列の行列と, 右下 4 行 4 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <p>これを考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{2} \\
                    X_{4} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{6} & W^{4} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} + x_{4} \\
                    x_{1} + x_{5} \\
                    x_{2} + x_{6} \\
                    x_{3} + x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{3} \\
                    X_{5} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{3} & W^{6} & W^{1} \\
                    W^{0} & W^{5} & W^{2} & W^{7} \\
                    W^{0} & W^{7} & W^{6} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} - x_{4} \\
                    x_{1} - x_{5} \\
                    x_{2} - x_{6} \\
                    x_{3} - x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここまでの処理によって, <span class="">$N = 8$</span> の高速フーリエ変換を <span class="">$N = 4$</span> に帰着することができたので, 再帰的に
                <span class="">$N = 4$</span> の場合も, 行の偶奇を入れ替えて回転因子の性質を利用して,
                <span class="math-inline">$N = 2$</span> の場合の高速フーリエ変換に帰着させます.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} & W^{0} & W^{0} \\
                    W^{0} & W^{4} & W^{0} & W^{4} \\
                    W^{0} & W^{2} & W^{4} & W^{6} \\
                    W^{0} & W^{6} & W^{4} & W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} + x_{4} \\
                    x_{2} + x_{6} \\
                    x_{1} + x_{5} \\
                    x_{3} + x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                ここで, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は対称になっている, すなわち,
                同じ回転方向の回転因子の行列になっています. 同様に, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は負の対称になっている, すなわち,
                回転方向が互いに逆方向の回転因子の行列になっています.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} & W^{2} & W^{3} \\
                    W^{0} & W^{5} & W^{2} & W^{7} \\
                    W^{0} & W^{3} & W^{6} & W^{1} \\
                    W^{0} & W^{7} & W^{6} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    x_{0} - x_{4} \\
                    x_{2} - x_{6} \\
                    x_{1} - x_{5} \\
                    x_{3} - x_{7} \\
                  \end{bmatrix}
                $
              </div>
              <p>
                こちらも, 回転因子の回転方向を考慮すると, 左上 2 行 2 列の行列と, 右上 2 行 2 列の行列は時計回りに
                <span class="math-inline">$\frac{2}{8}$</span> 回転, また, 左下 2 行 2 列の行列と, 右下 2 行 2 列の行列は反時計回りに
                <span class="math-inline">$\frac{2}{8}$</span> 回転しているという対称性があります.
              </p>
              <p>これら考慮すると, 行列演算はさらに以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & W^{4} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) + (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) + (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{2} \\
                    W^{0} & W^{6} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) - (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) - (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & W^{5} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) + W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) + W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{3} \\
                    W^{0} & W^{7} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) - W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) - W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <p>ここでさらに回転因子の性質を利用すると, 以下のように変形できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{0} \\
                    X_{4} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{0} \\
                    W^{0} & -W^{0} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) + (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) + (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{2} \\
                    X_{6} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{2} \\
                    W^{0} & -W^{2} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} + x_{4}) - (x_{2} + x_{6}) \\
                    (x_{1} + x_{5}) - (x_{3} + x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{1} \\
                    X_{5} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{1} \\
                    W^{0} & -W^{1} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) + W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) + W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{bmatrix}
                    X_{3} \\
                    X_{7} \\
                  \end{bmatrix}
                  =
                  \begin{bmatrix}
                    W^{0} & W^{3} \\
                    W^{0} & -W^{3} \\
                  \end{bmatrix}
                  \begin{bmatrix}
                    (x_{0} - x_{4}) - W^{2}(x_{2} - x_{6}) \\
                    (x_{1} - x_{5}) - W^{2}(x_{3} - x_{7}) \\
                  \end{bmatrix}
                $
              </div>
              <p>
                <span class="math-inline">$N = 2$</span> の場合の高速フーリエ変換に帰着できたので, 最後にビットリバースを適用してインデックスを並び替えます.
              </p>
              <table>
                <caption>
                  ビットリバースの対応表 3 ビット (<span class="math-inline">$N = 8$</span>)
                </caption>
                <thead>
                  <tr>
                    <th scope="col">Index</th>
                    <th scope="col"><span class="math-inline">$x_{n}$</span></th>
                    <th scope="col"><span class="math-inline">$X_{k}$</span></th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>0</td>
                    <td>000</td>
                    <td>000</td>
                  </tr>
                  <tr>
                    <td>1</td>
                    <td>001</td>
                    <td>100</td>
                  </tr>
                  <tr>
                    <td>2</td>
                    <td>010</td>
                    <td>010</td>
                  </tr>
                  <tr>
                    <td>3</td>
                    <td>011</td>
                    <td>110</td>
                  </tr>
                  <tr>
                    <td>4</td>
                    <td>100</td>
                    <td>001</td>
                  </tr>
                  <tr>
                    <td>5</td>
                    <td>101</td>
                    <td>101</td>
                  </tr>
                  <tr>
                    <td>6</td>
                    <td>110</td>
                    <td>011</td>
                  </tr>
                  <tr>
                    <td>7</td>
                    <td>111</td>
                    <td>111</td>
                  </tr>
                </tbody>
              </table>
              <p>
                高速フーリエ変換のサイズを一般化して, <span class="math-inline">$N = 2^{m}$</span> の場合も,
                <span class="math-inline">$2^{m}, 2^{m - 1}, 2^{m - 2}\cdots 32, 16, 8, 4$</span> と再帰的に高速フーリエ変換を導出することが可能です.
              </p>
              <p>
                同様に, 逆離散フーリエ変換の (時間) 計算量も <span class="math-inline">$O\left(N^{2}\right)$</span> から
                <span class="math-inline">$O\left(N\mathrm{log_{2}}N\right)$</span> に減らすことができます (<b>逆高速フーリエ変換</b> (<b>IFFT</b>:
                <b>Inverse Fast Fourier Transform</b>)).
              </p>
              <p>
                このセクションで解説した高速フーリエ変換は, <b>時間間引き型高速フーリエ変換</b>のアルゴリズムとなります (次のセクションで解説する, FFT
                のバタフライ演算のフロー図において, 左側 (時間領域) から, 右側 (周波数領域) へフローするアルゴリズムです. 逆に, 右側 (周波数領域) から, 左側
                (時間領域) へフローするアルゴリズムは, <b>周波数間引き型高速フーリエ変換</b>と呼ばれます. しかし, この 2
                つは高速フーリエ変換と逆高速フーリエ変換の関係にあるだけで, アルゴリズムの本質は同じです).
              </p>
            </section>
            <section id="section-fast-fourier-transform-code">
              <h5>高速フーリエ変換の実装</h5>
              <p>
                離散フーリエ変換の定義式から, 高速フーリエ変換が導出できることはわかりましたが,
                回転因子の性質や行列の変形をコードに記述するのは少し難しいと思います. そこで, 一般的には,
                高速フーリエ変換と等価な<b>バタフライ演算</b>のフロー図を考えることで, より実装に近い形式で考えることができます.
              </p>
              <figure>
                <svg id="svg-figure-fft-symbols" width="720" height="88" />
                <figcaption>バタフライ演算のフロー図の記号</figcaption>
              </figure>
              <p><span class="math-inline">$N = 4$</span> の場合の, 高速フーリエ変換の導出をバタフライ演算のフロー図を記載します.</p>
              <figure>
                <svg id="svg-figure-fft-4" width="720" height="405" />
                <figcaption>FFT のサイズ <span class="math-inline">$N = 4$</span> の場合のバタフライ演算のフロー図</figcaption>
              </figure>
              <p>同様に, <span class="math-inline">$N = 8$</span> の場合の, 高速フーリエ変換の導出をバタフライ演算のフロー図を記載します.</p>
              <figure>
                <svg id="svg-figure-fft-8" width="1600" height="405" />
                <figcaption>FFT のサイズ <span class="math-inline">$N = 8$</span> の場合のバタフライ演算のフロー図</figcaption>
              </figure>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">function pow2(n) {
  return 2 ** n;
}

/**
 * FFT
 *
 * @param {Float32Array} reals This argument is instance of `Float32Array` for real number.
 * @param {Float32Array} imags This argument is instance of `Float32Array` for imaginary number.
 * @param {number} size This argument is FFT size (power of two).
 */
function FFT(reals, imags, size) {
  const indexes = new Uint16Array(size);  // FFT size is `0` between `65535`.

  const numberOfStages = Math.log2(size);

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      const rest = numberOfStages - stage;

      for (let j = 0; j &lt; pow2(rest); j++) {
        const n = i * pow2(rest + 1) + j;
        const m = pow2(rest) + n;
        const w = 2.0 * Math.PI * j * pow2(stage - 1);

        const areal = reals[n];
        const aimag = imags[n];
        const breal = reals[m];
        const bimag = imags[m];
        const wreal = Math.cos(w / size);
        const wimag = -1 * Math.sin(w / size);  // Clockwise

        if (stage &lt; numberOfStages) {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = (wreal * (areal - breal)) - (wimag * (aimag - bimag));
          imags[m] = (wreal * (aimag - bimag)) + (wimag * (areal - breal));
        } else {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = areal - breal;
          imags[m] = aimag - bimag;
        }
      }
    }
  }

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    const rest = numberOfStages - stage;

    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      indexes[pow2(stage - 1) + i] = indexes[i] + pow2(rest);
    }
  }

  for (let k = 0; k &lt; size; k++) {
    if (indexes[k] &lt;= k) {
      continue;
    }

    const real = reals[indexes[k]];
    const imag = imags[indexes[k]];

    reals[indexes[k]] = reals[k];
    imags[indexes[k]] = imags[k];

    reals[k] = real;
    imags[k] = imag;
  }
}

/**
 * IFFT
 *
 * @param {Float32Array} reals This argument is instance of `Float32Array` for real number.
 * @param {Float32Array} imags This argument is instance of `Float32Array` for imaginary number.
 * @param {number} size This argument is IFFT size (power of two).
 */
function IFFT(reals, imags, size) {
  const indexes = new Uint16Array(size);  // FFT size is `0` between `65535`.

  const numberOfStages = Math.log2(size);

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      const rest = numberOfStages - stage;

      for (let j = 0; j &lt; pow2(rest); j++) {
        const n = i * pow2(rest + 1) + j;
        const m = pow2(rest) + n;
        const w = 2.0 * Math.PI * j * pow2(stage - 1);

        const areal = reals[n];
        const aimag = imags[n];
        const breal = reals[m];
        const bimag = imags[m];
        const wreal = Math.cos(w / size);
        const wimag = Math.sin(w / size);  // Counterclockwise

        if (stage &lt; numberOfStages) {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = (wreal * (areal - breal)) - (wimag * (aimag - bimag));
          imags[m] = (wreal * (aimag - bimag)) + (wimag * (areal - breal));
        } else {
          reals[n] = areal + breal;
          imags[n] = aimag + bimag;
          reals[m] = areal - breal;
          imags[m] = aimag - bimag;
        }
      }
    }
  }

  for (let stage = 1; stage &lt;= numberOfStages; stage++) {
    const rest = numberOfStages - stage;

    for (let i = 0; i &lt; pow2(stage - 1); i++) {
      indexes[pow2(stage - 1) + i] = indexes[i] + pow2(rest);
    }
  }

  for (let k = 0; k &lt; size; k++) {
    if (indexes[k] &lt;= k) {
      continue;
    }

    const real = reals[indexes[k]];
    const imag = imags[indexes[k]];

    reals[indexes[k]] = reals[k];
    imags[indexes[k]] = imags[k];

    reals[k] = real;
    imags[k] = imag;
  }

  for (let k = 0; k &lt; size; k++) {
    reals[k] /= size;
    imags[k] /= size;
  }
}</code></pre>
              <p>
                FFT と IFFT の実装上の違いは, 回転因子の回転方向が互いに逆なのと, IFFT の場合 <span class="math-inline">$N$</span> 倍された値になるので,
                最後に正規化の処理がある点です.
              </p>
              <article id="section-fast-fourier-transform-algorithms">
                <h6>改良された高速フーリエ変換の実装</h6>
                <p>
                  記載した FFT / IFFT の実装は <b>Cooley-Tukey 型 FFT</b> をもとに, 少し簡素化して理解しやすくした実装です (実際の Cooley-Tukey 型 FFT は,
                  分割統治的な再帰呼び出しであったり, 回転因子の演算を事前にテーブルを確保して計算しておいてより高速化していたりします). また,
                  高速フーリエ変換のアルゴリズム・実装は 1 つではなく, さらに高速化するために基数を利用した実装 (<b>Radix-4 FFT</b>, <b>Radix-8 FFT</b>, さらに,
                  それらを組み合わせた <b>Split-Radix FFT</b> (加算と乗算のトータルの演算回数が最小となる FFT アルゴリズムです) などがあります) や,
                  <a href="https://qiita.com/habuyoshiaki/items/68d502c7bc0c35c168ef" target="_blank" rel="noopener noreferrer"
                    >2 の冪乗でなくても高速フーリエ変換を適用できるアルゴリズム (任意要素数の高速フーリエ変換)</a>
                  が知られています. アプリケーションの要件では, 上記で記載した FFT / IFFT, あるいは, それらを WebAssembly
                  で高速化した実装でも十分かもしれませんが, それ以上のパフォーマンスを必要とする場合は, ぜひ適切な FFT / IFFT
                  のアルゴリズムを調べて実装してみてください.
                </p>
              </article>
            </section>
          </section>
          <figure>
            <img src="images/fourier-transform.png" alt="" width="800" height="600" loading="lazy" />
            <figcaption>フーリエ解析 (フーリエ級数・フーリエ変換) のイメージ</figcaption>
          </figure>
        </section>
        <section id="section-fundamental-frequency-and-harmonic">
          <h3>基本周波数と倍音</h3>
          <p>
            音響特徴量は振幅スペクトルにあらわれることが多いことから, 音の分析, イコール, スペクトル分析と表現しても過言ではないぐらいです. したがって,
            このセクションでは スペクトルの基本構造に関して解説したいと思います.
          </p>
          <p>
            周波数成分は, <b>基本周波数</b> (略して <span class="math-inline">$f_{0}$</span> と呼ぶことも多いです) と<b>倍音</b>に分類することができます.
            最も低い周波数成分を基本周波数と呼び, 基本周波数の整数倍となる周波数成分を倍音と呼びます.
          </p>
          <p>
            <code>OscillatorNode</code> の <code>frequency</code> / <code>detune</code> プロパティは周波数を設定するプロパティ (<code>AudioParam</code>)
            と解説しましたが, 厳密には, <b>基本周波数を設定するプロパティ</b>です.
          </p>
          <p>基本波形を例にとって, 基本周波数と倍音をより具体的に解説します.</p>
          <p>
            基本波形の最小単位は正弦波です. 正弦波は倍音をもちません. 基本周波数の成分しかもたないので<b>純音</b>とも呼ばれます. そして,
            基本周波数と倍音を合成した波形が, 矩形波やノコギリ波, 三角波です.
          </p>
          <table>
            <caption>
              基本波形における, 基本周波数と倍音
            </caption>
            <thead>
              <tr>
                <th scope="col">Wave Type</th>
                <th scope="col">Spectrum</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>正弦波</td>
                <td>基本周波数成分のみをもつ</td>
              </tr>
              <tr>
                <td>矩形波</td>
                <td>基本周波数と奇数次の倍音成分をもつ</td>
              </tr>
              <tr>
                <td>ノコギリ波</td>
                <td>基本周波数と奇数次・偶数次の倍音成分をもつ</td>
              </tr>
              <tr>
                <td>三角波</td>
                <td>基本周波数と奇数次の倍音成分をもつ (高音域の倍音成分が小さい)</td>
              </tr>
            </tbody>
          </table>
          <div class="app-container">
            <dl>
              <dt>Time Domain</dt>
              <dd><svg id="svg-time" class="svg-time" width="720" height="240"></svg></dd>
              <dt>Frequency Domain (Spectrum)</dt>
              <dd><svg id="svg-spectrum" class="svg-spectrum" width="720" height="240"></svg></dd>
            </dl>
            <div>
              <button type="button" id="button-spectrum" class="button-spectrum">start</button>
              <form id="form-oscillator-type-spectrum" class="form-oscillator-type">
                <label><span>sine</span><input type="radio" name="radio-oscillator-type-spectrum" value="sine" checked /></label>
                <label><span>square</span><input type="radio" name="radio-oscillator-type-spectrum" value="square" /></label>
                <label><span>sawtooth</span><input type="radio" name="radio-oscillator-type-spectrum" value="sawtooth" /></label>
                <label><span>triangle</span><input type="radio" name="radio-oscillator-type-spectrum" value="triangle" /></label>
              </form>
              <div class="ranges-oscillator">
                <label><span>gain</span><input type="range" id="range-gain-spectrum" value="1" min="0" max="1" step="0.05" /></label>
                <label><span>frequency</span><input type="range" id="range-frequency-spectrum" value="440" min="27.5" max="1000" step="0.5" /></label>
                <label><span>detune</span><input type="range" id="range-detune-spectrum" value="0" min="-600" max="600" step="1" /></label>
              </div>
            </div>
          </div>
          <p>
            基本波形と同じように, 楽器音や音声も基本周波数と倍音の周波数成分によって構成されています. 厳密には,
            自然の音は必ずしもこのような整数倍になっていません. しかし, 実はこのことが人工的な音と感じさせない要因ともなっています. また, エフェクターの 1
            つである, オーバードライブやディストーションは, 本来発生しない倍音を発生させることによって歪みを与えます
            (オーディオ信号処理における<b>非線形処理</b>によって発生させることができるエフェクターです).
          </p>
          <p>また, ホワイトノイズのような雑音はすべての周波数成分を含んでいるので, そのスペクトルは一様になります.</p>
        </section>
      </section>
      <section id="section-effectors">
        <h2>エフェクター</h2>
        <p>
          このサイトのオーナーはエレキギターを弾くので, オーナー個人的には, オーディオプログラミングの最大の楽しみはエフェクターを実装することだと思っています.
        </p>
        <p>
          Web Audio API のユースケースとしても, エフェクターは考慮されており, <code>GainNode</code>, <code>DelayNode</code>, <code>BiquadFilterNode</code>,
          <code>WaveShaperNode</code>, <code>DynamicsCompressorNode</code> などによって,
          エフェクターの原理さえ簡単に理解していれば実装が容易なぐらいに抽象化されています (エフェクターのためにここまで抽象化されているオーディオ API は,
          現時点でおそらく他にありません).
        </p>
        <section id="section-effectors-overview">
          <h3>エフェクター実装の基本</h3>
          <p>一方で, 抽象化されているがゆえに, Web Audio API でエフェクターを実装する場合に理解しておくべきことが 2 つあります.</p>
          <ul>
            <li>LFO (Low Frequency Oscillator) の実装</li>
            <li><code>AudioParam</code> への接続</li>
          </ul>
          <section id="section-effectors-lfo">
            <h4>LFO (Low Frequency Oscillator)</h4>
            <p>
              エフェクターにはいくつかの種類があり, モジュレーション系と呼ばれるエフェクター (コーラス, フランジャー, フェイザー, トレモロ, ワウなど)
              を実装するためには, <b>特定のパラメータを時間経過とともに周期的に変化させる必要があります</b>. 具体的には, コーラス / フランジャーは,
              ディレイタイム (遅延時間) を時間経過とともに周期的に変化させることによって実装可能です. そして,
              特定のパラメータを時間経過とともに周期的に変化させる機能が <b>LFO</b> (<b>Low Frequency Oscillator</b>) です.
            </p>
            <p>
              LFO の実装は Web Audio API に限ったことではないのですが, <code>OscillatorNode</code> を利用して LFO を実装する場合には, Web Audio API
              特有のことをもう 1 つ理解している必要があります. それが, <code>AudioParam</code> への接続です.
            </p>
          </section>
          <section id="section-effectors-connect-to-audio-param">
            <h4>AudioParam への接続</h4>
            <p>
              結論から記載すると, <code>AudioNode</code> の <code>connect</code> メソッドはオーバーロードされており, 第 1 引数は
              <code>AudioNode</code> インスタンスだけでなく, <code>AudioParam</code> インスタンスを指定することも可能です. つまり,
              <code>OscillatorNode</code> の接続先を <code>AudioParam</code> にすることで,
              対象のパラメータを時間経過とともに周期的に変化させることが可能になります.
            </p>
            <p>
              また, LFO のソースとなる <code>OscillatorNode</code> に <code>GainNode</code> を接続することで, パラメータの変化量を調整することが可能になります.
              一般的なエフェクターのパラメータの, <b>Depth</b> は <code>GainNode</code> の <code>gain</code> プロパティの値に, <b>Rate</b> は
              <code>OscillatorNode</code> の <code>frequency</code> プロパティの値と <code>detune</code> プロパティの値に相当しますプロパティの値に相当します.
            </p>
          </section>
          <section id="section-effectors-vibrato">
            <h4>LFO の実装例 (ビブラート)</h4>
            <p>
              LFO と <code>AudioParam</code> への接続, Depth / Rate の制御を具体的に理解するために, 簡易的なビブラートを実装を記載します. (<code
                >OscillatorNode</code>
              の <code>frequency</code> プロパティのデフォルト値である) <code>440 Hz</code> を基準に, Depth で設定した値が Rate の周期で変化することになります.
              初期値で言うと, <code>440 Hz</code> &plusmn; <code>10 Hz</code> の範囲で, <code>frequency</code> プロパティの値が,
              <code>1 sec</code> の間に変化することになります.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-lfo-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-depth&quot; value=&quot;10&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-depth-value&quot;&gt;10&lt;/span&gt;
&lt;label for=&quot;range-lfo-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-rate&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;10&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-rate-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;
let depth      = null;

let depthValue = 10;
let rateValue  = 1;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDepthElement = document.getElementById(&apos;range-lfo-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-lfo-rate&apos;);

const spanPrintDepthElement = document.getElementById(&apos;print-lfo-depth-value&apos;);
const spanPrintRateElement  = document.getElementById(&apos;print-lfo-rate-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context, { frequency: 440 });
  lfo        = new OscillatorNode(context, { frequency: rateValue });
  depth      = new GainNode(context, { gain: depthValue });

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; OscillatorNode.frequency (AudioParam)
  // 440 Hz +- ${depthValue} Hz
  lfo.connect(depth);
  depth.connect(oscillator.frequency);

  // Start immediately
  oscillator.start(0);

  // Start LFO
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;
  depth      = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthValue = event.currentTarget.valueAsNumber;

  if (depth) {
    depth.gain.value = depthValue;
  }

  spanPrintDepthElement.textContent = Math.trunc(depthValue).toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = Math.trunc(rateValue).toString(10);
});</code></pre>
          </section>
          <section id="section-effectors-general-vibrato">
            <h4>汎用的な LFO と Depth の制御</h4>
            <p>
              基準値と Depth の関係から, パラメータ変化の最小値を考慮しておく必要があるのは, LFO の実装として汎用性に欠けます (先ほどのビブラートの実装だと,
              基準値を <code>27.5 Hz</code> にした場合, Depth の値によっては, 負数の周波数になってしまいます). より汎用的な LFO にするために,
              パラメータの変化量を直接 Depth に設定するのではなく, 基準値に対する変化割合を格納する変数を追加して, その比率と基準値から実際の Depth
              を算出します. このような実装にすることで, 基準値に関わらず, Depth の値は <code>0</code> から <code>1</code> (<code>0 %</code> から
              <code>100 %</code>) になるのでより汎用的な実装になります, また, 各 <code>AudioParam</code> のパラメータの値の範囲 (<code>AudioParam</code> の
              <code>minValue</code> / <code>maxValue</code> プロパティにそれぞれ, 最小値と最大値が設定されています)
              を意図せずに超えてしまうバグも防ぐことができます.
            </p>
            <div class="app-container app-vibrato">
              <button type="button" id="button-vibrato">start</button>
              <label for="range-oscillator-frequency">OscillatorNode frequency</label>
              <input type="range" id="range-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" />
              <span id="print-oscillator-frequency-value">440 Hz</span>
              <label for="range-lfo-depth">Depth</label>
              <input type="range" id="range-lfo-depth" value="0.1" min="0" max="1" step="0.05" />
              <span id="print-lfo-depth-value">0.1</span>
              <label for="range-lfo-rate">Rate</label>
              <input type="range" id="range-lfo-rate" value="1" min="1" max="10" step="1" />
              <span id="print-lfo-rate-value">1</span>
            </div>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-oscillator-frequency&quot;&gt;OscillatorNode frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-oscillator-frequency&quot; value=&quot;440&quot; min=&quot;27.5&quot; max=&quot;4000&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-oscillator-frequency-value&quot;&gt;440&lt;/span&gt;
&lt;label for=&quot;range-lfo-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-depth&quot; value=&quot;0.1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-lfo-depth-value&quot;&gt;10&lt;/span&gt;
&lt;label for=&quot;range-lfo-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-lfo-rate&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;10&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-lfo-rate-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;
let depth      = null;

let frequency = 440;
let depthRate = 0.1;
let rateValue = 1;

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeFrequencyElement = document.getElementById(&apos;range-oscillator-frequency&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-lfo-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-lfo-rate&apos;);

const spanPrintFrequencyElement = document.getElementById(&apos;print-oscillator-frequency-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-lfo-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-lfo-rate-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context, { frequency });
  lfo        = new OscillatorNode(context, { frequency: rateValue });
  depth      = new GainNode(context, { gain: oscillator.frequency.value * depthRate });

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; OscillatorNode.frequency (AudioParam)
  lfo.connect(depth);
  depth.connect(oscillator.frequency);

  // Start immediately
  oscillator.start(0);

  // Start LFO
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;
  depth      = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  frequency = event.currentTarget.valueAsNumber;

  if (oscillator &amp;&amp; depth) {
    oscillator.frequency.value = frequency;
    depth.gain.value           = oscillator.frequency.value * depthRate;
  }

  spanPrintFrequencyElement.textContent = `${(Math.trunc(frequency * 10) / 10)} Hz`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  if (oscillator &amp;&amp; depth) {
    depth.gain.value = oscillator.frequency.value * depthRate;
  }

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = Math.trunc(rateValue).toString(10);
});</code></pre>
          </section>
          <article id="section-effectors-lfo-wave-type">
            <h4>LFO の波形</h4>
            <p>
              ここまでの解説やコードでは, LFO の波形は <code>OscillatorNode</code> のデフォルト値である sin 波 (<code>&apos;sine&apos;</code>) を使っていました.
              LFO としてはこれで十分機能しますが, 波形 (<code>OscillatorNode</code> の <code>type</code> プロパティ (<code>OscillatorOptions</code>))
              をノコギリ波や三角波にしても LFO として機能します. また, ランダムなパラメータ変化をさせるためにホワイトノイズを使う場合もあります.
              もし必要であれば, LFO の波形も選択できるようにすると, エフェクトにバリエーションが出せるかもしれません.
            </p>
          </article>
        </section>
        <section id="section-effectors-delay-and-reverb">
          <h3>ディレイ・リバーブ</h3>
          <p>
            ディレイ・リバーブがどんなエフェクターかを簡単に表現すると, ディレイはやまびこ現象を再現するエフェクター, リバーブはコンサートホールなどの (主に,
            室内の) 音の響きを再現するエフェクターとなるでしょう.
          </p>
          <p>
            表現上はまったく別のエフェクターのように思えますが, その原理は同じです (また, ディレイのパラメータの設定しだいでは,
            簡易的なリバーブを再現することも可能です). ディレイ・リバーブともに, <b>FIR フィルタ</b>, つまり,
            加算・乗算・遅延というデジタルフィルタにおける基本処理で実装可能な点です.
          </p>
          <section id="section-effectors-delay">
            <h4>ディレイ</h4>
            <p>ディレイを実装するために必要な処理は, <code>DelayNode</code>の接続とフィードバックです.</p>
            <section id="section-effectors-delay-delay-node">
              <h5>DelayNode</h5>
              <p>
                遅延処理を (抽象化して) 実装するために定義されているのが, <b><code>DelayNode</code></b> です. コンストラクタの第 2 引数 (<b
                  ><code>DelayOptions</code></b>
                の <b><code>maxDelayTime</code></b> プロパティ) には (ファクトリメソッドの場合, 第 1 引数), 遅延時間 (ディレイタイム)
                の最大値を秒単位で指定します. 省略した場合のデフォルト値は <code>1 sec</code> です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { maxDelayTime: 5 });  // 5 sec

// If use `createDelay`
// const delay = context.createDelay(5);  // 5 sec</code></pre>
              <img src="images/delay-node.png" alt="DelayNode and delayTime property" width="1232" height="770" loading="lazy" />
              <p>
                <code>DelayNode</code> インスタンスには, <code>AudioParam</code> である <b><code>delayTime</code></b> プロパティが定義されています. これが,
                遅延時間 (ディレイタイム) を決定づけるプロパティです. 最小値は <code>0 sec</code>, 最大値はインスタンス生成時に指定した値 (sec) です.
              </p>
              <p>遅延した音を生成するには, サウンドの入力点となるノード (<code>OscillatorNode</code> など) を <code>DelayNode</code> に接続します.</p>
              <p>
                以下のコードは, サウンド出力点である <code>AudioDestinationNode</code> に対して 2 つの入力ノードが接続されています. 1 つは, 原音を出力するため,
                そして, もう 1 つは遅延音を出力するためです. このように, 複数のノードを入力ノードとして接続することで,
                それぞれ入力された音をミックスすることが可能になります. これは, ディレイだけではなく他のエフェクターを実装する場合においても,
                <b>原音とエフェクト音をミックスする</b>という処理は必要になります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context);

// If use `createDelay`
// const delay = context.createDelay();

delay.delayTime.value = 0.5;

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>
                これで, ディレイが実装できました ... と言いたいところですが, このコードでは, <b>エフェクターとしてのディレイ</b>は実現できていません.
                <b>エフェクターとしての</b>という意味は, 上記のコードでも遅延した音は発生します. しかしながら, やまびこ現象のように,
                遅延音が少しずつ減衰しながら何度も繰り返し生成することが実装できていません.
              </p>
              <p>
                Web Audio API の設計思想からの観点で説明すると,
                <b><code>DelayNode</code> は, 指定された遅延時間で遅延音を 1 つだけ生成することだからです</b>. すなわち,
                エフェクターのディレイを実現するという役割までは担いません.
              </p>
              <p>
                そこで, エフェクターとしてのディレイを実装するには,
                <code>DelayNode</code> の接続と次のセクションで解説する<b>フィードバック</b>という処理が必要になります.
              </p>
            </section>
            <section id="section-effectors-delay-feedback">
              <h5>フィードバック</h5>
              <p>
                <b>フィードバック</b>とは, 出力された音を入力音として利用することです. つまり, <b>DelayNode</b> によって出力された遅延音を,
                再び入力音とすればディレイを実現することが可能です. これを, Web Audio APIで実装するには,
                <b>フィードバックのための <code>GainNode</code></b> を接続して, その入出力に同じ <code>DelayNode</code> を接続します. また,
                フィードバックの実装は, ディレイに限らず, 様々なエフェクターで必要となります.
              </p>
              <p>
                ちなみに, エレキギターではフィードバック奏法と呼ばれる奏法があります. この奏法は, アンプから出力された音で弦を振動させて,
                それをピックアップが拾い, 再びアンプから出力させることで, 理論上, 永遠の音の伸びを奏でる奏法です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context);

// If use `createDelay`
// const delay = context.createDelay();

delay.delayTime.value = 0.5;

const feedback = new GainNode(context, { gain: 0.5 });

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

// Connect nodes for feedback
// (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
delay.connect(feedback);
feedback.connect(delay);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>上記のコードでは, 大きく 3 つの接続ができました.</p>
              <ul>
                <li>原音を出力する接続</li>
                <li>エフェクト音 (遅延音) を出力する接続</li>
                <li>フィードバック (エフェクト音を再び入力する) のための接続</li>
              </ul>
              <p>
                フィードバック (エフェクト音を再び入力する) の接続によって, 遅延音が少しずつ減衰しながら何度も繰り返し生成されます. 具体例として, 原音のゲインを
                <code>1</code>, フィードバッグのゲインが <code>0.5</code> とすると, 1 つめの遅延音のゲインは, <code>0.5</code> (<code>1 x 0.5</code>), 2
                つめの遅延音のゲインは, <code>0.25</code> (<code>0.5 x 0.5</code>), 3 つめの遅延音のゲインは, <code>0.125</code> (<code>0.25 x 0.5</code>) ...
                といった繰り返しで, 遅延音が少しずつ減衰しながら生成されます. つまり, フィードバックの <code>GainNode</code> の
                <code>gain</code> プロパティを適切に設定すれば, エフェクターとしてのディレイにバリエーションが出せるというこでもあります.
              </p>
              <p>
                ただし, <b>フィードバックの値は <code>1</code> 未満にする必要があります</b>. これは, 直感的な説明をすれば,
                <code>1</code> 以上にすると減衰しない状態 (無限ループのような状態) になってしまうからです. 数学的・工学的な厳密性で説明すると,
                <a href="#section-fourier-transform">絶対可積分</a>を満たさなくなり, (のちのセクションで解説する) FIR
                フィルタが安定しないフィルタとなるからです.
              </p>
              <figure>
                <svg id="svg-animation-feedback" width="720" height="405" data-parameters="true" />
                <figcaption>
                  <span>フィードバックのイメージ</span>
                  <button type="button" id="button-feedback-animation">start</button>
                </figcaption>
              </figure>
            </section>
            <section id="section-effectors-delay-dry-and-wet">
              <h5>Dry / Wet</h5>
              <p>
                <b>Dry</b> / <b>Wet</b> とは, <b>原音とエフェクト音のゲインを調整するパラメータ</b> (または, そのような機能) のことです.
                現実世界のエフェクターにおいても, Dry (原音) / Wet (エフェクト音) としてコントロール可能になっているものが多いので,
                このドキュメントでもそれに従って Dry / Wet (あるいは, それらを合わせる Mix) と呼ぶことにします.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { delayTime: 0.5 });

// If use `createDelay`
// const delay = context.createDelay();
// delay.delayTime.value = 0.5;

const dry      = new GainNode(context, { gain: 0.7 });  // for gain of original sound
const wet      = new GainNode(context, { gain: 0.3 });  // for gain of delay sound
const feedback = new GainNode(context, { gain: 0.5 });  // for feedback

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(wet);
wet.connect(context.destination);

// Connect nodes for feedback
// (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
delay.connect(feedback);
feedback.connect(delay);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
              <p>
                上記のコードは, Dry / Wet のための <code>GainNode</code> を接続して, ディレイの実装完成コードです. Dry / Wet の制御を可能にすることで,
                原音とエフェクト音のゲインを調整するだけで, ディレイにさらなるバリエーションが生まれます. さらに, ノード接続を変更することなくエフェクターのオン
                / オフを切り替えることが可能になります. 例えば, Dry を <code>1</code>, Wet を <code>0</code> にすれば, エフェクターオフ (つまり, 原音のみ)
                のサウンドになります.
              </p>
              <figure>
                <svg id="svg-figure-node-connections-for-delay" width="1200" height="520" />
                <figcaption>ディレイのノード接続図</figcaption>
              </figure>
              <p>
                フィードバックと同様に, Dry / Web (あるいは, Mix) のパラメータ制御は, ディレイだけではなく, 他のエフェクターでも利用されるので, まずは,
                ノード接続が比較的単純なディレイの実装でそれらを理解しておくとよいでしょう.
              </p>
            </section>
            <article id="section-effectors-delay-ring-buffer">
              <h5>遅延音の実装とリングバッファ</h5>
              <p>
                ディレイはエフェクター実装における基本を理解するためにも最適なエフェクターですが, ディレイのコアとなる,
                遅延音はどのようにして実装するのでしょうか ? (Web Audio API では,
                <code>DelayNode</code> がそれを抽象化しているので気にすることはほとんどないと思いますが). 結論的には,
                <b>リングバッファ</b>に入力された音を格納する (enqueue) ことで, 過去の入力音をリングバッファのサイズだけ蓄積する事が可能です (<code
                  >DelayNode</code>
                コンストラクタで指定する遅延時間の最大値は, このリングバッファのサイズを決定するためにあります). 指定した
                <code>delayTime</code> の値が経過したら, リングバッファに格納した過去の入力音を, 原音 (現在時刻の音) と合成して出力します.
                加算・乗算・遅延はデジタルフィルタを構成する基本処理なので, 遅延に関しても,
                ローレイヤーでどのように実装されているかを理解しておくと応用が利くはずです. 実装言語は C++ になりますが, より詳細な解説は,
                <a href="https://www.utsbox.com/?p=1517" target="_blank" rel="noopener noreferrer">ディレイの実装例 | C++でVST作り</a>を参考にするとよいと思います.
              </p>
            </article>
          </section>
          <section id="section-effectors-reverb">
            <h4>リバーブ</h4>
            <p>
              簡易的なリバーブであれば, ディレイのパラメータを適切に設定することで実装することは可能です. しかしながら, 現実世界のエフェクターのリバーブは,
              ディレイと実装は異なり, シミュレートしたい<b>音響空間のインパルス応答</b> (<b>RIR</b>: <b>Room Impulse Response</b>)
              と呼ばれるオーディオデータを利用します. インパルス応答の詳細に関しては, 後半のセクションで解説しますので, とりあえず,
              リバーブを実装してみましょう.
            </p>
            <section id="section-effectors-reverb-convolver-node">
              <h5>ConvolverNode</h5>
              <p>
                インパルス応答のオーディオデータをエフェクターとして利用するには, <b><code>ConvolverNode</code></b> を利用します. <code>ConvolverNode</code> は,
                <b>コンボリューション積分</b> (<b>畳み込み積分</b>, <b>合成積</b>) という数学的な演算を抽象化する <code>AudioNode</code> です
                (デジタルフィルタの視点では, <b>FIR フィルタ</b>を抽象化します. のちほど解説しますが, 見立ての違いであり, 本質的には, コンボリューション積分も
                FIR フィルタも同じです. コンボリューション積分も FIR フィルタも次のセクション以降で解説しています).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const convolver = new ConvolverNode(context);

// If use `createConvolver`
// const convolver = context.createConvolver();</code></pre>
              <img src="images/convolver-node.png" alt="ConvolverNode" width="1232" height="770" loading="lazy" />
              <p>
                <code>ConvolverNode</code> には, <b><code>buffer</code></b> プロパティが定義されており, この <code>buffer</code> プロパティに, インパルス応答
                (RIR) のオーディオデータの <code>AudioBuffer</code> インスタンスを設定します (コンストラクタ生成であれば,
                <b><code>ConvolverOptions</code></b> の <code>buffer</code> プロパティで設定することも可能です).
              </p>
              <p><code>AudioBuffer</code> インスタンスの生成は, <a href="#section-create-audio-buffer">ワンショットオーディオ</a>と同じです.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/rirs/rir.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const convolver = new ConvolverNode(context);

      // If use `ConvolverOptions`
      // const convolver = new ConvolverNode(context, { buffer: audioBuffer });

      // If use `createConvolver`
      // const convolver = context.createConvolver();

      convolver.buffer = audioBuffer;
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                サウンドの入力点となるノード (<code>OscillatorNode</code> など) を <code>ConvolverNode</code> に接続して, <code>ConvolverNode</code> を
                <code>AudioDestinationNode</code> に接続します.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

fetch(&apos;./assets/rirs/rir.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const convolver = new ConvolverNode(context);

      // If use `ConvolverOptions`
      // const convolver = new ConvolverNode(context, { buffer: audioBuffer });

      // If use `createConvolver`
      // const convolver = context.createConvolver();

      convolver.buffer = audioBuffer;

      const dry = new GainNode(context, { gain: 0.6 });  // for gain of original sound
      const wet = new GainNode(context, { gain: 0.4 });  // for gain of reverb sound

      const oscillator = new OscillatorNode(context);

      // Connect nodes for original sound
      // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
      oscillator.connect(dry);
      dry.connect(context.destination);

      // Connect nodes for reverb sound
      // OscillatorNode (Input) -&gt; ConvolverNode (Reverb) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
      oscillator.connect(convolver);
      convolver.connect(wet);
      wet.connect(context.destination);

      oscillator.start(0);
      oscillator.stop(context.currentTime + 5);
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <p>
                ディレイの場合と同じように, 原音とエフェクト音の接続, そして, Dry / Wet の <code>GainNode</code> も接続しています.
                遅延音の生成処理と遅延音に対する演算処理は, <code>ConvolverNode</code> が抽象化しているので, フィードバックのための接続は不要です.
              </p>
              <figure>
                <svg id="svg-figure-node-connections-for-reverb" width="800" height="520" />
                <figcaption>リバーブのノード接続図</figcaption>
              </figure>
              <p>
                以上で, リバーブが完成しました. ところで, リバーブを利用するためには, インパルス応答 (RIR) のオーディオファイルが必要です.
                楽曲やワンショットオーディオファイルはあっても, インパルス応答のオーディオデータをもっている人は少ないと思います.
                機材をもっていれば実際に測定するのも可能ですが, そこまでするのはちょっとめんどうです. となると, Web
                上で公開されているファイルを利用することになるのですが, 利用条件や著作権の関係から無償で自由に利用できるのは意外とありません. とりあえず, 1
                つ紹介するのは,
                <a href="http://legacy.spa.aalto.fi/projects/poririrs/" target="_blank" rel="noopener noreferrer"
                  >Concert Hall Impulse Responses - Pori, Finland</a>
                です. <a href="http://legacy.spa.aalto.fi/projects/poririrs/docs/readme.txt" target="_blank" rel="noopener noreferrer">readme.txt</a> の 5.
                Copyright のセクションに, 「<b
                  >The data are provided free for noncommercial purposes, provided the authors are cited when the data are used in any research application.</b>」と記載されているので, 非商用利用であれば, 自身が開発されている Web アプリケーションに利用しても問題なさそうです.
              </p>
            </section>
            <section id="section-effectors-reverb-rir">
              <h5>インパルス応答</h5>
              <p>
                インパルス応答を理解するには, まずは, <b>インパルス音</b>について理解する必要があります. インパルス音とは,
                ごく短時間において瞬間的に発生する音です. 具体的には, ピストルの音や風船が破裂するときの音がインパルス音と言えるでしょう.
                インパルス音のイメージは以下のグラフのようになります. このような物理現象を数式でモデリングするための最適な関数が, <b>デルタ関数</b>です.
              </p>
              <figure>
                <svg id="svg-figure-impulse" width="720" height="405" data-parameters="true" />
                <figcaption>インパルス音 (デルタ関数) のグラフ表現</figcaption>
              </figure>
              <p>
                以下は, デルタ関数の定義です. デルタ関数は, <span class="math-inline">$t = t_{r}$</span> において, 横幅が
                <span class="math-inline">$\lim_{\mathrm{w}\to 0}$</span>, 高さが <span class="math-inline">$\lim_{\mathrm{h}\to \infty}$</span> となる関数
                (数学での関数をより一般化した, <b>超関数</b>に分類されます) なので, 無限の区間において積分すると, その値 (つまり, 面積) が
                <code>1</code> となります.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &\delta\left(t - t_{r}\right)dt =
                    \begin{cases}
                      \infty & (\mathrm{if} \quad t = t_{r}) \\
                      0 & (\mathrm{if} \quad t \neq t_{r})
                    \end{cases}
                  \end{flalign}
                $
              </div>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &\int_{-\infty}^{\infty}\delta\left(t - t_{r}\right)dt = 1 \\
                  \end{flalign}
                $
              </div>
              <p>
                インパルス音を室内で発生させると, 音が天井や壁などに反射して, 音の響き, すなわち, <b>残響</b>が発生します. カラオケが好きなかたであれば,
                <b>エコー</b>のような効果と考えてもよいでしょう.
              </p>
              <figure>
                <img src="images/reverb.png" alt="" width="800" height="600" loading="lazy" />
                <figcaption>直接音以外にも反射音 (初期反射音) や何度も反射して聴こえる残響音 (後期残響音) が室内では発生します</figcaption>
              </figure>
              <figure>
                <svg id="svg-animation-impulse-response" width="720" height="405" data-parameters="true" />
                <figcaption>
                  <span>インパルス応答のイメージ</span>
                  <button type="button" id="button-impulse-response-animation">start</button>
                </figcaption>
              </figure>
              <p>
                これが, <b>インパルス応答</b>です. つまり, <b>インパルス音を音響空間に対する入力としたときの出力 (応答) </b>ということです. そして, その出力が<b
                  >音響空間における, 残響特性を表すことになるので</b>, インパルス応答を利用することによって,
                コンサートホールなどの音響空間の音の響きをシミュレートするエフェクターであるリバーブが実現できるというわけです. ちなみに,
                室内でのインパルス応答をシミュレートする事が多いので, <b>RIR</b> (Room Impulse Response) と表現されることもあります.
              </p>
              <figure>
                <svg id="svg-rir" width="720" height="405" data-parameters="true" data-a="1" />
                <figcaption>
                  <span>実際の RIR の波形</span>
                  <button type="button" id="button-rir">start</button>
                </figcaption>
              </figure>
              <p>
                また, この残響が, ディレイにおけるフィードバックと類似した音響効果を発生させることになります
                (フィードバックのアニメーションとインパルス応答のアニメーションが類似していることに気付いたかもしれません).
              </p>
              <p>
                イメージでインパルス応答は理解できるかと思いますが, それをコンピュータで実現する場合, 数式でモデリングできる必要があります.
                <code>ConvolverNode</code> の命名 (<b>Convolve</b>: 畳み込む) が表すように, その演算処理が<b>コンボリューション積分</b> (<b>畳み込み積分</b>,
                <b>合成積</b>) です. 言い換えると, <code>ConvolverNode</code> は, コンボリューション積分を抽象化した <code>AudioNode</code> です.
              </p>
            </section>
          </section>
          <section id="section-effectors-delay-and-reverb-convolution">
            <h4>コンボリューション積分</h4>
            <p>
              <b>コンボリューション積分</b>とは, 以下の数式で定義されるように, 信号の遅延と乗算, それらの無限区間の積分によって構成されています.
              <span class="math-inline">$x\left(t\right)$</span> は入力信号, <span class="math-inline">$y\left(t\right)$</span> は出力信号,
              <span class="math-inline">$h\left(t_{r}\right)$</span> は, インパルス応答の信号 (<code>ConvolverNode</code> の
              <code>buffer</code> プロパティに設定する, <code>AudioBuffer</code> のオーディオデータと考えてもよいでしょう) です.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(t\right) = \int_{0}^{\infty}x\left(t - t_{r}\right)h\left(t_{r}\right)dt
              \end{flalign}
              $
            </div>
            <p>
              コンピュータでは, 連続信号をあつかうことはできないので, サンプリングされた離散信号の遅延と乗算, それらの加算となります. また,
              無限の加算はできないので, 有界な値 (サンプル数) <span class="math-inline">$N$</span> となります.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right)
              \end{flalign}
              $
            </div>
            <p>具体的に理解するために, <span class="math-inline">$N = 5$</span> として, 展開してみます.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(n\right) = x\left(n\right)h\left(0\right) + x\left(n - 1\right)h\left(1\right) + x\left(n - 2\right)h\left(2\right) + x\left(n - 3\right)h\left(3\right) + x\left(n - 4\right)h\left(4\right) + x\left(n - 5\right)h\left(5\right)
              \end{flalign}
              $
            </div>
            <p>
              理解しやすいように, 入力信号 <span class="math-inline">$x\left(n\right)$</span> は, 振幅が <code>1</code> のパルス列とします. また,
              出力信号の実際の値を算出するために, インパルス応答の信号は, 以下の値とします.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &h\left(0\right) = 1.0    \\
                &h\left(1\right) = 0.75   \\
                &h\left(2\right) = 0.5    \\
                &h\left(3\right) = 0.25   \\
                &h\left(4\right) = 0.125  \\
                &h\left(5\right) = 0.0625 \\
              \end{flalign}
              $
            </div>
            <p>これらの信号のコンボリューション積分をイラストにすると, 以下のようなグラフになります.</p>
            <figure>
              <svg id="svg-figure-convolution" width="720" height="600" />
              <figcaption>コンボリューション積分</figcaption>
            </figure>
            <p>
              上部が入力信号のパルス列 (<span class="math-inline">$x\left(n\right)$</span>), 中央がインパルス応答 (<span class="math-inline"
                >$h\left(m\right)$</span>), 下部がコンボリューション積分結果の出力信号 (<span class="math-inline">$y\left(n\right)$</span>) となります.
              出力信号の振幅のみスケールが異なることに注意してください. ここで, <span class="math-inline">$n = 3$</span> に相当する時刻までの値を,
              先ほどの展開したコンボリューション積分に適用して実際の値を算出すると,
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = x\left(0\right)h\left(0\right) + x\left(-1\right)h\left(1\right) + x\left(-2\right)h\left(2\right) + x\left(-3\right)h\left(3\right) + x\left(-4\right)h\left(4\right) + x\left(-5\right)h\left(5\right) \\
                &y\left(1\right) = x\left(1\right)h\left(0\right) + x\left(0\right)h\left(1\right) + x\left(-1\right)h\left(2\right) + x\left(-2\right)h\left(3\right) + x\left(-3\right)h\left(4\right) + x\left(-4\right)h\left(5\right) \\
                &y\left(2\right) = x\left(2\right)h\left(0\right) + x\left(1\right)h\left(1\right) + x\left(0\right)h\left(2\right) + x\left(-1\right)h\left(3\right) + x\left(-2\right)h\left(4\right) + x\left(-3\right)h\left(5\right) \\
                &y\left(3\right) = x\left(3\right)h\left(0\right) + x\left(2\right)h\left(1\right) + x\left(1\right)h\left(2\right) + x\left(0\right)h\left(3\right) + x\left(-1\right)h\left(4\right) + x\left(-2\right)h\left(5\right) \\
              \end{flalign}
              $
            </div>
            <p>負数に相当する時刻は, 振幅が <code>0</code> とみなせるので, <span class="math-inline">$n \geq 0$</span> の項のみ記述すると,</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = x\left(0\right)h\left(0\right) \\
                &y\left(1\right) = x\left(1\right)h\left(0\right) + x\left(0\right)h\left(1\right) \\
                &y\left(2\right) = x\left(2\right)h\left(0\right) + x\left(1\right)h\left(1\right) + x\left(0\right)h\left(2\right) \\
                &y\left(3\right) = x\left(3\right)h\left(0\right) + x\left(2\right)h\left(1\right) + x\left(1\right)h\left(2\right) + x\left(0\right)h\left(3\right) \\
              \end{flalign}
              $
            </div>
            <p>また, 入力信号は, 振幅 <code>1</code> のパルス列なので, <span class="math-inline">$x\left(n\right) = 1$</span> となるので,</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = h\left(0\right) \\
                &y\left(1\right) = h\left(0\right) + h\left(1\right) \\
                &y\left(2\right) = h\left(0\right) + h\left(1\right) + h\left(2\right) \\
                &y\left(3\right) = h\left(0\right) + h\left(1\right) + h\left(2\right) + h\left(3\right) \\
              \end{flalign}
              $
            </div>
            <p>
              最後に, <span class="math-inline">$h\left(m\right)$</span> の実際の値を適用すれば, 出力信号 <span class="math-inline">$y\left(n\right)$</span> の
              <span class="math-inline">$n = 3$</span> までの値が算出できます.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
              \begin{flalign}
                &y\left(0\right) = 1.0 \\
                &y\left(1\right) = 1.0 + 0.75 = 1.75 \\
                &y\left(2\right) = 1.0 + 0.75 + 0.5 = 2.25 \\
                &y\left(3\right) = 1.0 + 0.75 + 0.5 + 0.25 = 2.5 \\
              \end{flalign}
              $
            </div>
            <p>
              目視レベルではありますが, グラフで表示されている出力信号と (大きく) 値の相違がないことが確認できます. 計算結果を抽象化すると,
              コンボリューション積分で生成された出力信号は, <b>現在の時刻の信号だけではなく, 過去の信号の影響も受ける</b>ということですということです (逆に,
              それを, 厳密に数式で定義したのがコンボリューション積分と言えます).
            </p>
            <figure>
              <svg id="svg-animation-convolution" width="720" height="600" />
              <figcaption>
                <span><span class="math-inline">$N = 5$</span>, <span class="math-inline">$n = 3$</span> の場合の, コンボリューション積分のイメージ</span>
                <button type="button" id="button-convolution-animation">start</button>
              </figcaption>
            </figure>
            <p>
              入力信号やインパルス応答が, 現実世界のようにより複雑になると, 計算自体も複雑になりますが, コンボリューション積分がどうのような演算か, そして,
              <b><code>ConvolverNode</code> が抽象化している演算</b>のイメージを理解するのに役立てばと思います.
            </p>
            <p>
              もっとも, 数学・物理的に理解しようとすると少し難しく感じるかもしれませんが, プログラミング言語で実装すれば, 2
              重ループと積和演算で構成される単純な処理です.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// コンボリューション積分のコード片

const X_L = 2400;
const H_L = 1200;
const Y_L = X_L + H_L - 1;

const x = new Float32Array(X_L);
const h = new Float32Array(H_L);
const y = new Float32Array(Y_L);

// 入力信号
for (let n = 0; n &lt; X_L; n++) {
  x[n] = Math.sin((2 * Math.PI * n * 2) / 1200);
}

// インパルス応答
for (let m = 0; m &lt; H_L; m++) {
  h[m] = 0.5 * Math.exp(-m);
}

// 出力信号
for (let n = 0; n &lt; Y_L; n++) {
  for (let m = 0; m &lt; H_L; m++) {
    if (((n - m) &gt;= 0) &amp;&amp; ((n - m) &lt; X_L)) {
      y[n] += (x[n - m] * h[m]);
    }
  }
}</code></pre>
            <p>
              ただし, コンボリューション積分した出力信号のサイズ <span class="math-inline">$Y_{L}$</span> は, 入力信号のサイズを
              <span class="math-inline">$X_{L}$</span>, インパルス応答のサイズを <span class="math-inline">$H_{L}$</span> とすると,
              <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> (つまり, 出力信号の配列 (コレクション) の末尾のインデックスは,
              <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span>) となることに注意してください. 現在時刻以降の入力信号が
              <code>0</code> (無音) でも, コンボリューション積分によって過去の信号の積和演算の影響を受けるからです.
              信号の末尾をイラストにイメージすると理解しやすいと思います.
            </p>
            <figure>
              <svg id="svg-figure-convolution-output-signal-size" width="720" height="600" />
              <figcaption>コンポリューション積分と出力信号のサイズ (末尾部分)</figcaption>
            </figure>
            <p>
              上記のイラストは, インパルス応答のサイズを <code>3</code> (<span class="math-inline">$H_{L} = 3$</span>) とした場合の, 出力信号の末尾の値
              (インデックス <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span> の値) の算出イメージです.
              過去の入力信号の積和演算の影響で, 出力信号のサイズが <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> となることが確認できると思います
              (出力信号の末尾から <span class="math-inline">$j$</span> 手前のインデックス
              <span class="math-inline">$I_{\mathrm{max} - j} = (X_{L} + H_{L} - 1) - (1 + j)$</span> の値も同様に,
              過去の入力信号の積和演算によって算出されます). そして, インデックス
              <span class="math-inline">$I_{\mathrm{max}} = (X_{L} + H_{L} - 1) - 1$</span> より後の時刻に相当するインデックスにおいては,
              過去の入力信号の積和演算の値が常に <code>0</code> (無音) となるので, 出力信号のサイズは,
              <span class="math-inline">$Y_{L} = X_{L} + H_{L} - 1$</span> より大きいサイズにはならないことも確認できると思います.
            </p>
            <article id="section-effectors-delay-and-reverb-cyclic-convolution">
              <h5>巡回畳み込み</h5>
              <p>
                コンボリューション積分は, <b>周波数領域においては乗算</b>となります. したがって, 時間領域の入力信号
                <span class="math-inline">$x\left(n\right)$</span> を (離散) フーリエ変換した信号を <span class="math-inline">$X\left(k\right)$</span>,
                インパルス応答を (離散) フーリエ変換した信号を <span class="math-inline">$H\left(k\right)$</span>, 出力信号
                <span class="math-inline">$y\left(n\right)$</span> を (離散) フーリエ変換した信号を
                <span class="math-inline">$Y\left(k\right)$</span> と定義すると, コンボリューション積分は以下のように定義することもできます,
                <span class="math-inline">$F$</span> はフーリエ変換 (離散フーリエ変換), <span class="math-inline">$F^{-1}$</span> は逆フーリエ変換
                (逆離散フーリエ変換) です.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right) = F^{-1}\left[Y\left(k\right)\right] = F^{-1}\left[X\left(k\right)H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                この性質を利用して, 時間領域ではコンボリューション積分になるオーディオ信号処理を, 周波数領域に変換して, 乗算のみで信号処理を適用して,
                時間領域に変換する<b>巡回畳み込み</b>という処理 (ある種のテクニック的な処理です) があります.
              </p>
              <p>
                インパルス応答も数学的には, デルタ関数のフーリエ変換と周波数領域で定義される<b>伝達関数</b>の乗算の逆フーリエ変換で定義することもできます (以下,
                簡単にですが, 導出を記載しておきます).
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &h\left(n\right) = \sum_{m = 0}^{\infty}\delta\left(n - m\right)h\left(m\right) = F^{-1}\left[F\left[\delta\left(n\right)\right]H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                ここで, デルタ関数のフーリエ変換を導出すると (導出の理解が難しければ, とりあえず
                <span class="math-inline">$F\left[\delta\left(t\right)\right] = 1$</span> と覚えてしまっていいでしょう. 他に覚えやすいフーリエ変換関係だと,
                矩形関数とシンク関数 (<span class="math-inline">$\frac{\sin\left(x\right)}{x}$</span>) は, 互いにフーリエ変換の関係にあります),
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &F\left[\delta\left(t\right)\right] = \int_{-\infty}^{\infty}\delta\left(t\right)e^{-j2 \pi ft}dt
                  \end{flalign}
                $
              </div>
              <p>
                デルタ関数の定義より, 上記のフーリエ変換において, <span class="math-inline">$t = 0$</span> 以外の積分区間では
                <span class="math-inline">$\delta\left(t\right) = 0$</span> となるので,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &F\left[\delta\left(t\right)\right] = \int_{-\infty}^{\infty}\delta\left(0\right)e^{-j2 \pi f \cdot 0}dt = \int_{-\infty}^{\infty}\delta\left(0\right)dt \cdot e^{0} = 1 \cdot 1 = 1
                  \end{flalign}
                $
              </div>
              <p>
                (数学的な厳密性は欠いてしまいますが, 同じように離散信号にも適用すると) デルタ関数のフーリエ変換は <code>1</code> です. つまり,
                <span class="math-inline">$F\left[\delta\left(n\right)\right] = 1$</span> なので,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &h\left(n\right) = \sum_{m = 0}^{\infty}\delta\left(n - m\right)h\left(m\right) = F^{-1}\left[F\left[\delta\left(n\right)\right]H\left(k\right)\right] = F^{-1}\left[H\left(k\right)\right]
                  \end{flalign}
                $
              </div>
              <p>
                Web Audio API は (他のオーディオ API と比較すると) 抽象度が高いので, 巡回畳み込みまで駆使するケースはほとんどないかもしれませんが,
                一応知っておくと実装のヒントになることがあるかもしれません.
              </p>
            </article>
          </section>
          <section id="section-effectors-delay-and-reverb-fir-filter">
            <h4>FIR フィルタ</h4>
            <p>
              デジタルフィルタを数学的な厳密性まで含めて解説すると, それだけで 1 冊の書籍になるぐらいの解説になるので, FIR
              フィルタをディレイ・リバーブの観点で解説します.
            </p>
            <p><b>FIR フィルタ</b> (<b>Finite Impulse Response filter</b>) は, 以下の数式で定義されるデジタルフィルタです.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &y\left(n\right) = \sum_{m = 0}^{N}x\left(n - m\right)h\left(m\right)
                \end{flalign}
              $
            </div>
            <p>
              数式的には, コンボリューション積分と同じです. すなわち, <b>見立ての違い</b>でしかありません. 数学的にはコンボリューション積分, 工学的には FIR
              フィルタと表現しています. 言い換えると, コンボリューション積分を実装に落とし込んだのが FIR フィルタということです.
            </p>
            <p>
              具体的に, <span class="math-inline">$N = 3$</span> (乗算器の数 <span class="math-inline">$N + 1$</span>)
              の加算器・乗算器・遅延器の要素を利用してブロック図として表現します (遅延器の <span class="math-inline">$z^{-1}$</span> の表記は
              <b><span class="math-inline">$z$</span> 変換</b>に由来しますが, ブロック図としては, 1 サンプル分遅延させる要素 (素子) の理解で問題ありません).
            </p>
            <figure>
              <svg id="svg-figure-fir-filter" width="720" height="520" />
              <figcaption>FIR フィルタ</figcaption>
            </figure>
            <p>FIR フィルタの図から, 逆に, コンボリューション積分の数式を導出することが可能なこともわかります.</p>
            <p>
              乗算器の数 (遅延器の数もそれに比例) と係数は, ディレイは制御可能なパラメータで決定できますが, リバーブは室内の特性によって決定されます. すなわち,
              <b>乗算器の数と係数の算出がディレイとリバーブの実装に違いに表れます</b>.
              <b>ディレイは遅延時間とフィードバックよって乗算器の数と係数を決定する</b>のに対して,
              <b>リバーブはインパルス応答 (RIR) のオーディオデータから乗算器の数と係数が決定されます</b>.
            </p>
            <article id="section-effectors-delay-and-reverb-schroeder-reverberator">
              <h5>Schroeder Reverberator (シュレーダーリバーブ)</h5>
              <p>
                コムフィルタ (遅延音を生成) と All-Pass Filter (位相変化によって, 遅延音の間を補間して残響音の密度を高める) を駆使して, 人工的にリバーブ
                (人工インパルス応答) を生成する実装が知られています. 実装言語は Python になりますが,
                <a href="https://www.wizard-notes.com/entry/asp/schroeder-reverberator" target="_blank" rel="noopener noreferrer"
                  >シュレーダーリバーブ（人工残響エフェクタ）のPython実装と試聴デモ</a>などが参考になります.
              </p>
            </article>
          </section>
        </section>
        <section id="section-effectors-chorus-and-flanger">
          <h3>コーラス・フランジャー</h3>
          <p>
            <b>コーラス</b>は, 音に揺らぎを与えるエフェクターです. 合唱では, どんなに歌唱力の高い人が集まって歌っても多少なりともピッチのずれは生じてしまいます.
            しかし, この微妙なずれが合唱らしさを生み出している要因でもあります. コーラスでは, この微妙なピッチのずれをオーディオ信号処理で再現します.
          </p>
          <p><b>フランジャー</b>は, ジェット機のエンジン音のように, 音に強烈なうねりを与えるエフェクターです.</p>
          <p>
            コーラスとフランジャーは, エフェクターとしてはまったく異なるように感じます. 現実世界のエフェクターでも,
            コーラスとフランジャーはそれぞれ別に存在していますが, その原理は同じです. <b>ディレイタイム (遅延時間) を周期的に変化させる</b>ことによって,
            <b>FM 変調</b>を発生させている点です. 原理が同じであるにも関わらず, 別のエフェクターとして感じるのは,
            パラメータの設定値やフィードバックの有無が影響しています (また, 音楽的に目的が異なるので, それぞれ,
            コーラス・フランジャーとして別になっていると思います).
          </p>
          <p>
            コーラス・フランジャーはディレイが基本となっているので, ディレイの実装がよくわからないという場合は, (前のセクション解説しているので)
            ディレイの実装を理解してから, このセクションを進めてください.
          </p>
          <section id="section-effectors-chorus">
            <h4>コーラス</h4>
            <p>
              コーラスは, <b>ディレイタイムを周期的に変化させたエフェクト音</b>を原音とミックスすることにより実装できます.
              ディレイタイムを周期的に変化させることが実装のポイントになりますが, ここで LFO を利用することで, ディレイタイムを周期的に変化させることができます.
              つまり, Web Audio API においては, LFO の接続先を <code>DelayNode</code> の AudioParam である
              <code>delayTime</code> プロパティに接続すれば実装完了です.
            </p>
            <p>まず, 原音の出力の接続と, エフェクト音の出力の接続 (<code>DelayNode</code> の接続) のみを実装します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const delay = new DelayNode(context, { delayTime: 0.02 });

const oscillator = new OscillatorNode(context);

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
            <p>
              そして, <a href="#section-effectors-lfo">LFO</a> の実装で解説したように, LFO のための <code>OscillatorNode</code> インスタンスと
              <code>GainNode</code> インスタンス (Depth パラメータ) を生成して, <b><code>DelayNode</code> の <code>delayTime</code> プロパティ</b> に接続します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseDelayTime = 0.020;
const depthValue    = 0.005;
const rateValue     = 1;

const delay = new DelayNode(context, { delayTime: baseDelayTime });

const oscillator = new OscillatorNode(context);

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(context.destination);

// Connect nodes for LFO that changes delay time periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
lfo.connect(depth);
depth.connect(delay.delayTime);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              ディレイのノード接続からフィードバックを除いて, LFO を <code>DelayNode</code> の <code>delayTime</code> プロパティ (<code>AudioParam</code>)
              に接続したノード接続と同じです. パラメータに関しては, コーラスの場合, 基準となるディレイタイムを <code>20 - 30 msec</code> にして,
              <code>&plusmn; 5 ~ 10 msec</code>, Rate はゆっくりと <code>1 Hz</code> ぐらいがよいでしょう. (もっとも, 実際のプロダクトでは,
              ある程度自由度高く設定できるように, <a href="#section-effectors-general-vibrato">汎用的な LFO</a> になるように実装することになるでしょう).
            </p>
            <p>
              コーラスの原理的な実装としてはこれで完了ですが, エフェクターとしてはまだコーラスっぽくありません.
              原音とエフェクト音が同じゲインで合成されているので, 原音とエフェクト音が別々に出力されているように聴こえると思います.
              原音が少し揺れているぐらいにエフェクト音を合成するとコーラスらしくなるので, Dry / Wet のための <code>GainNode</code> を接続して,
              原音とエフェクト音のゲインを調整します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseDelayTime = 0.020;
const depthValue    = 0.005;
const rateValue     = 1;

const delay = new DelayNode(context, { delayTime: baseDelayTime });

const oscillator = new OscillatorNode(context);

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

const dry = new GainNode(context, { gain: 0.7 });  // for gain of original sound
const wet = new GainNode(context, { gain: 0.3 });  // for gain of chorus sound

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for delay sound
// OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(delay);
delay.connect(wet);
wet.connect(context.destination);

// Connect nodes for LFO that changes delay time periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
lfo.connect(depth);
depth.connect(delay.delayTime);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <figure>
              <svg id="svg-figure-node-connections-for-chorus" width="1200" height="520" />
              <figcaption>コーラスのノード接続図</figcaption>
            </figure>
            <p>
              以上で, コーラスの実装は完了です. ハードコーディングしているパラメータが多いので, ある程度実際のアプリケーションを想定して, UI
              からパラメータ設定を可能にすると以下のようなコードとなるでしょう (Dry / Wet は同時に設定する Mix としています).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-chorus-delay-time&quot;&gt;Delay time&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-delay-time&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-chorus-delay-time-value&quot;&gt;0 msec&lt;/span&gt;
&lt;label for=&quot;range-chorus-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-chorus-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-chorus-mix&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-chorus-mix&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-chorus-mix-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;

let depthRate  = 0;
let rateValue  = 0;
let mixValue   = 0;

const delay = new DelayNode(context);
const depth = new GainNode(context, { gain: delay.delayTime.value * depthRate });
const dry   = new GainNode(context, { gain: 1 - mixValue });
const wet   = new GainNode(context, { gain: mixValue });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDelayTimeElement = document.getElementById(&apos;range-chorus-delay-time&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-chorus-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-chorus-rate&apos;);
const rangeMixElement       = document.getElementById(&apos;range-chorus-mix&apos;);

const spanPrintDelayTimeElement = document.getElementById(&apos;print-chorus-delay-time-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-chorus-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-chorus-rate-value&apos;);
const spanPrintMixElement       = document.getElementById(&apos;print-chorus-mix-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  // Connect nodes for original sound
  // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
  oscillator.connect(dry);
  dry.connect(context.destination);

  // Connect nodes for delay sound
  // OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
  oscillator.connect(delay);
  delay.connect(wet);
  wet.connect(context.destination);

  // Connect nodes for LFO that changes delay time periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
  lfo.connect(depth);
  depth.connect(delay.delayTime);

  // Start oscillator and LFO immediately
  oscillator.start(0);
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDelayTimeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  delay.delayTime.value = event.currentTarget.valueAsNumber * 0.001;
  depth.gain.value      = delay.delayTime.value * depthRate;

  spanPrintDelayTimeElement.textContent = `${Math.trunc(delay.delayTime.value * 1000)} msec`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = delay.delayTime.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeMixElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  mixValue = event.currentTarget.valueAsNumber;

  dry.gain.value = 1 - mixValue;
  wet.gain.value = mixValue;

  spanPrintMixElement.textContent = mixValue.toString(10);
});</code></pre>
            <div class="app-container app-chorus">
              <button type="button" id="button-chorus">start</button>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-chorus-delay-time">Delay time</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-delay-time" value="0" min="0" max="50" step="1" />
                      <span id="print-chorus-delay-time-value">0 msec</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-rate" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-rate-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-chorus-mix">Mix</label></dt>
                    <dd>
                      <input type="range" id="range-chorus-mix" value="0" min="0" max="1" step="0.05" />
                      <span id="print-chorus-mix-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-flanger">
            <h4>フランジャー</h4>
            <p>
              <b>フランジャー</b>は, コーラスの実装にエフェクト音のフィードバックの接続を追加するだけです (ディレイの実装に LFO を追加して,
              ディレイタイムを周期的に変化させるとも言えます). つまり, コーラスの実装をより汎用的にした実装となります. パラメータしだいで, フランジャーになり,
              コーラスにもなります. 原理的には, 同じなのでこのあたりの区別は, 音楽的な感覚による違いでしかありません.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-flanger" width="1200" height="520" />
              <figcaption>フランジャーのノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-flanger-delay-time&quot;&gt;Delay time&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-delay-time&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;50&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-flanger-delay-time-value&quot;&gt;0 msec&lt;/span&gt;
&lt;label for=&quot;range-flanger-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-flanger-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-mix&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-mix&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-mix-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-flanger-feedback&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-flanger-feedback&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;0.9&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-flanger-feedback-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;

let depthRate  = 0;
let rateValue  = 0;
let mixValue   = 0;

const delay    = new DelayNode(context);
const depth    = new GainNode(context, { gain: delay.delayTime.value * depthRate });
const dry      = new GainNode(context, { gain: 1 - mixValue });
const wet      = new GainNode(context, { gain: mixValue });
const feedback = new GainNode(context, { gain: 0 });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeDelayTimeElement = document.getElementById(&apos;range-flanger-delay-time&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-flanger-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-flanger-rate&apos;);
const rangeMixElement       = document.getElementById(&apos;range-flanger-mix&apos;);
const rangeFeedbackElement  = document.getElementById(&apos;range-flanger-feedback&apos;);

const spanPrintDelayTimeElement = document.getElementById(&apos;print-flanger-delay-time-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-flanger-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-flanger-rate-value&apos;);
const spanPrintMixElement       = document.getElementById(&apos;print-flanger-mix-value&apos;);
const spanPrintFeedbackElement  = document.getElementById(&apos;print-flanger-feedback-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  // Connect nodes for original sound
  // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
  oscillator.connect(dry);
  dry.connect(context.destination);

  // Connect nodes for delay sound
  // OscillatorNode (Input) -&gt; DelayNode (Delay) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
  oscillator.connect(delay);
  delay.connect(wet);
  wet.connect(context.destination);

  // Connect nodes for feedback
  // (OscillatorNode (Input) -&gt;) DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; DelayNode (Delay) -&gt; GainNode (Feedback) -&gt; ...
  delay.connect(feedback);
  feedback.connect(delay);

  // Connect nodes for LFO that changes delay time periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
  lfo.connect(depth);
  depth.connect(delay.delayTime);

  // Start oscillator and LFO immediately
  oscillator.start(0);
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDelayTimeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  delay.delayTime.value = event.currentTarget.valueAsNumber * 0.001;
  depth.gain.value      = delay.delayTime.value * depthRate;

  spanPrintDelayTimeElement.textContent = `${Math.trunc(delay.delayTime.value * 1000)} msec`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = delay.delayTime.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeMixElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  mixValue = event.currentTarget.valueAsNumber;

  dry.gain.value = 1 - mixValue;
  wet.gain.value = mixValue;

  spanPrintMixElement.textContent = mixValue.toString(10);
});

rangeFeedbackElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const feedbackValue = event.currentTarget.valueAsNumber;

  feedback.gain.value = feedbackValue;

  spanPrintFeedbackElement.textContent = feedbackValue.toString(10);
});</code></pre>
            <div class="app-container app-flanger">
              <button type="button" id="button-flanger">start</button>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-flanger-delay-time">Delay time</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-delay-time" value="0" min="0" max="50" step="1" />
                      <span id="print-flanger-delay-time-value">0 msec</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-flanger-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-rate" value="0" min="0" max="10" step="0.5" />
                      <span id="print-flanger-rate-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-mix">Mix</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-mix" value="0" min="0" max="1" step="0.05" />
                      <span id="print-flanger-mix-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-flanger-feedback">Feedback</label></dt>
                    <dd>
                      <input type="range" id="range-flanger-feedback" value="0" min="0" max="0.9" step="0.05" />
                      <span id="print-flanger-feedback-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-chorus-and-flanger-fm-modulation">
            <h4>ディレイタイムの周期的な変化とFM 変調</h4>
            <p><b>FM 変調</b> (<b>Frequency Modulation</b>) とは, 時間の経過とともに信号の周波数を変化させることです.</p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-frequency-modulation-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-frequency-modulation-spectrum" width="720" height="240" /></dd>
              </dl>
              <figcaption>
                <span>FM 変調のイメージ (スペクトルのピークが <code>880 Hz &plusmn; 440 Hz</code> の間で変調します)</span>
                <button type="button" id="button-frequency-modulation-animation">start</button>
              </figcaption>
            </figure>
            <p>
              コーラス・フランジャーは, 結果的に FM 変調を発生させていると解説しましたが, これに対して疑問に思うことがあるかもしれません.
              周波数を周期的に変化させるために, なぜ, ディレイタイムを周期的に変化させているのかということです
              (ディレイタイムの周期的な変化が原理となっているかということです). <a href="section-effectors-vibrato">LFO の実装例</a>で解説したように,
              直接的に周波数を周期的に変化させればよいはずです. しかしながら, 基本波形のように, 基本周波数が明確な場合はそれで問題ないのですが, アンサンブル
              (楽曲) や音声において, 一般的に, 基本周波数を (精度高く) 推定するアルゴリズム (<span class="math-inline">$f_{0}$</span> 推定)
              は複雑になってしまい, 計算量も多くなります. したがって, 汎用的なコーラス・フランジャーを実装するとなると,
              直接的に周波数を変化させることは難しくなります.
            </p>
            <p>
              ここで, コンボリューション積分とフーリエ変換の性質から,
              <b>時間領域における遅延は, 周波数領域においては周波数成分の変化となります</b> (数学的な詳細を知る必要はないですが,
              <a href="#section-effectors-delay-and-reverb-cyclic-convolution">巡回畳み込み</a>のセクションが参考になると思います). すなわち,
              <b>時間領域においてディレイタイムを周期的に変化させることは, 周波数領域において各周波数成分を周期的に変化させる</b>こととなり, 結果として
              (汎用的な) FM 変調となります.
            </p>
            <p>
              ちなみに, すでにサンプルコードを実行して気づいたかもしれませんが,
              <b>コーラス・フランジャーで, エフェクト音のみの出力にした場合, ビブラートとなります</b>. ビブラートはまさに FM 変調であり, 言い換えれば,
              コーラス・フランジャーは, ビブラートをベースにしたエフェクターであり, 汎用的な実装とパラメータ設定でビブラートにすることも可能ということです.
            </p>
          </section>
        </section>
        <section id="section-effectors-phaser">
          <h3>フェイザー</h3>
          <p>
            <b>フェイザー</b>は, (テキストでは表現しにくいですが) シュワシュワという独特な感じのエフェクトを与えます. パラメータの設定しだいでは,
            フランジャーっぽい感じにもなります. 実際, 楽曲を聴くと, フェイザーかフランジャーを使っているかは判断ができないぐらい似ているエフェクターです.
          </p>
          <p>
            フランジャーに似ているエフェクトでありながら, その原理はまったく異なります. フェイザーは, 特定の周波数帯域の音の<b>位相</b>を周期的に変化させて,
            原音と合成して, 音を<b>干渉</b>させることによって実装できるエフェクターです. ちなみに, フェイザーの正式な名称は, <b>フェイズ・シフター</b>であり,
            まさに, 名称がその原理を表していると言えます.
          </p>
          <section id="section-effectors-phaser-phase">
            <h4>位相</h4>
            <p>
              <b>位相</b>とは, 時間領域における波の位置のことです (数学的には, ある時刻の複素数平面における偏角と考えることもできます). したがって,
              周期性をもつ波の場合, 位相はその周期内の値だけを考慮すれば事足ります (正弦波の場合,
              <span class="math-inline"
                >$\sin\theta = \sin\left(\theta + 2\pi\right) = \sin\left(\theta + 4\pi\right) = \cdots = \sin\left(\theta + 2n\pi\right)$</span>
              となるので, <span class="math-inline">$2\pi$</span> の区間の位相を考慮すればよいことになります). また, 正弦波と余弦波は位相の違いでしかありません
              (<span class="math-inline">$\sin\theta = \cos\left(\theta - \frac{\pi}{2}\right)$</span>. これは, 位相で考えなくても,
              加法定理で数式から導出できることでもあります).
            </p>
            <figure>
              <svg id="svg-figure-phase" width="900" height="300" />
              <figcaption>位相 (例: <span class="math-inline">$\theta = \frac{\pi}{4}$</span> の場合)</figcaption>
            </figure>
            <p>位相と周期性から, 位相の変化をイメージすると, 横軸を位相 (単位は <code>radian</code>: ラジアン) としたときに, 以下のような変化になります.</p>
            <figure>
              <svg id="svg-animation-phase-shift" width="720" height="405" />
              <figcaption>
                <span>位相変化のイメージ</span>
                <button type="button" id="button-phase-shift-animation">start</button>
                <button type="button" id="button-phase-shift-half-pi" aria-label="+ π/2"><span class="math-inline">$+\frac{\pi}{2}$</span></button>
              </figcaption>
            </figure>
            <p>
              正弦波の場合, <span class="math-inline">$\frac{\pi}{2}$</span> シフトすると反転した余弦波,
              <span class="math-inline">$\pi$</span> シフトすると反転した自身の波 (反転した正弦波),
              <span class="math-inline">$\frac{3\pi}{2}$</span> シフトすると余弦波, <span class="math-inline">$2\pi$</span> シフトすると元の正弦波に戻ります.
            </p>
            <p>フェイザーの原理を理解するうえで, この位相変化のイメージは重要になるのでおさえておいてください.</p>
          </section>
          <section id="section-effectors-phaser-all-pass-filter">
            <h4>All-Pass Filter</h4>
            <p>
              フェイザーは, 位相を変化させる周波数帯域を周期的に変化させて, 原音と合成して音を干渉させることによって実装できます. つまり,
              <b>位相を変化させる</b>ことがフェイザーにとって重要な処理となりますが, <b>All-Pass Filter</b> を使うことで,
              対象の周波数成分の位相を変化させることができます (他のフィルタと異なり, すべての周波数成分のゲイン (振幅) を変化させずに通過させるので,
              <b>オールパス</b>という名前がついています).
            </p>
            <p>
              Web Audio API では, <code>BiquadFilterNode</code> インスタンスの <code>type</code> プロパティに <code>&apos;allpass&apos;</code> を設定することで,
              簡単に All-Pass Filter を使うことができます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const allpass    = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: 880 });

oscillator.connect(allpass);
allpass.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
          </section>
          <section id="section-effectors-phaser-interference">
            <h4>位相変化と干渉</h4>
            <p>
              ところで, All-Pass Filter を通過させた音, つまり, 位相を変化させただけの音というのは, 特に変化を知覚することはできません.
              <a href="#section-spectrum">スペクトル</a>のセクションでも記載しましたが, <b>人間の聴覚というのは位相の違いには鈍感</b>だからです. では, なぜ,
              フェイザーはエフェクトとして知覚することができるのでしょうか ?
            </p>
            <p>
              フェイザーは, 位相の変化を知覚しているのではなく, 位相を変化させたエフェクト音と原音を合成することで音波を<b>干渉</b>させて, 結果として発生する,
              振幅の増減や<b>うねり</b>を知覚しているからです.
            </p>
            <figure>
              <svg id="svg-animation-interference" width="720" height="405" />
              <figcaption>
                <span>位相変化と干渉のイメージ</span>
                <button type="button" id="button-interference-animation">start</button>
                <button type="button" id="button-interference-half-pi" aria-label="+ π/2"><span class="math-inline">$+\frac{\pi}{2}$</span></button>
              </figcaption>
            </figure>
            <p>
              上記の正弦波で説明すると, 開始時刻において, 原音とエフェクト音 (ともに, 半透明の青色の波) は, 同じ位相にあります. ここで,
              <span class="">$\theta = \frac{\pi}{2}$</span> の位相の点に着目すると, 同じ位相なので, 音波が重なり, 合成された結果, 出力音 (マゼンタ色の波) は
              <span class="math-inline">$0.5 \cdot \sin\left(\frac{\pi}{2}\right) + 0.5 \cdot \sin\left(\frac{\pi}{2}\right) = 1$</span>
              となって振幅が増幅します. エフェクト音の位相を <span class="math-inline">$\frac{\pi}{2}$</span> ずらすと, 反転した余弦波となり, 位相
              <span class="math-inline">$\theta = \frac{\pi}{2}$</span> でのエフェクト音は
              <span class="math-inline">$-0.5 \cdot \cos\left(\frac{\pi}{2}\right) = 0$</span> となるので, 合成された結果, 出力音の振幅値は,
              <span class="math-inline">$0.5 \cdot \sin\left(\frac{\pi}{2}\right) - 0.5\cdot \cos\left(\frac{\pi}{2}\right) = 0.5$</span> となります. さらに,
              エフェクト音の位相を <span class="math-inline">$\frac{\pi}{2}$</span> (開始時刻を基準にすると, <span class="math-inline">$\pi$</span>) ずらすと,
              反転した正弦波となり, 位相 <span class="math-inline">$\theta = \frac{\pi}{2}$</span> でのエフェクト音は
              <span class="math-inline">$-0.5 \cdot \sin\left(\frac{\pi}{2}\right) = -0.5$</span> となるので, 合成された結果の振幅値は,
              <span class="math-inline">$0.5 \cdot \sin\left(\frac{\pi}{2}\right) - 0.5 \cdot \sin\left(\frac{\pi}{2}\right) = 0$</span> となります (位相
              <span class="math-inline">$\theta = \frac{\pi}{2}$</span> に限らず, 反転した正弦波と合成するので, すべての位相においてその振幅は
              <code>0</code> であり, すなわち, 無音となります). そして, エフェクト音の位相を 1 周期分, つまり, <span class="math-inline">$2\pi$</span> ずらすと,
              エフェクト音は再び元の位相に戻るので, 開始時刻と同様の出力音となります.
            </p>
            <p>
              周期関数なので, 1 周期分以上の位相の変化 (<span class="">$2\pi$</span> 以上の位相の変化) においては, 同様の振幅の増減の現象が繰り返し発生します.
            </p>
            <p>
              このように, 複数の音波を合成した結果, 生じる振幅の増減現象を<b>干渉</b>と呼びます (実は, これまでも, エフェクターの解説で実装した,
              原音とエフェクト音を合成する処理というのも, 物理的な視点では, 音波の干渉です. 音楽的には, このような音を重ねる処理を, 合成, あるいは,
              ミキシングと呼ぶので, 合成という用語を優先して使いました).
            </p>
            <p>
              また, 位相がわずかに異なる時点で音波を重ねることを<b>うねり</b> (うなりとも呼びます) と呼び, 音楽的な効果が高いことが知られています (同様に,
              周波数をわずかにずらした音波を重ねた場合でもうねり現象は発生します. 実は, これが原始的なコーラスでもあります).
            </p>
            <p>したがって, 位相を変化させたエフェクト音と原音を合成するような <code>AudioNode</code> の接続を実装すると以下のようになります.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const allpass    = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: 880 });

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for shifting phase
// OscillatorNode (Input) -&gt; BiquadFilterNode (All-Pass Filter) -&gt; AudioDestinationNode (Output)
oscillator.connect(allpass);
allpass.connect(context.destination);

oscillator.start(0);
oscillator.stop(context.currentTime + 2);</code></pre>
            <p>
              ところが, この実装では, 位相を変化する周波数成分が固定されたままなので, 干渉による振幅の増減変化が知覚できるほど発生しないので,
              フェイザーとして聴こえません. 位相変化させる周波数成分を周期的に変化させるように, LFO を All-Pass Filter の <code>frequency</code> プロパティ
              (<code>AudioParam</code>) に接続します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseFrequency = 880;
const depthValue    = 220;
const rateValue     = 1;

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const allpass    = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency });

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Connect nodes for shifting phase
// OscillatorNode (Input) -&gt; BiquadFilterNode (All-Pass Filter) -&gt; AudioDestinationNode (Output)
oscillator.connect(allpass);
allpass.connect(context.destination);

// Connect nodes for LFO that changes All-Pass Filter frequency periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
lfo.connect(depth);
depth.connect(allpass.frequency);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              これで, 位相を変化させた音と干渉によって生じる振幅の増減, つまり, フェイザーのエフェクト音を知覚できるようになったと思います. あとは,
              コーラスやフランジャーと同様に, 原音とエフェクト音のゲインを制御できるように, Dry / Wet のための <code>GainNode</code> を接続します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseFrequency = 880;
const depthValue    = 220;
const rateValue     = 1;

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const allpass    = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency });

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

const dry = new GainNode(context, { gain: 0.5 });  // for gain of original sound
const wet = new GainNode(context, { gain: 0.5 });  // for gain of phaser sound

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for shifting phase
// OscillatorNode (Input) -&gt; BiquadFilterNode (All-Pass Filter) -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(allpass);
allpass.connect(wet);
wet.connect(context.destination);

// Connect nodes for LFO that changes All-Pass Filter frequency periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
lfo.connect(depth);
depth.connect(allpass.frequency);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              現実世界のフェイザーは, よりアグレッシブな干渉を発生させるために, All-Pass Filter を複数接続します. 2 個, 4 個, 12 個, 24
              個の接続可能なフェイザーが多く, それらは, All-Pass Filter の接続数 <span class="math-inline">$n$</span> 個によって,
              <b><span class="math-inline">$n$</span> 段フェイザー</b>と呼ばれることもあります.
            </p>
            <p>
              以下は, All-Pass Filter を 4 つ接続したフェイザー (4 段フェイザー) の実装です. 1 つだけの接続の場合より, 干渉による変化が大きく聴こえると思います.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const baseFrequency = 880;
const depthValue    = 220;
const rateValue     = 1;

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

const allpasses  = [
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency })
];

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

const dry = new GainNode(context, { gain: 0.5 });  // for gain of original sound
const wet = new GainNode(context, { gain: 0.5 });  // for gain of phaser sound

// Connect nodes for original sound
// OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
oscillator.connect(dry);
dry.connect(context.destination);

// Connect nodes for shifting phase
// OscillatorNode (Input) -&gt; BiquadFilterNode (All-Pass Filter) x 4 -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
oscillator.connect(allpasses[0]);

for (let i = 0; i &lt; 3; i++) {
  allpasses[i].connect(allpasses[i + 1]);
}

allpasses[3].connect(wet);
wet.connect(context.destination);

// Connect nodes for LFO that changes All-Pass Filter frequency periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
lfo.connect(depth);

for (let i = 0; i &lt; 4; i++) {
  depth.connect(allpasses[i].frequency);
}

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <figure>
              <svg id="svg-figure-node-connections-for-phaser" width="1200" height="728" />
              <figcaption>フェイザーのノード接続図 (4 段フェイザー)</figcaption>
            </figure>
          </section>
          <p>
            さらに, アグレッシブなエフェクトを発生させたい場合は, フィードバック接続を追加することも考えられます (ただし, フェイザーにおいて,
            フィードバックは一般的に必須というわけではありません). また, <b>レゾナンス</b> (<code>BiquadFilterNode</code> の <code>Q</code> プロパティ
            (<code>AudioParam</code>)) を変更可能にすると, フェイザーによりバリエーションを付加することができます (<code>BiquadFilterNode</code> の
            <code>Q</code> プロパティは, <code>type</code> プロパティ (フィルタの種類) によって制御しているフィルタの特性が異なるので, 詳細は<a
              href="#section-effectors-filter-biquad-filter-node"
              >フィルタのセクション</a>で解説します).
          </p>
          <p>
            以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, All-Pass Filter の接続数や,
            位相変化させる周波数成分などフェイザーに関わるパラメータを制御できるようにしたコード例です. フェイザーはその原理から,
            エフェクト音のみでは変化を得ることができないので, Mix の値が <code>1</code> 未満になるように上限を設定していることにも着目してください.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;select id=&quot;select-phaser-stages&quot;&gt;
  &lt;option value=&quot;2&quot;&gt;2 stages&lt;/option&gt;
  &lt;option value=&quot;4&quot; selected &gt;4 stages&lt;/option&gt;
  &lt;option value=&quot;8&quot;&gt;8 stages&lt;/option&gt;
  &lt;option value=&quot;12&quot;&gt;12 stages&lt;/option&gt;
  &lt;option value=&quot;24&quot;&gt;24 stages&lt;/option&gt;
&lt;/select&gt;
&lt;label for=&quot;range-phaser-frequency&quot;&gt;Frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-phaser-frequency&quot; value=&quot;880&quot; min=&quot;100&quot; max=&quot;4000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-phaser-frequency-value&quot;&gt;880 Hz&lt;/span&gt;
&lt;label for=&quot;range-phaser-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-phaser-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-phaser-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-phaser-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-phaser-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-phaser-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-phaser-resonance&quot;&gt;Resonance&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-phaser-resonance&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;20&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-phaser-resonance-value&quot;&gt;1&lt;/span&gt;
&lt;label for=&quot;range-phaser-mix&quot;&gt;Mix&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-phaser-mix&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;0.9&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-phaser-mix-value&quot;&gt;0&lt;/span&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let oscillator = null;
let lfo        = null;

let numberOfStages = 4;
let baseFrequency  = 880;
let depthRate      = 0;
let rateValue      = 0;
let resonance      = 1;
let mixValue       = 0;

const allpasses  = [
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency }),
  new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency })
];

const depth = new GainNode(context, { gain: baseFrequency * depthRate });
const dry   = new GainNode(context, { gain: 1 - mixValue });
const wet   = new GainNode(context, { gain: mixValue });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const selectPhaserStagesElement = document.getElementById(&apos;select-phaser-stages&apos;);
const rangeFrequencyElement     = document.getElementById(&apos;range-phaser-frequency&apos;);
const rangeDepthElement         = document.getElementById(&apos;range-phaser-depth&apos;);
const rangeRateElement          = document.getElementById(&apos;range-phaser-rate&apos;);
const rangeResonanceElement     = document.getElementById(&apos;range-phaser-resonance&apos;);
const rangeMixElement           = document.getElementById(&apos;range-phaser-mix&apos;);

const spanPrintFrequencyElement = document.getElementById(&apos;print-phaser-frequency-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-phaser-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-phaser-rate-value&apos;);
const spanPrintResonanceElement = document.getElementById(&apos;print-phaser-resonance-value&apos;);
const spanPrintMixElement       = document.getElementById(&apos;print-phaser-mix-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, (event) =&gt; {
  if ((oscillator !== null) || (lfo !== null)) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  // Connect nodes for original sound
  // OscillatorNode (Input) -&gt; GainNode (Dry) -&gt; AudioDestinationNode (Output)
  oscillator.connect(dry);
  dry.connect(context.destination);

  // Connect nodes for shifting phase
  // OscillatorNode (Input) -&gt; BiquadFilterNode (All-Pass Filter) x N -&gt; GainNode (Wet) -&gt; AudioDestinationNode (Output)
  oscillator.connect(allpasses[0]);

  for (let i = 0; i &lt; (numberOfStages - 1); i++) {
    allpasses[i].connect(allpasses[i + 1]);
  }

  allpasses[numberOfStages - 1].connect(wet);
  wet.connect(context.destination);

  // Connect nodes for LFO that changes All-Pass Filter frequency periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
  lfo.connect(depth);

  for (let i = 0; i &lt; numberOfStages; i++) {
    depth.connect(allpasses[i].frequency);
  }

  // Start oscillator and LFO immediately
  oscillator.start(0);
  lfo.start(0);

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, (event) =&gt; {
  if ((oscillator === null) || (lfo === null)) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  // GC (Garbage Collection)
  oscillator = null;
  lfo        = null;

  buttonElement.textContent = &apos;start&apos;;
});

selectPhaserStagesElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  numberOfStages = Number(event.currentTarget.value);

  for (let i = 0, len = allpasses.length; i &lt; len; i++) {
    allpasses[i].disconnect(0);
  }

  for (let i = 0; i &lt; numberOfStages; i++) {
    allpasses[i] = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: baseFrequency });
  }

  if (oscillator !== null) {
    oscillator.connect(allpasses[0]);
  }

  for (let i = 0; i &lt; (numberOfStages - 1); i++) {
    allpasses[i].connect(allpasses[i + 1]);
  }

  allpasses[numberOfStages - 1].connect(wet);
  wet.connect(context.destination);

  for (let i = 0; i &lt; numberOfStages; i++) {
    depth.connect(allpasses[i].frequency);
  }
});

rangeFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  baseFrequency = event.currentTarget.valueAsNumber;

  for (let i = 0; i &lt; numberOfStages; i++) {
    allpasses[i].frequency.value = baseFrequency;
  }

  spanPrintFrequencyElement.textContent = `${Math.trunc(baseFrequency)} Hz`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = baseFrequency * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeResonanceElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  resonance = event.currentTarget.valueAsNumber;

  for (let i = 0; i &lt; numberOfStages; i++) {
    allpasses[i].Q.value = resonance;
  }

  spanPrintResonanceElement.textContent = resonance.toString(10);
});

rangeMixElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  mixValue = event.currentTarget.valueAsNumber;

  dry.gain.value = 1 - mixValue;
  wet.gain.value = mixValue;

  spanPrintMixElement.textContent = mixValue.toString(10);
});</code></pre>
          <div class="app-container app-phaser">
            <button type="button" id="button-phaser">start</button>
            <div>
              <dl>
                <div>
                  <dt><label for="select-phaser-stages">Phaser stages</label></dt>
                  <dd>
                    <select id="select-phaser-stages">
                      <option value="2">2 stages</option>
                      <option value="4" selected>4 stages</option>
                      <option value="8">8 stages</option>
                      <option value="12">12 stages</option>
                      <option value="24">24 stages</option>
                    </select>
                  </dd>
                </div>
                <div>
                  <dt><label for="range-phaser-frequency">Frequency</label></dt>
                  <dd>
                    <input type="range" id="range-phaser-frequency" value="880" min="100" max="4000" step="1" />
                    <span id="print-phaser-frequency-value">880 Hz</span>
                  </dd>
                </div>
                <div>
                  <dt><label for="range-phaser-depth">Depth</label></dt>
                  <dd>
                    <input type="range" id="range-phaser-depth" value="0" min="0" max="1" step="0.05" />
                    <span id="print-phaser-depth-value">0</span>
                  </dd>
                </div>
                <div>
                  <dt><label for="range-phaser-rate">Rate</label></dt>
                  <dd>
                    <input type="range" id="range-phaser-rate" value="0" min="0" max="10" step="0.5" />
                    <span id="print-phaser-rate-value">0</span>
                  </dd>
                </div>
                <div>
                  <dt><label for="range-phaser-resonance">Resonance</label></dt>
                  <dd>
                    <input type="range" id="range-phaser-resonance" value="1" min="1" max="20" step="1" />
                    <span id="print-phaser-resonance-value">1</span>
                  </dd>
                </div>
                <div>
                  <dt><label for="range-phaser-mix">Mix</label></dt>
                  <dd>
                    <input type="range" id="range-phaser-mix" value="0" min="0" max="0.9" step="0.05" />
                    <span id="print-phaser-mix-value">0</span>
                  </dd>
                </div>
              </dl>
            </div>
          </div>
        </section>
        <section id="section-effectors-tremolo-and-ringmodulator">
          <h3>トレモロ・リングモジュレーター</h3>
          <p>
            <b>トレモロ</b>は, 音の大きさに揺らぎを与えるエフェクターです. ギターでは素早くピッキングを繰り返して,
            音が震えているように奏でるトレモロ奏法があります. トレモロは, トレモロ奏法をオーディオ信号処理によって実現するエフェクターとも言えます (ギターでは,
            もう 1 つトレモロという名称がついているものがあります. ストラトキャスター (タイプ) のギターに搭載されているトレモロアームです. しかし,
            トレモロアームは, ビブラートの効果を与えるものなので, 同じトレモロという名称ですが, 効果としては別なものです).
          </p>
          <p>
            <b>リングモジュレーター</b>は, 金属的な音色に変化させるエフェクトです. 例えば, ピアノにリングモジュレーターをかけると, 鐘のような音色に変化します.
          </p>
          <p>
            エフェクターとして, トレモロとリングモジュレーターはかなり感じが違いますが, 原理は共通しています. その原理とは,
            <b>AM 変調</b>を利用したエフェクターであることです.
          </p>
          <section id="section-effectors-tremolo">
            <h4>トレモロ</h4>
            <p>
              トレモロは振幅 (音の大きさ) を周期的に変化させることによって, 実装することができます. つまり, LFO を, <code>GainNode</code> の
              <code>gain</code> プロパティ (<code>AudioParam</code>) に接続することによって実装できます. トレモロは, これまで解説したディレイ・リバーブ,
              コーラス・フランジャー, フェイザーなどと異なり, 原音を変化させるので, <code>AudioNode</code> の接続も実装も非常にシンプルです (トレモロのようのな,
              原音を直接変化させるエフェクターを<b>インサートエフェクト</b>と呼ぶことがあります).
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-tremolo" width="800" height="520" />
              <figcaption>トレモロのノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const depthValue = 0.25;
const rateValue  = 2.5;

const amplitude = new GainNode(context, { gain: 0.5 });  // 0.5 +- ${depthValue}

const oscillator = new OscillatorNode(context);

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: depthValue });

// Connect nodes
// OscillatorNode (Input) -&gt; GainNode (Amplitude) -&gt; AudioDestinationNode (Output)
oscillator.connect(amplitude);
amplitude.connect(context.destination);

// Connect nodes for LFO that changes gain periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; gain (AudioParam)
lfo.connect(depth);
depth.connect(amplitude.gain);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              トレモロは, 振幅 <code>1</code> を基準に値が変化するように定義されています (<span class="math-inline">$f_{s}$</span> はサンプリング周波数).
              しかしながら, そのまま実装すると, 実際には音割れ (クリッピング) が発生してしまうので, 実装では, <code>0.5</code> を基準に, Depth
              の値が増減するようにしています.
            </p>
            <div class="math-block">
              $y\left(n\right) = \left(1 + \mathrm{depth} \cdot \sin\left(\frac{2\pi \cdot \mathrm{rate} \cdot n}{f_{s}}\right)\right) \cdot x\left(n\right)$
            </div>
            <p>
              以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, トレモロに関わるパラメータを制御できるようにしたコード例です.
              トレモロのような (他には, コンプレッサーやディストーションなど) インサートエフェクトでは, パラメータの制御のみでエフェクターを OFF
              にする場合が難しい場合もあるので, コード例のように, フラグなどに応じて, <code>AudioNode</code> の接続自体を切り替えるような実装が必要になります
              (もっとも, トレモロの場合, Depth を <code>0</code> に設定することで, 原音をそのまま出力することが可能です).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-tremolo&quot; checked /&gt;
  &lt;span id=&quot;print-checked-tremolo&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-tremolo-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-tremolo-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-tremolo-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-tremolo-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-tremolo-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-tremolo-rate-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let depthRate = 0;
let rateValue = 0;

let oscillator = new OscillatorNode(context);
let lfo        = new OscillatorNode(context, { frequency: rateValue });

let isStop = true;

const amplitude = new GainNode(context, { gain: 0.5 });  // 0.5 +- ${depthValue}
const depth     = new GainNode(context, { gain: amplitude.gain.value * depthRate });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeDepthElement = document.getElementById(&apos;range-tremolo-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-tremolo-rate&apos;);

const spanPrintCheckedElement = document.getElementById(&apos;print-checked-tremolo&apos;);
const spanPrintDepthElement   = document.getElementById(&apos;print-tremolo-depth-value&apos;);
const spanPrintRateElement    = document.getElementById(&apos;print-tremolo-rate-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);
  amplitude.disconnect(0);
  lfo.disconnect(0);

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Amplitude) -&gt; AudioDestinationNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(context.destination);

    // Connect nodes for LFO that changes gain periodically
    // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; gain (AudioParam)
    lfo.connect(depth);
    depth.connect(amplitude.gain);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Amplitude) -&gt; AudioDestinationNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  } else {
    amplitude.disconnect(0);

    // Connect nodes (Tremolo OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  }

  // Connect nodes for LFO that changes gain periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; gain (AudioParam)
  lfo.connect(depth);
  depth.connect(amplitude.gain);

  lfo.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = amplitude.gain.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});</code></pre>
            <div class="app-container app-tremolo">
              <div class="app-headline">
                <button type="button" id="button-tremolo">start</button>
                <label>
                  <input type="checkbox" id="checkbox-tremolo" checked />
                  <span id="print-checked-tremolo">ON</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-tremolo-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-tremolo-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-tremolo-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-tremolo-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-tremolo-rate" value="0" min="0" max="10" step="0.5" />
                      <span id="print-tremolo-rate-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-ringmodulator">
            <h4>リンクモジュレーター</h4>
            <p>
              <b>リングモジュレーター</b>は, <code>AudioNode</code> の接続としてはトレモロと同じです. LFO の Rate (変調の周波数)を, およそ
              <code>100 Hz</code> 以上にしていくと, 原音の周波数成分とは異なる周波数成分が発生するようになります.
              この周波数成分が金属的な音を生み出す要因となって, 原理は同じながらも, トレモロとは異なるエフェクトを得ることができます.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-ringmodulator" width="900" height="520" />
              <figcaption>リングモジュレーターのノード接続図</figcaption>
            </figure>
            <p>
              リングモジュレーターは, 原音の振幅を正弦波で変調するように定義されているので, トレモロと異なり, 基準となる <code>gain</code> プロパティの値は
              <code>0</code> を設定しています.
            </p>
            <div class="math-block">
              $y\left(n\right) = \left(\mathrm{depth} \cdot \sin\left(\frac{2\pi \cdot \mathrm{rate} \cdot n}{f_{s}}\right)\right) \cdot x\left(n\right)$
            </div>
            <p>
              以下は, 同様に実際のアプリケーションを想定して, ユーザーインタラクティブに,
              リングモジュレーターに関わるパラメータを制御できるようにしたコード例です. 定義式にしたがって, 基準となる <code>gain</code> プロパティの値を
              <code>0</code> にしていること, また, Rate がトレモロより高い値に設定できるようにしていることに着目してください.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-ringmodulator&quot; checked /&gt;
  &lt;span id=&quot;print-checked-ringmodulator&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-ringmodulator-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-ringmodulator-depth&quot; value=&quot;1&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-ringmodulator-depth-value&quot;&gt;1&lt;/span&gt;
&lt;label for=&quot;range-ringmodulator-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-ringmodulator-rate&quot; value=&quot;1000&quot; min=&quot;0&quot; max=&quot;2000&quot; step=&quot;100&quot; /&gt;
&lt;span id=&quot;print-ringmodulator-rate-value&quot;&gt;1000&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let depthRate = 1;
let rateValue = 1000;

let oscillator = new OscillatorNode(context);
let lfo        = new OscillatorNode(context, { frequency: rateValue });

let isStop = true;

const amplitude = new GainNode(context, { gain: 0 });  // 0 +- ${depthValue}
const depth     = new GainNode(context, { gain: depthRate });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeDepthElement = document.getElementById(&apos;range-ringmodulator-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-ringmodulator-rate&apos;);

const spanPrintCheckedElement = document.getElementById(&apos;print-checked-ringmodulator&apos;);
const spanPrintDepthElement   = document.getElementById(&apos;print-ringmodulator-depth-value&apos;);
const spanPrintRateElement    = document.getElementById(&apos;print-ringmodulator-rate-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);
  amplitude.disconnect(0);
  lfo.disconnect(0);

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Amplitude) -&gt; AudioDestinationNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(context.destination);

    // Connect nodes for LFO that changes gain periodically
    // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; gain (AudioParam)
    lfo.connect(depth);
    depth.connect(amplitude.gain);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Amplitude) -&gt; AudioDestinationNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  } else {
    amplitude.disconnect(0);

    // Connect nodes (Ring Modulator OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  }

  // Connect nodes for LFO that changes gain periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; gain (AudioParam)
  lfo.connect(depth);
  depth.connect(amplitude.gain);

  lfo.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});</code></pre>
            <div class="app-container app-ringmodulator">
              <div class="app-headline">
                <button type="button" id="button-ringmodulator">start</button>
                <label>
                  <input type="checkbox" id="checkbox-ringmodulator" checked />
                  <span id="print-checked-ringmodulator">ON</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-ringmodulator-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-ringmodulator-depth" value="1" min="0" max="1" step="0.05" />
                      <span id="print-ringmodulator-depth-value">1</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-ringmodulator-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-ringmodulator-rate" value="1000" min="0" max="2000" step="100" />
                      <span id="print-ringmodulator-rate-value">1000</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-tremolo-and-ringmodulator-am-modulation">
            <h4>AM 変調</h4>
            <p><b>AM 変調</b> (<b>Amplitude Modulation</b>) とは, 時間の経過とともに信号の振幅を変化させることです.</p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-amplitude-modulation-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-amplitude-modulation-spectrum" width="720" height="240" /></dd>
              </dl>
              <figcaption>
                <span>AM 変調のイメージ</span>
                <button type="button" id="button-amplitude-modulation-animation">start</button>
                <span class="app-headline">
                  <label for="range-amplitude-modulation-rate">Rate</label>
                  <input type="range" id="range-amplitude-modulation-rate" value="1" min="0" max="8000" step="1" />
                  <span id="print-amplitude-modulation-rate">1 Hz</span>
                </span>
              </figcaption>
            </figure>
            <p>
              変調の周期を短くしていくと (LFO の Rate を高くしていくと), 原音の周波数成分だけではなく, LFO の周波数も周波数成分として発生します. これは,
              原音の波形が<b>キャリア</b> (<b>搬送波</b>) となって, LFO の正弦波が<b>モジュレーター</b>となってエンベロープを形成して周波数成分となるからです
              (出力音のエンベロープが正弦波になっていることに着目してください).
            </p>
            <p>
              この仕組みを発展させた音合成が, <b>FM シンセサイザー</b> (FM 音源) で, キャリア (搬送波) とモジュレーターの波形を正弦波として,
              それらを合成する正弦波によって定義されます (<span class="math-inline">$A$</span> はキャリアの振幅,
              <span class="math-inline">$f_{c}$</span> はキャリアの周波数, <span class="math-inline">$\beta$</span> は変調指数 (LFO の Depth に相当),
              <span class="math-inline">$f_{m}$</span> はモジュレーターの周波数 (LFO の Rate に相当)).
            </p>
            <div class="math-block">
              $y\left(n\right) = A \cdot \sin\left(\frac{2\pi \cdot f_{c} \cdot n}{f_{s}} + \left(\beta \cdot \sin\left(\frac{2\pi \cdot f_{m} \cdot
              n}{f_{s}}\right)\right)\right)$
            </div>
            <p>(命名的に混同しますが) リングモジュレーター自体の原理は AM 変調であり, それが, FM シンセサイザーの原理になっているということです.</p>
          </section>
        </section>
        <section id="section-effectors-filter">
          <h3>フィルタ</h3>
          <p>
            フィルタという言葉は日常生活でも使われます. コンピュータサイエンスにおいても, UNIX 系 OS でパイプとフィルタがあります. フィルタの概念としては,
            ある結果を遮断して, ある結果を通過させるということでしょう.
          </p>
          <p>
            オーディオ信号処理における<b>フィルタ</b>も同様に, ある周波数成分の音を通過・遮断, あるいは, 増幅・減衰させて, 周波数特性を変化させます.
            フィルタだけをエフェクターとして使うことはあまりなく, すでに解説したフェイザーや, このあとのセクションで解説する,
            イコライザーやワウなどフィルタ系のエフェクターで使われることが多いです.
          </p>
          <p>また, 音響特徴量はスペクトル, つまり, 周波数成分として表れることが多いので, 音の加工においてもフィルタを理解することは重要となります.</p>
          <article id="section-effectors-decibel">
            <h4>デシベル</h4>
            <p>
              フィルタの解説において, 仕様上, <b>デシベル</b> (<b>dB</b>) という単位が使われるので (<code>BiquadFilterNode</code> の
              <code>gain</code> プロパティやフィルタの特性グラフなど), フィルタに限ったことではないですが, ここで解説をします.
            </p>
            <p>デシベルとは, 端的には, <b>音圧レベル</b>を表す単位です. <b>音圧</b>とは, 音の実体である媒体の振動によって伝わる圧力 (力) のことです.</p>
            <p>
              ここで, 基準の音圧 (<span class="math-inline">$P_{0} = 2 \cdot 10^{-5} \mathrm{[Pa]}$</span>) を基準 (ちなみに, この基準の音圧は,
              <code>1 KHz</code> における可聴な最小の音圧とされています) に, 対象の音の音圧 <span class="math-inline">$P$</span> の比率の対数をとった値
              (以下の定義式. 上は時間領域, 下は周波数領域での定義式. 多くのケースにおいて, <b>音圧パワー</b> (音圧の 2 乗) の比を算出することが有用なので,
              各音圧の 2 乗の比で定義しています) が音圧レベルとなります.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &10\log_{10}\left(\frac{P}{P_{0}}\right)^{2} = 20\log_{10}\left(\frac{P}{P_{0}}\right) \quad \left(P_{0} = 2 \cdot 10^{-5} \mathrm{[Pa]}\right) \\
                  & \\
                  &10\log_{10}\left|X\left(k\right)\right|^{2} = 20\log_{10}\left|X\left(k\right)\right| \\
                  & \\
                \end{flalign}
              $
            </div>
            <p>対数で表す理由は大きく 2 つあります.</p>
            <ul>
              <li>音圧は非常に広範囲な値となるので, 人間の感覚とうまく対応ない</li>
              <li><b>フェヒナーの法則</b>という心理学の理論で, <b>人間の感覚量は刺激強度の対数に比例する</b>という法則が適用できる</li>
            </ul>
            <p>
              (以下の表を参考にして) <code>6 dB</code> 音圧レベルが大きくなれば音圧は約 2 倍 (<span class="math-inline">$20\log_{10}2$</span>) になります.
              <code>20 dB</code> が大きくなれば 10 倍 (<span class="math-inline">$20\log_{10}10$</span>), <code>40 dB</code> 大きくなれば 100 倍 (<span
                class="math-inline"
                >$20\log_{10}100$</span>) ... という関係で, 本来であれば広範囲におよぶ値を対数をとることによって解決しています.
            </p>
            <table class="auto-table">
              <caption>
                デシベル差と倍率
              </caption>
              <thead>
                <tr>
                  <th scope="col">Difference decibel</th>
                  <th scope="col">Magnification</th>
                  <th scope="col">Example</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0 dB</td>
                  <td>1 倍</td>
                  <td>人間の聴力の限界</td>
                </tr>
                <tr>
                  <td>6 dB</td>
                  <td>2 倍</td>
                  <td></td>
                </tr>
                <tr>
                  <td>10 dB</td>
                  <td>3 倍</td>
                  <td></td>
                </tr>
                <tr>
                  <td>20 dB</td>
                  <td>10 倍</td>
                  <td>木の葉のふれあう音</td>
                </tr>
                <tr>
                  <td>40 dB</td>
                  <td>100 倍</td>
                  <td>図書館</td>
                </tr>
                <tr>
                  <td>60 dB</td>
                  <td>1,000 倍</td>
                  <td>会話</td>
                </tr>
                <tr>
                  <td>80 dB</td>
                  <td>10,000 倍</td>
                  <td>目覚まし時計</td>
                </tr>
                <tr>
                  <td>100 dB</td>
                  <td>100,000 倍</td>
                  <td>電車のガード下</td>
                </tr>
                <tr>
                  <td>120 dB</td>
                  <td>1,000,000 倍</td>
                  <td>飛行機のエンジン付近</td>
                </tr>
              </tbody>
            </table>
            <p>
              フィルタの特性を表すグラフでは, <code>0 dB</code> を基準に見ていただくのがよいのですが, これは,
              <b><code>0 dB</code> が入力音と出力音の振幅比が変わらない (つまり, そのまま通過させる) ことを意味しているからです</b> (<span class="math-inline"
                >$20\log_{10}1 = 0$</span>). これを理解しておくと, <code>BiquadFilterNode</code> の <code>gain</code> プロパティが (特定のフィルタの種類において)
              ある周波数成分を増幅させたり, 減衰させたりすることも理解できると思います.
            </p>
          </article>
          <section id="section-effectors-filter-biquad-filter-node">
            <h4>BiquadFilterNode</h4>
            <p>
              Web Audio API において, 様々なフィルタを簡単に利用するには <code>BiquadFilterNode</code> を使うのが最適です. <b>Biquad</b> とは,
              <b>双 2 次</b>という意味で, <code>BiquadFilterNode</code> の次数は 2 次となります (以下の伝達関数で定義されています). したがって,
              <code>BiquadFilterNode</code> では実装できないフィルタ, 具体的には, <code>奇数次</code>のフィルタを使いたい場合, あとのセクションで解説する
              <code>IIRFilterNode</code> を使う必要があります (<code>BiquadFilterNode</code> は 2 次の IIR フィルタです).
            </p>
            <p>
              <code>BiquadFilterNode</code> では, フィルタの特性に関わるプロパティとして, <b><code>type</code></b> プロパティ (<b
                ><code>BiquadFilterType</code></b>), <b><code>frequency</code></b> / <b><code>detune</code></b> プロパティ (どちらも <code>AudioParam</code>), <b><code>Q</code></b> プロパティ
              (<code>AudioParam</code>), <b><code>gain</code></b> プロパティ (<code>AudioParam</code>) が定義されています. <code>type</code> プロパティ以外は,
              <code>type</code> プロパティ (すなわち, フィルタの種類) によって, 制御するフィルタの特性が異なったり, あるいは, そもそも無効だったりするので,
              <code>BiquadFilterNode</code> で使える 8 つのフィルタの種類ごとに解説を進めます.
            </p>
            <p>
              フィルタの種類に関わらず, <code>BiquadFilterNode</code> は, そのインスタンスを接続するだけで機能します (また, コンストラクタ形式であれば,
              インスタンス生成時に, 第 2 引数に <b>BiquadFilterOptions</b> を指定して, 初期値を変更することも可能です).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const filter     = new BiquadFilterNode(context);

// If use `createBiquadFilter`
// const filter = context.createBiquadFilter();

// OscillatorNode (Input) -&gt; BiquadFilterNode -&gt; AudioDestinationNode (Output)
oscillator.connect(filter);
filter.connect(context.destination);

oscillator.start(0);</code></pre>
            <img src="images/biquad-filter-node.png" alt="BiquadFilterNode" width="1232" height="770" loading="lazy" />
            <section id="section-effectors-filter-biquad-filter-node-frequency-and-detune">
              <h5>frequency / detune プロパティ</h5>
              <p>
                フィルタの種類に関わらず, <b><code>frequency</code></b> / <b><code>detune</code></b> プロパティはすべてのフィルタにおいて有効になります (ただし,
                フィルタの特性への影響はフィルタの種類ごとに異なります). <code>OscillatorNode</code> などと同じように, <code>frequency</code> プロパティと
                <code>detune</code> プロパティを合わせて算出される周波数 (<span class="math-inline">$f_{\mathrm{computed}}\left(t\right)$</span>)
                は以下のように定義されています.
              </p>
              <div class="math-block">
                $f_{\mathrm{computed}}\left(t\right) = \mathrm{frequency}\left(t\right) \cdot \mathrm{pow}\left(2, \left(\mathrm{detune}\left(t\right) / 1200
                \right)\right)$
              </div>
              <p>
                <code>frequency</code> / <code>detune</code> プロパティ, <code>Q</code> プロパティ, <code>gain</code> プロパティは, すべて
                <code>AudioParam</code> なので, オートメーションさせたり, LFO を接続したりすることが可能です.
              </p>
            </section>
            <article class="section-biquad-filter-node-definition">
              <h5>BiquadFilterNode の定義式</h5>
              <p>時間領域での <code>BiquadFilterNode</code> の定義式 (2 次の IIR フィルタ) は, 以下のように定義されています.</p>
              <div class="math-block">
                $a_{0}y\left(n\right) + a_{1}y\left(n - 1\right) + a_{2}y\left(n - 2\right) = b_{0}x\left(n\right) + b_{1}x\left(n - 1\right) + b_{2}x\left(n -
                2\right)$
              </div>
              <p>
                これを <span class="math-inline">$z$</span> 変換すると, 伝達関数 (周波数領域での <code>BiquadFilterNode</code> の定義式) は,
                以下のように定義されます (時間領域の遅延は, <span class="math-inline">$z$</span> 変換の次数となります).
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &H\left(z\right) = \frac{\frac{b_{0}}{a_{0}} + \frac{b_{1}}{a_{0}}z^{-1} + \frac{b_{2}}{a_{0}}z^{-2}}{1 + \frac{a_{1}}{a_{0}}z^{-1} + \frac{a_{2}}{a_{0}}z^{-2}}
                  \end{flalign}
                $
              </div>
              <p>
                Biquad Filter (双 2 次フィルタ) はオーディオ信号処理で頻繁に利用される, Robert Bristow-Johnson 氏が書いた有名な設計手法の解説, Audio-EQ-Cookbook
                というのがあり,
                <a href="https://www.w3.org/TR/audio-eq-cookbook/" target="_blank" rel="noopener noreferrer">W3C のドキュメント</a>としても公開されています. Web
                Audio API の <code>BiquadFilterNode</code> も Audio EQ Cookbook をベースにした実装になっています (厳密には, 多少の改変がされています).
              </p>
              <p>IIR フィルタに関してはあとのセクションで解説します.</p>
            </article>
            <section id="section-effectors-filter-biquad-filter-node-lowpass">
              <h5>Low-Pass Filter</h5>
              <p>
                <b>Low-Pass Filter</b> (<b>低域通過フィルタ</b>) とは, <b>カットオフ周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>)
                付近までの周波数成分を通過させ, それより大きい周波数成分を遮断するフィルタです. すでに解説しましたが, サンプリング定理のために, A/D 変換や D/A
                変換で使われたり, エフェクターのワウで使われたり, エフェクト音のトーンを設定したり, おそらく最も使用頻度の高いフィルタになります
                (おそらくその理由で, デフォルト値になっていると思われます).
              </p>
              <p>
                Low-Pass Filter における, <b><code>Q</code></b> プロパティ (<b>クオリティファクタ</b>, または, <b>レゾナンス</b>と呼ばれることが多いです) は,
                カットオフ周波数付近の急峻を変化させます. 正の値にすると, 急峻が鋭くなって, カットオフ周波数付近の周波数成分を増幅させます (これは,
                ワウの実装において重要になる点です). 負の値を設定すると, カットオフ周波数付近の周波数成分を減衰させるフィルタ特性になります.
              </p>
              <p>
                Low-Pass Filter においては, <b><code>gain</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-lowpass-frequency">frequency</label>
                  <input type="range" id="range-filter-lowpass-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-lowpass-frequency">350 Hz</span>
                  <label for="range-filter-lowpass-detune">detune</label>
                  <input type="range" id="range-filter-lowpass-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-lowpass-detune">0 cent</span>
                  <label for="range-filter-lowpass-Q">Q</label>
                  <input type="range" id="range-filter-lowpass-Q" value="1" min="-20" max="20" step="1" />
                  <span id="print-filter-lowpass-Q">1 dB</span>
                </div>
                <svg id="svg-figure-filter-response-lowpass" width="600" height="300" />
                <figcaption>Low-Pass Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-highpass">
              <h5>High-Pass Filter</h5>
              <p>
                <b>High-Pass Filter</b> (<b>高域通過フィルタ</b>) とは, Low-Pass Filter と逆で, カットオフ周波数 (<span class="math-inline"
                  >$f_{\mathrm{computed}}$</span>) 付近までの周波数成分を遮断して, それより大きい周波数成分を通過させるフィルタです. Low-Pass Filter と比較すると, 使用頻度は低いですが,
                プリアンプ (アンプシミュレーター) や歪み系のエフェクターの実装では重要なフィルタとなります.
              </p>
              <p>
                High-Pass Filter における, <b><code>Q</code></b> プロパティは, Low-Pass Filter と同様に, カットオフ周波数付近の急峻を変化させます.
                正の値にすると, 急峻が鋭くなり, カットオフ周波数付近の周波数成分を増幅させます. 負の値を設定すると,
                カットオフ周波数付近の周波数成分を減衰させるフィルタ特性になります.
              </p>
              <p>
                High-Pass Filter においても, <b><code>gain</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-highpass-frequency">frequency</label>
                  <input type="range" id="range-filter-highpass-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-highpass-frequency">350 Hz</span>
                  <label for="range-filter-highpass-detune">detune</label>
                  <input type="range" id="range-filter-highpass-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-highpass-detune">0 cent</span>
                  <label for="range-filter-highpass-Q">Q</label>
                  <input type="range" id="range-filter-highpass-Q" value="1" min="-20" max="20" step="1" />
                  <span id="print-filter-highpass-Q">1 dB</span>
                </div>
                <svg id="svg-figure-filter-response-highpass" width="600" height="300" />
                <figcaption>High-Pass Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-bandpass">
              <h5>Band-Pass Filter</h5>
              <p>
                <b>Band-Pass Filter</b> (<b>帯域通過フィルタ</b>) とは, <b>中心周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>)
                付近の周波数成分を通過させ, それ以外の周波数成分を遮断するフィルタです.
              </p>
              <p>
                Band-Pass Filter における, <b><code>Q</code></b> プロパティは, 中心周波数を基準にした帯域幅に影響を与えます.
                <code>Q</code> プロパティの値を大きくするほど, 中心周波数付近の帯域幅が狭くなります (急峻になります).
                <b>0 以下の値を設定すると, 中心周波数として機能しなくなるので, 正の値を指定するようにします</b>.
              </p>
              <p>
                Band-Pass Filter においても, <b><code>gain</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-bandpass-frequency">frequency</label>
                  <input type="range" id="range-filter-bandpass-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-bandpass-frequency">350 Hz</span>
                  <label for="range-filter-bandpass-detune">detune</label>
                  <input type="range" id="range-filter-bandpass-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-bandpass-detune">0 cent</span>
                  <label for="range-filter-bandpass-Q">Q</label>
                  <input type="range" id="range-filter-bandpass-Q" value="1" min="1" max="20" step="1" />
                  <span id="print-filter-bandpass-Q">1</span>
                </div>
                <svg id="svg-figure-filter-response-bandpass" width="600" height="300" />
                <figcaption>Band-Pass Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-lowshelf">
              <h5>Low-Shelving Filter</h5>
              <p>
                <b>Low-Shelving Filter</b> とは, <b>カットオフ周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>) 付近までの周波数成分を増幅,
                または, 減衰させ, それより大きい周波数成分をそのまま通過させるフィルタです. Low-Shelving Filter における, <b><code>gain</code></b> プロパティが,
                増幅, または, 減衰の値を決定します. 単位は, デシベル (<code>dB</code>) です.
              </p>
              <p>
                Low-Shelving Filter においては, <b><code>Q</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません (一般的な, Biquad Filter
                においては, フィルタ特性に影響しますが, Web Audio API の <code>BiquadFilterNode</code> でやや実装が改変されている点の 1 つです).
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-lowshelf-frequency">frequency</label>
                  <input type="range" id="range-filter-lowshelf-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-lowshelf-frequency">350 Hz</span>
                  <label for="range-filter-lowshelf-detune">detune</label>
                  <input type="range" id="range-filter-lowshelf-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-lowshelf-detune">0 cent</span>
                  <label for="range-filter-lowshelf-gain">gain</label>
                  <input type="range" id="range-filter-lowshelf-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-filter-lowshelf-gain">0 dB</span>
                </div>
                <svg id="svg-figure-filter-response-lowshelf" width="600" height="300" />
                <figcaption>Low-Shelving Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-highshelf">
              <h5>High-Shelving Filter</h5>
              <p>
                <b>High-Shelving Filter</b> とは, <b>カットオフ周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>)
                付近までの周波数成分をそのまま通過させ, それより大きい周波数成分を増幅, または, 減衰させるフィルタです. High-Shelving Filter における,
                <b><code>gain</code></b> プロパティが, 増幅, または, 減衰の値を決定します. 単位は, デシベル (<code>dB</code>) です.
              </p>
              <p>
                High-Shelving Filter においても, <b><code>Q</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません (一般的な, Biquad Filter
                においては, フィルタ特性に影響しますが, Web Audio API の <code>BiquadFilterNode</code> でやや実装が改変されている点の 1 つです).
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-highshelf-frequency">frequency</label>
                  <input type="range" id="range-filter-highshelf-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-highshelf-frequency">350 Hz</span>
                  <label for="range-filter-highshelf-detune">detune</label>
                  <input type="range" id="range-filter-highshelf-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-highshelf-detune">0 cent</span>
                  <label for="range-filter-highshelf-gain">gain</label>
                  <input type="range" id="range-filter-highshelf-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-filter-highshelf-gain">0 dB</span>
                </div>
                <svg id="svg-figure-filter-response-highshelf" width="600" height="300" />
                <figcaption>High-Shelving Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-peaking">
              <h5>Peaking Filter</h5>
              <p>
                <b>Peaking Filter</b> とは, <b>中心周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>) 付近の周波数成分を増幅, または,
                減衰させ, それ以外の周波数成分をそのまま通過させるフィルタです. Peaking Filter における, <b><code>gain</code></b> プロパティが, 増幅, または,
                減衰の値を決定します. 単位は, デシベル (<code>dB</code>) です.
              </p>
              <p>
                Peaking Filter における, <b><code>Q</code></b> プロパティは, Band-Pass Filter と同様に, 中心周波数を基準にした帯域幅に影響を与えます.
                <code>Q</code> プロパティの値を大きくするほど, 中心周波数付近の帯域幅が狭くなります (急峻になります).
                <b>0 以下の値を設定すると, 中心周波数として機能しなくなるので, 正の値を指定するようにします</b>.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-peaking-frequency">frequency</label>
                  <input type="range" id="range-filter-peaking-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-peaking-frequency">350 Hz</span>
                  <label for="range-filter-peaking-detune">detune</label>
                  <input type="range" id="range-filter-peaking-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-peaking-detune">0 cent</span>
                  <label for="range-filter-peaking-Q">Q</label>
                  <input type="range" id="range-filter-peaking-Q" value="1" min="1" max="20" step="1" />
                  <span id="print-filter-peaking-Q">1</span>
                  <label for="range-filter-peaking-gain">gain</label>
                  <input type="range" id="range-filter-peaking-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-filter-peaking-gain">0 dB</span>
                </div>
                <svg id="svg-figure-filter-response-peaking" width="600" height="300" />
                <figcaption>Peaking Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-notch">
              <h5>Notch Filter</h5>
              <p>
                <b>Notch Filter</b> (<b>帯域除去フィルタ</b>) とは, <b>中心周波数</b> (<span class="math-inline">$f_{\mathrm{computed}}$</span>)
                付近の周波数成分を遮断して, それ以外の周波数成分を通過させるフィルタです. (厳密には, その定義が異なる点はありますが)
                <b>Band-Elimination Filter</b> (<b>帯域阻止フィルタ</b>) と呼ばれることもあります. また, 実装的には, Low-Pass Filter と High-Pass Filter
                を組み合わせることでも実装は可能です.
              </p>
              <p>
                Notch Filter における, <b><code>Q</code></b> プロパティは, Band-Pass Filter と同様に, 中心周波数を基準にした帯域幅に影響を与えます.
                <code>Q</code> プロパティの値を大きくするほど, 中心周波数付近の帯域幅が狭くなります (急峻になります).
                <b>0 以下の値を設定すると, 中心周波数として機能しなくなるので, 正の値を指定するようにします</b>.
              </p>
              <p>対となる Band-Pass Filter と比較すると, 同じ <code>Q</code> プロパティの値でも, 中心周波数付近の帯域幅が狭くなっています.</p>
              <p>
                Notch Filter においても, <b><code>gain</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-notch-frequency">frequency</label>
                  <input type="range" id="range-filter-notch-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-notch-frequency">350 Hz</span>
                  <label for="range-filter-notch-detune">detune</label>
                  <input type="range" id="range-filter-notch-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-notch-detune">0 cent</span>
                  <label for="range-filter-notch-Q">Q</label>
                  <input type="range" id="range-filter-notch-Q" value="1" min="1" max="20" step="1" />
                  <span id="print-filter-notch-Q">1</span>
                </div>
                <svg id="svg-figure-filter-response-notch" width="600" height="300" />
                <figcaption>Notch Filter のフィルタ特性</figcaption>
              </figure>
            </section>
            <section id="section-effectors-filter-biquad-filter-node-allpass">
              <h5>All-Pass Filter</h5>
              <p>
                <b>All-Pass Filter</b> (<b>全域通過フィルタ</b>) とは, 振幅特性は変化させずに, <b>中心周波数</b> (<span class="math-inline"
                  >$f_{\mathrm{computed}}$</span>) 付近の周波数成分の<b>位相特性を変化させるフィルタです</b>. したがって, フィルタ特性のグラフも, All-Pass Filter のみは,
                <b>位相スペクトル</b> (縦軸が, 位相で単位は <code>radian</code>) となっています (振幅特性が変わらないので, 振幅スペクトルで表示すると,
                パラメータを変化させてもフィルタ特性は変わりません).
              </p>
              <p>
                中心周波数では, <span class="math-inline">$\pm \pi$</span> で最も位相が変化し (<span class="math-inline">$\pm \pi$</span> 位相変化すると,
                逆位相となります), 中心周波数から離れる周波数成分ほど, ほとんど位相は変化しなくなります.
              </p>
              <p>
                All-Pass Filter における, <b><code>Q</code></b> プロパティは, 中心周波数付近の急峻に影響を与えます.
                <code>Q</code> プロパティの値を大きくするほど, 中心周波数付近のフィルタ特性が急峻になって,
                それ以外の周波数成分の位相特性に影響を与えなくなります. つまり, 位相特性を変化させる周波数帯域をより狭くします.
                <b>0 以下の値を設定すると, 中心周波数として機能しなくなるので, 正の値を指定するようにします</b>.
              </p>
              <p>
                All-Pass Filter においては, <b><code>gain</code></b> プロパティは無効で, フィルタ特性に影響を与えることはありません.
              </p>
              <figure>
                <div class="app-headline">
                  <label for="range-filter-allpass-frequency">frequency</label>
                  <input type="range" id="range-filter-allpass-frequency" value="350" min="1" max="8000" step="1" />
                  <span id="print-filter-allpass-frequency">350 Hz</span>
                  <label for="range-filter-allpass-detune">detune</label>
                  <input type="range" id="range-filter-allpass-detune" value="0" min="-1200" max="1200" step="1" />
                  <span id="print-filter-allpass-detune">0 cent</span>
                  <label for="range-filter-allpass-Q">Q</label>
                  <input type="range" id="range-filter-allpass-Q" value="1" min="1" max="20" step="1" />
                  <span id="print-filter-allpass-Q">1</span>
                </div>
                <svg id="svg-figure-filter-response-allpass" width="600" height="300" />
                <figcaption>All-Pass Filter のフィルタ特性 (位相スペクトル)</figcaption>
              </figure>
              <article id="section-effectors-filter-biquad-filter-node-allpass-parallel-connection">
                <h6>All-Pass Filter の並列接続</h6>
                <p>
                  All-Pass Filter は, 直列接続では, フェイザーや Schroeder Reverberator (シュレーダーリバーブ) などのユースケースがありますが, 実は,
                  並列接続することで, さまざまなフィルタを実装することが可能です. 例えば, 高音域の位相を <span class="math-inline">$\pi$</span> (<span
                    class="math-inline"
                    >$-\pi$</span>) 変化させる All-Pass Filter を並列接続することで, 高音域を遮断する, すなわち, Low-Pass Filter を実装することが可能です. 詳細は,
                  <a href="https://www.wizard-notes.com/entry/asp/allpass-filter" target="_blank" rel="noopener noreferrer"
                    >信号処理に欠かせないオールパスフィルタ―の概要・使い方・利用例</a>などを参考にしてください.
                </p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const allpass0   = new BiquadFilterNode(context, { type: &apos;allpass&apos; });
const allpass1   = new BiquadFilterNode(context, { type: &apos;allpass&apos;, frequency: 1000 });

//                         |-&gt; BiquadFilterNode (All-Pass Filter) |
// OscillatorNode (Input) -|                                      |-&gt; AudioDestinationNode (Output)
//                         |-&gt; BiquadFilterNode (All-Pass Filter) |
oscillator.connect(allpass0);
oscillator.connect(allpass1);

allpass0.connect(context.destination);
allpass1.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);</code></pre>
              </article>
            </section>
          </section>
          <section id="section-effectors-iir-filter-node">
            <h4>IIRFilterNode</h4>
            <p>
              <code>BiquadFilterNode</code> では実装できない IIR フィルタを実装する場合, 次の手段としては,
              <b><code>IIRFilterNode</code></b> クラスを利用することです (最後の手段は, AudioWorklet で実装することです).
            </p>
            <p>
              <code>IIRFilterNode</code> では, <code>BiquadFilterNode</code> でフィルタの特性に影響を与えていた, <code>frequency</code> プロパティや
              <code>Q</code> プロパティ, <code>gain</code> プロパティなどは, リアルタイムに変化させることができなくなる点には注意してください.
              <code>IIRFilterNode</code> に与えるパラメータは, <code>AudioParam</code> ではないからです.
            </p>
            <p>
              実装としては, <code>IIRFilterNode</code> コンストラクタの第 2 引数に, <b><code>IIRFilterOptions</code></b> として,
              フィルタの係数の配列を設定します. <code>IIRFilterOptions</code> オブジェクトの <b><code>feedforward</code></b> プロパティは,
              <b>IIR フィルタの伝達関数の分子となる係数</b> (以下の伝達関数の <span class="math-inline">$b_{m}$</span>),
              <b><code>feedback</code></b> プロパティは, <b>IIR フィルタの伝達関数の分母となる係数</b> (以下の伝達関数の
              <span class="math-inline">$a_{n}$</span>) をそれぞれ設定します (ファクトリメソッドの場合, 第 1 引数に <code>feedforward</code>, 第 2 引数に
              <code>feedback</code> を指定します). <code>IIRFilterNode</code> の伝達関数は以下の定義式となります.
              <code>BiquadFilterNode</code> の伝達関数と異なり, フィルタの次数を自由に設定できる点に着目してください.
            </p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &H\left(z\right) = \frac{\sum_{m=0}^{M}b_{m}z^{-m}}{\sum_{n=0}^{N}a_{n}z^{-n}}
                \end{flalign}
              $
            </div>
            <p>
              ただし, まったく制約がないわけではなく, 0 次のフィルタはエラーとなります (それ以外にも, <span class="math-inline">$a_{0}$</span> は,
              <code>0</code> 以外の値である必要があったり, 係数がすべて <code>0</code> の <code>feedforward</code> はエラーとなったりします). また, 実装上,
              <b>20 次までのフィルタが上限となります</b>.
            </p>
            <p>簡易的ではありますが, 1 次の IIR フィルタによる, Low-Pass Filter と High-Pass Filter の実装例です.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const cutoff = 1000;  // 1000 Hz

const b = (cutoff / context.sampleRate) * Math.PI;

const b0 = b;
const b1 = b;
const a0 =  1 + b;
const a1 = -1 + b;

const feedforward = new Float64Array([b0, b1]);
const feedback    = new Float64Array([a0, a1]);

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const filter     = new IIRFilterNode(context, { feedforward, feedback });

// If use `createIIRFilter`
// const filter = context.createIIRFilter(feedforward, feedback);

// OscillatorNode (Input) -&gt; IIRFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
oscillator.connect(filter);
filter.connect(context.destination);

oscillator.start(0);</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const cutoff = 4000;  // 4000 Hz

const a = (cutoff / context.sampleRate) * Math.PI;

const b0 =  1;
const b1 = -1;
const a0 =  1 + a;
const a1 = -1 + a;

const feedforward = new Float64Array([b0, b1]);
const feedback    = new Float64Array([a0, a1]);

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const filter     = new IIRFilterNode(context, { feedforward, feedback });

// If use `createIIRFilter`
// const filter = context.createIIRFilter(feedforward, feedback);

// OscillatorNode (Input) -&gt; IIRFilterNode (High-Pass Filter) -&gt; AudioDestinationNode (Output)
oscillator.connect(filter);
filter.connect(context.destination);

oscillator.start(0);</code></pre>
            <img src="images/iir-filter-node.png" alt="IIRFilterNode" width="1232" height="770" loading="lazy" />
          </section>
          <section id="section-effectors-iir-filter">
            <h4>IIR フィルタ</h4>
            <p><b>IIR フィルタ</b> (<b>Infinite Impulse Response filter</b>) は, 以下の数式で定義されるデジタルフィルタです.</p>
            <div class="math-block">
              <!-- prettier-ignore -->
              $
                \begin{flalign}
                  &y\left(n\right) = \sum_{m = 0}^{J}b\left(m\right)x\left(n - m\right) - \sum_{m = 1}^{I}a\left(m\right)y\left(n - m\right)
                \end{flalign}
              $
            </div>
            <p>
              フィルタを通過した音が再度フィルタを通ることになる, <b>フィードバックがある</b>ことが, FIR フィルタと大きく異なる点です.
              定義式上は無限にインパルス応答が続くことになるので, <b>Infinite</b> (無限の) と命名されています (もちろん,
              コンピュータでは無限のフィルタを実装することはできないので, 有限の次数でうちきる必要があります. <code>IIRFilterNode</code> の場合, その上限が
              <code>20</code> ということです). また, フィードバックの項 (<span class="math-inline">$a\left(m\right)$</span>) は,
              <span class="math-inline">$m = 1$</span> から始まっている点に着目してください.
            </p>
            <p>
              フィードバックがある利点は, 次数の低いフィルタでも性能のよいフィルタ, つまり, 通過する周波数成分と遮断する周波数成分を可能な限りはっきりと分ける
              (<b>理想フィルタ</b>に近づける) フィルタが低次数で実装できます (実際, <code>BiquadFilterNode</code> の次数は 2 次です).
            </p>
            <p>
              具体的に, 2 次の IIR フィルタ (<span class="math-inline">$J = I = 2$</span>) を加算器・乗算器・遅延器の要素を利用してブロック図として表現します.
            </p>
            <figure>
              <svg id="svg-figure-iir-filter" width="720" height="400" />
              <figcaption>IIR フィルタ</figcaption>
            </figure>
            <article class="section-effectors-filter-ideal-filter-and-transition-bandwidth">
              <h5>理想フィルタと遷移帯域幅</h5>
              <p>
                アナログフィルタでは, カットオフ周波数や中心周波数を境に, 通過する周波数成分と阻止される周波数成分がはっきりと分離されます. しかし,
                <code>BiquadFilterNode</code> のセクションで表示しているフィルタ特性のように, デジタルフィルタにおいては, はっきりと分離されることなく,
                曖昧な帯域が存在することがわかるかと思います. これは, アナログフィルタにおいては, 無限の区間で定義されるフィルタを,
                コンピュータでは無限の区間をあつかうことはできないので, 有限の区間でうちきる必要があるからです. その処理によって,
                どうしても曖昧な帯域が発生してしまいます. これを<b>遷移帯域幅</b>と言います. つまり, デジタルフィルタで, アナログフィルタのような,
                遷移帯域幅のない<b>理想フィルタ</b>を実装することはコンピュータの原理上, 不可能となります. しかしながら, 可能な限り遷移帯域幅を小さくして,
                アナログフィルタ (理想フィルタ) に近づけることは可能であり,
                <b>理想フィルタに近いフィルタが, デジタルフィルタにおいて性能のよいフィルタの重要な指標となります</b>. そして, IIR フィルタは低次数で,
                性能のよいフィルタ, すなわち, 理想フィルタに近いフィルタを実装することが可能です.
              </p>
            </article>
            <article class="section-effectors-filter-transfer-function">
              <h5>FIR フィルタと IIR フィルタの伝達関数</h5>
              <p>
                これまで, <b>伝達関数</b>という用語をそれとなく使っていましたが, ここで詳細を解説します. 伝達関数とは, その名のとおり,
                <b>伝わりやすさ</b>を数学の関数として定義したものです. オーディオ信号処理に限定して定義すると,
                <b>入力音のスペクトルに対する出力音のスペクトルの比の関数</b>で定義できます.
              </p>
              <p>
                FIR フィルタの場合, <span class="math-inline">$x\left(n\right)$</span>, <span class="math-inline">$y\left(n\right)$</span>,
                <span class="math-inline">$b\left(m\right)$</span> をそれぞれ離散フーリエ変換した関数を <span class="math-inline">$X\left(k\right)$</span>,
                <span class="math-inline">$Y\left(k\right)$</span>, <span class="math-inline">$H\left(k\right)$</span> とすると,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &y\left(n\right) = \sum_{m = 0}^{N}b\left(m\right)x\left(n - m\right)
                  \end{flalign}
                $
              </div>
              <p>時間領域でのコンボリューション積分は周波数領域では乗算となるので,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &Y\left(k\right) = H\left(k\right)X\left(k\right)
                  \end{flalign}
                $
              </div>
              <p><span class="math-inline">$H\left(k\right)$</span> が伝達関数となるので, 式を変形すると, 入出力スペクトルの比になることがわかります.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &H\left(k\right) = \frac{Y\left(k\right)}{X\left(k\right)}
                  \end{flalign}
                $
              </div>
              <p>
                IIR フィルタも同様に, <span class="math-inline">$x\left(n\right)$</span>, <span class="math-inline">$y\left(n\right)$</span>,
                <span class="math-inline">$b\left(m\right)$</span>, <span class="math-inline">$a\left(m\right)$</span> をそれぞれ離散フーリエ変換した関数を
                <span class="math-inline">$X\left(k\right)$</span>, <span class="math-inline">$Y\left(k\right)$</span>,
                <span class="math-inline">$B\left(k\right)$</span>, <span class="math-inline">$A\left(k\right)$</span> とすると,
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &y\left(n\right) = \sum_{m = 0}^{J}b\left(m\right)x\left(n - m\right) - \sum_{m = 1}^{I}a\left(m\right)y\left(n - m\right)
                  \end{flalign}
                $
              </div>
              <p>時間領域でのコンボリューション積分は周波数領域では乗算となるので,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &Y\left(k\right) = B\left(k\right)X\left(k\right) - A\left(k\right)Y\left(k\right)
                  \end{flalign}
                $
              </div>
              <p>ここで, IIR フィルタの伝達関数を <span class="math-inline">$H\left(k\right)$</span> として, 式変形すると,</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &H\left(k\right) = \frac{Y\left(k\right)}{X\left(k\right)} = \frac{B\left(k\right)}{1 + A\left(k\right)}
                  \end{flalign}
                $
              </div>
              <p>
                FIR フィルタと異なり, フィードバックがあるので, その伝達関数は分数式として表現されます (<code>IIRFilterNode</code> の
                <span class="math-inline">$a_{0}$</span> が <code>0</code> 以外の値でなければならない理由です).
              </p>
              <p>
                ところで, 伝達関数をオーディオ信号処理に限らずに, 数学的に一般化すると (複素平面へ拡張すると), 離散フーリエ変換ではなく,
                <b><span class="math-inline">$z$</span> 変換</b>したあるシステムへの入出力比の関数となります (例えば, RIR は, ある室内をシステムとみなして,
                システムへの入力をインパルス音とした場合の出力ということに特化して説明できます).
              </p>
              <p><span class="math-inline">$z$</span> 変換での FIR フィルタ, IIR フィルタの伝達関数は以下のようになります.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &H\left(z\right) = \frac{Y\left(z\right)}{X\left(z\right)} \quad (FIR) \\
                    &H\left(z\right) = \frac{B\left(z\right)}{1 + A\left(z\right)} \quad (IIR) \\
                  \end{flalign}
                $
              </div>
              <p>
                言い換えると, <span class="math-inline">$z$</span> 変換での伝達関数を, 物理的な音のスペクトル比に特化すると,
                離散フーリエ変換での入出力比が伝達関数になると言えます.
              </p>
              <p>また, 音の伝達特性 (周波数特性) は, RIR 以外にも, 声道フィルタや HRTF (頭部伝達関数), スピーカーキャビネットの響きなどがあります.</p>
            </article>
          </section>
        </section>
        <section id="section-effectors-equalizer">
          <h3>イコライザー</h3>
          <p>
            フィルタの組み合わせのみでできるエフェクターとして, <b>イコライザー</b>があります. 元々は, アナログ方式で録音されていた時代に,
            振幅が大きくなってしまう低音域や振幅が小さくなってしまう高音域を等しくする (equalize) 用途で使われていましたが, 現在では,
            積極的に音を加工するエフェクターとして使われています. 楽器演奏や音楽制作ではもちろんですが, 音楽プレイヤーでもイコライザーは標準的に実装されており,
            音楽を聴く場合にもバリエーションを与えています.
          </p>
          <figure>
            <img src="images/macos-music-equalizer.png" alt="" width="433" height="219" loading="lazy" />
            <figcaption>音楽プレイヤーのイコライザー (macOS Music アプリ イコライザー)</figcaption>
          </figure>
          <p>
            イコライザーにはいくつか種類がありますが, 頻繁に使われるイコライザーとして, 低音域・中音域・高音域の 3 つの帯域を強調・減衰可能な
            <b>3 バンドイコライザー</b> (ギターアンプなどでは, 超高音域が追加されているイコライザーも多くあります) と, 10
            帯域ぐらいをきめ細かく強調・減衰可能な<b>グラフィックイコライザー</b>があります.
          </p>
          <p>
            このセクションでは, <code>BiquadFilterNode</code> を組み合わせて, 3 バンドイコライザーとグラフィックイコライザーの実装を解説します. また,
            イコライザーでは, 特定の周波数帯域を強調することを<b>ブースト</b>, 減衰させることを<b>カット</b>と呼ぶことが多いので,
            これ以降はこれらの用語を使うことにします.
          </p>
          <section id="section-effectors-3-bands-equalizer">
            <h4>3 バンドイコライザー</h4>
            <p>
              <b>3 バンドイコライザー</b>は, 低音域・中音域・高音域の 3 つの帯域をブースト・カット可能なイコライザーですが, 実装としては, Low-Shelving Filter,
              Peaking Filter, High-Shelving Filter を使うだけで実装できます. つまり, 3 つの帯域を制御する <code>BiquadFilterNode</code> インスタンスを生成して,
              接続することで実装可能です. このとき, それぞれのフィルタの
              <span class="math-inline">$f_{\mathrm{computed}}$</span> は厳密に決まっているわけではありませんが, 以下のような値が設定されることが多いようです.
            </p>
            <dl>
              <dt>Low-Shelving Filter (低音域)</dt>
              <dd><code>250 Hz</code> ~ <code>500 Hz</code></dd>
              <dt>Peaking Filter (中音域)</dt>
              <dd><code>1000 Hz</code> ~ <code>2000 Hz</code></dd>
              <dt>High-Shelving Filter (高音域)</dt>
              <dd><code>4000 Hz</code> ~ <code>8000 Hz</code></dd>
            </dl>
            <p>
              また, Peaking Filter のみ, <code>Q</code> プロパティの値を設定可能ですが, これも厳密に決まっているわけではありませんが, コード例としては
              <span class="math-inline">$\frac{1}{\sqrt{2}}$</span> を設定しています.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-3-bands-equalizer" width="320" height="920" />
              <figcaption>3 バンドイコライザーのノード接続図</figcaption>
            </figure>
            <p>
              現実世界のイコライザーでは, それぞれ 3 つのフィルタの <code>gain</code> プロパティの値を変更して, ブースト・カットします. また, それらのパラメータ
              (<code>gain</code> プロパティの値) は, 低音域は <b>Bass</b>, 中音域は <b>Middle</b>, 高音域は
              <b>Treble</b> として制御可能になっているイコライザーがほとんどです (ちなみに, 超高音域がある場合, <b>Presence</b> となっています).
            </p>
            <figure>
              <div class="app-headline">
                <label for="range-3-bands-equalizer-bass-gain">Bass</label>
                <input type="range" id="range-3-bands-equalizer-bass-gain" value="0" min="-24" max="24" step="1" />
                <span id="print-3-bands-equalizer-bass-gain">0 dB</span>
                <label for="range-3-bands-equalizer-middle-gain">Middle</label>
                <input type="range" id="range-3-bands-equalizer-middle-gain" value="0" min="-24" max="24" step="1" />
                <span id="print-3-bands-equalizer-middle-gain">0 dB</span>
                <label for="range-3-bands-equalizer-treble-gain">Treble</label>
                <input type="range" id="range-3-bands-equalizer-treble-gain" value="0" min="-24" max="24" step="1" />
                <span id="print-3-bands-equalizer-treble-gain">0 dB</span>
              </div>
              <svg id="svg-figure-filter-response-3-bands-equalizer" width="600" height="300" />
              <figcaption>3 バンドイコライザーのフィルタ特性</figcaption>
            </figure>
            <p>
              以下は, 上記のフィルタ特性となる 3 バンドイコライザーを実際のアプリケーションを想定して, ユーザーインタラクティブに, 3
              つの周波数帯域をブースト・カットできるようにしたコード例です. 原音を直接変化させるインサートエフェクトなので, 本質的な実装は, 3 つの帯域を制御する
              <code>BiquadFilterNode</code> インスタンスの接続処理です. 現実世界の 3 バンドイコライザーでは, ユーザーが操作できるパラメータではありませんが,
              <span class="math-inline">$f_{\mathrm{computed}}$</span> の値や Peaking Filter の <code>Q</code> プロパティの値なども変更してみて,
              好みの値を探索してみるのもよいと思います.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-3-bands-equalizer&quot; checked /&gt;
  &lt;span id=&quot;print-checked-3-bands-equalizer&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label&gt;
  &lt;span&gt;OscillatorNode frequency&lt;/span&gt;
  &lt;input type="range" id="range-3-bands-equalizer-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" /&gt;
  &lt;span id="print-3-bands-equalizer-oscillator-frequency-value"&gt;440 Hz&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-3-bands-equalizer-bass&quot;&gt;Bass&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-3-bands-equalizer-bass&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-3-bands-equalizer-bass-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-3-bands-equalizer-middle&quot;&gt;Middle&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-3-bands-equalizer-middle&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-3-bands-equalizer-middle-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-3-bands-equalizer-treble&quot;&gt;Treble&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-3-bands-equalizer-treble&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-3-bands-equalizer-treble-value&quot;&gt;0 dB&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let frequency = 440;

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency });

let isStop = true;

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;, frequency: 250 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;, frequency: 1000, Q: Math.SQRT1_2 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 4000 });

const buttonElement   = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeBassElement   = document.getElementById(&apos;range-3-bands-equalizer-bass&apos;);
const rangeMiddleElement = document.getElementById(&apos;range-3-bands-equalizer-middle&apos;);
const rangeTrebleElement = document.getElementById(&apos;range-3-bands-equalizer-treble&apos;);

const spanPrintCheckedElement = document.getElementById(&apos;print-checked-3-bands-equalizer&apos;);
const spanPrintBassElement    = document.getElementById(&apos;print-3-bands-equalizer-bass-value&apos;);
const spanPrintMiddleElement  = document.getElementById(&apos;print-3-bands-equalizer-middle-value&apos;);
const spanPrintTrebleElement  = document.getElementById(&apos;print-3-bands-equalizer-treble-value&apos;);

const rangeOscillatorFrequencyElement     = document.getElementById(&apos;range-3-bands-equalizer-oscillator-frequency&apos;);
const spanPrintOscillatorFrequencyElement = document.getElementById(&apos;print-3-bands-equalizer-oscillator-frequency-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);

  if (checkboxElement.checked) {
    // OscillatorNode (Input) -&gt; Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; AudioDestinationNode (Output)
    oscillator.connect(bass);
    bass.connect(middle);
    middle.connect(treble);
    treble.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;ON&apos;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes (Equalizer ON)
    // OscillatorNode (Input) -&gt; Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; AudioDestinationNode (Output)
    oscillator.connect(bass);
    bass.connect(middle);
    middle.connect(treble);
    treble.connect(context.destination);
  } else {
    // Connect nodes (Equalizer OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);
  }

  // Start oscillator
  oscillator.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeOscillatorFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  frequency = event.currentTarget.valueAsNumber;

  if (oscillator) {
    oscillator.frequency.value = frequency
  }

  spanPrintOscillatorFrequencyElement.textContent = `${frequency} Hz`;
});

rangeBassElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const gain = event.currentTarget.valueAsNumber;

  bass.gain.value = gain;

  spanPrintBassElement.textContent = `${gain} dB`;
});

rangeMiddleElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const gain = event.currentTarget.valueAsNumber;

  middle.gain.value = gain;

  spanPrintMiddleElement.textContent = `${gain} dB`;
});

rangeTrebleElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const gain = event.currentTarget.valueAsNumber;

  treble.gain.value = gain;

  spanPrintTrebleElement.textContent = `${gain} dB`;
});</code></pre>
            <div class="app-container app-3-bands-equalizer">
              <div class="app-headline">
                <button type="button" id="button-3-bands-equalizer">start</button>
                <label>
                  <input type="checkbox" id="checkbox-3-bands-equalizer" checked />
                  <span id="print-checked-3-bands-equalizer">ON</span>
                </label>
                <label>
                  <span>OscillatorNode frequency</span>
                  <input type="range" id="range-3-bands-equalizer-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" />
                  <span id="print-3-bands-equalizer-oscillator-frequency-value">440 Hz</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-3-bands-equalizer-bass">Bass</label></dt>
                    <dd>
                      <input type="range" id="range-3-bands-equalizer-bass" value="0" min="-24" max="24" step="1" />
                      <span id="print-3-bands-equalizer-bass-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-3-bands-equalizer-middle">Middle</label></dt>
                    <dd>
                      <input type="range" id="range-3-bands-equalizer-middle" value="0" min="-24" max="24" step="1" />
                      <span id="print-3-bands-equalizer-middle-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-3-bands-equalizer-treble">Treble</label></dt>
                    <dd>
                      <input type="range" id="range-3-bands-equalizer-treble" value="0" min="-24" max="24" step="1" />
                      <span id="print-3-bands-equalizer-treble-value">0 dB</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-graphic-equalizer">
            <h4>グラフィックイコライザー</h4>
            <p>
              <b>グラフィックイコライザー</b>は, 10 帯域ほどの周波数成分をブースト・カットできるイコライザーですが, 実装としては, Peaking Filter
              を制御したい周波数帯域の数だけ接続するだけです. <span class="math-inline">$f_{\mathrm{computed}}$</span> を制御したい周波数帯域に設定します.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-graphic-equalizer" width="720" height="1020" />
              <figcaption>グラフィックイコライザーのノード接続図</figcaption>
            </figure>
            <figure>
              <div class="app-headline grid-3x3-layout">
                <div>
                  <label for="range-graphic-equalizer-32Hz-gain">32 Hz</label>
                  <input type="range" id="range-graphic-equalizer-32Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-32Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-62Hz-gain">62.5 Hz</label>
                  <input type="range" id="range-graphic-equalizer-62Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-62Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-125Hz-gain">125 Hz</label>
                  <input type="range" id="range-graphic-equalizer-125Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-125Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-250Hz-gain">250 Hz</label>
                  <input type="range" id="range-graphic-equalizer-250Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-250Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-500Hz-gain">500 Hz</label>
                  <input type="range" id="range-graphic-equalizer-500Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-500Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-1000Hz-gain">1000 Hz</label>
                  <input type="range" id="range-graphic-equalizer-1000Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-1000Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-2000Hz-gain">2000 Hz</label>
                  <input type="range" id="range-graphic-equalizer-2000Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-2000Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-4000Hz-gain">4000 Hz</label>
                  <input type="range" id="range-graphic-equalizer-4000Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-4000Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-8000Hz-gain">8000 Hz</label>
                  <input type="range" id="range-graphic-equalizer-8000Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-8000Hz-gain">0 dB</span>
                </div>
                <div>
                  <label for="range-graphic-equalizer-16000Hz-gain">1.6 kHz</label>
                  <input type="range" id="range-graphic-equalizer-16000Hz-gain" value="0" min="-24" max="24" step="1" />
                  <span id="print-graphic-equalizer-16000Hz-gain">0 dB</span>
                </div>
              </div>
              <svg id="svg-figure-filter-response-graphic-equalizer" width="600" height="300" />
              <figcaption>グラフィックイコライザーのフィルタ特性</figcaption>
            </figure>
            <p>
              以下は, 上記のフィルタ特性となるグラフィックイコライザーを実際のアプリケーションを想定して, ユーザーインタラクティブに,
              各周波数帯域をブースト・カットできるようにしたコード例です.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-graphic-equalizer&quot; checked /&gt;
  &lt;span id=&quot;print-checked-graphic-equalizer&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label&gt;
  &lt;span&gt;OscillatorNode frequency&lt;/span&gt;
  &lt;input type="range" id="range-graphic-equalizer-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" /&gt;
  &lt;span id="print-graphic-equalizer-oscillator-frequency-value"&gt;440 Hz&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-graphic-equalizer-32Hz&quot;&gt;32 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-32Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-32Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-62Hz&quot;&gt;62.5 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-62Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-62Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-125Hz&quot;&gt;125 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-125Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-125Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-250Hz&quot;&gt;250 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-250Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-250Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-500Hz&quot;&gt;500 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-500Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-500Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-1000Hz&quot;&gt;1000 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-1000Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-1000Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-2000Hz&quot;&gt;2000 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-2000Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-2000Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-4000Hz&quot;&gt;4000 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-4000Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-4000Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-8000Hz&quot;&gt;8000 Hz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-8000Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-8000Hz-value&quot;&gt;0 dB&lt;/span&gt;
&lt;label for=&quot;range-graphic-equalizer-16000Hz&quot;&gt;1.6 kHz&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-graphic-equalizer-16000Hz&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-graphic-equalizer-16000Hz-value&quot;&gt;0 dB&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let frequency = 440;

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency });

let isStop = true;

const buttonElement   = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const spanPrintCheckedElement = document.getElementById(&apos;print-checked-graphic-equalizer&apos;);

const rangeOscillatorFrequencyElement     = document.getElementById(&apos;range-graphic-equalizer-oscillator-frequency&apos;);
const spanPrintOscillatorFrequencyElement = document.getElementById(&apos;print-graphic-equalizer-oscillator-frequency-value&apos;);

const centerFrequencies = [32, 62.5, 125, 250, 500, 1000, 2000, 4000, 8000, 16000];

const peakingFilters = centerFrequencies.map((frequency) =&gt; {
  return new BiquadFilterNode(context, { type: &apos;peaking&apos;, frequency, Q: Math.SQRT1_2 });
});

centerFrequencies.forEach((frequency, index) =&gt; {
  document.getElementById(`range-graphic-equalizer-${Math.trunc(frequency)}Hz`).addEventListener(&apos;input&apos;, (event) =&gt; {
    const peakingFilter = peakingFilters[index];

    peakingFilter.gain.value = event.currentTarget.valueAsNumber;

    document.getElementById(`print-graphic-equalizer-${Math.trunc(frequency)}Hz-value`).textContent = `${peakingFilter.gain.value} dB`;
  });
});

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);

  if (checkboxElement.checked) {
    oscillator.connect(peakingFilters[0]);

    for (let i = 0, len = peakingFilters.length - 1; i &lt; len; i++) {
      peakingFilters[i].connect(peakingFilters[i + 1]);
    }

    peakingFilters[peakingFilters.length - 1].connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    oscillator.connect(peakingFilters[0]);

    for (let i = 0, len = peakingFilters.length - 1; i &lt; len; i++) {
      peakingFilters[i].connect(peakingFilters[i + 1]);
    }

    peakingFilters[peakingFilters.length - 1].connect(context.destination);
  } else {
    oscillator.connect(context.destination);
  }

  oscillator.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  oscillator.stop(0);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeOscillatorFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  frequency = event.currentTarget.valueAsNumber;

  if (oscillator) {
    oscillator.frequency.value = frequency;
  }

  spanPrintOscillatorFrequencyElement.textContent = `${frequency} Hz`;
});</code></pre>
            <div class="app-container app-graphic-equalizer">
              <div class="app-headline">
                <button type="button" id="button-graphic-equalizer">start</button>
                <label>
                  <input type="checkbox" id="checkbox-graphic-equalizer" checked />
                  <span id="print-checked-graphic-equalizer">ON</span>
                </label>
                <label>
                  <span>OscillatorNode frequency</span>
                  <input type="range" id="range-graphic-equalizer-oscillator-frequency" value="440" min="27.5" max="4000" step="0.5" />
                  <span id="print-graphic-equalizer-oscillator-frequency-value">440 Hz</span>
                </label>
              </div>
              <div>
                <dl class="grid-3x3-layout">
                  <div>
                    <dt><label for="range-graphic-equalizer-32Hz">32 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-32Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-32Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-62Hz">62.5 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-62Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-62Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-125Hz">125 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-125Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-125Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-250Hz">250 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-250Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-250Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-500Hz">500 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-500Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-500Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-1000Hz">1000 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-1000Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-1000Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-2000Hz">2000 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-2000Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-2000Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-4000Hz">4000 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-4000Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-4000Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-8000Hz">8000 Hz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-8000Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-8000Hz-value">0 dB</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-graphic-equalizer-16000Hz">1.6 kHz</label></dt>
                    <dd>
                      <input type="range" id="range-graphic-equalizer-16000Hz" value="0" min="-24" max="24" step="1" />
                      <span id="print-graphic-equalizer-16000Hz-value">0 dB</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
        </section>
        <section id="section-effectors-wah">
          <h3>ワウ</h3>
          <p>
            フィルタを利用したよく使われるエフェクターとして, <b>ワウ</b>があります. 原理は, Low-Pass Filter のカットオフ周波数, もしくは, Band-Pass Filter
            の中心周波数 (<span class="math-inline">$f_{\mathrm{computed}}$</span>) を時間経過とともに変化させることによって,
            まさに「ワ」「ウ」と発声しているようなエフェクト音を生成することができます (エレキギターなどでは, いわゆる飛び道具的なエフェクターとして使われます).
            音声分析合成技術である, <b>ボコーダー</b> (<b>Vocoder</b>: <b>Voice Coder</b>) と似ているエフェクト音ですが, ワウの発明起源は諸説あるようです (また,
            ボコーダーから, さらに音楽用途に改良して使われるようになったエフェクターとしては, <b>フェーズボコーダ</b>があります).
          </p>
          <p>
            ワウを実装するためには, <span class="math-inline">$f_{\mathrm{computed}}$</span> (Low-Pass Filter のカットオフ周波数, もしくは, Band-Pass Filter
            の中心周波数) を時間経過とともに変化させる必要があります. これをワウペダル (参考:
            <a href="https://voxamps.com/ja/product/v847-wah-pedal/" target="_blank" rel="noopener noreferrer">VOX 社のワウペダル</a>)
            と呼ばれるコントローラーで変化させるするのが<b>ペダルワウ</b>, 楽器の発音時の振幅の変化 (打弦や撥弦の強弱)
            に応じて変化させるのが<b>オートワウ</b>となります .
          </p>
          <figure>
            <svg id="svg-figure-wah-principle" width="600" height="300" />
            <figcaption>
              <span>ワウの原理 (<span class="math-inline">$f_{\mathrm{computed}}$</span> が時間経過とともに変化することに着目してください)</span>
              <button type="button" id="button-wah-principle-animation">start</button>
            </figcaption>
          </figure>
          <p>ワウは原音を直接変化させるエフェクターなので, インサートエフェクトとなります.</p>
          <section id="section-effectors-pedal-wah">
            <h4>ペダルワウ</h4>
            <p>
              Web ブラウザでペダルワウを実装するには, ハードウェア的な制約があるので, このセクションでは,
              <b>LFO を擬似的なワウペダルとして</b>ペダルワウの実装を解説します (もし, Web MIDI API の理解があり, MIDI 機器があれば, ベロシティなどに応じて
              <span class="math-inline">$f_{\mathrm{computed}}$</span> を変化させるようにすると, よりペダルワウに近い体験ができると思います).
            </p>
            <p>
              原理をそのまま実装に落とし込めば, ペダルワウの実装はトレモロなどと同じです. <code>BiquadFilterNode</code> を接続して,
              <code>frequency</code> プロパティ (<code>AudioParam</code>) に LFO を接続して時間経過とともに
              <span class="math-inline">$f_{\mathrm{computed}}$</span> を変化させることで実装可能です.
              <b>Low-Pass Filter を使う場合, カットオフ周波数付近の急峻を鋭くしておく必要があるので</b>, <code>Q</code> プロパティの値は <code>10</code> ~
              <code>20</code> 程度に設定しておく必要があります (Band-Pass Filter の場合は, デフォルト値で問題ありません).
            </p>
            <p>
              <code>IIRFilterNode</code> を使っても実装は可能ですが, その場合, カットオフ周波数は <code>AudioParam</code> ではないので, LFO も AudioWorklet
              を利用して実装する必要が生じます. 特にフィルタのチューニングが必要なければ (奇数次の IIR フィルタを使わなければならない理由があるなど),
              <code>BiquadFilterNode</code> を使うほうが実装は簡潔になります.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-pedal-wah" width="800" height="520" />
              <figcaption>ペダルワウのノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const cutoff    = 880;
const depthRate = 0.5;
const rateValue = 0.5;
const resonance = 10;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: cutoff, Q: resonance });

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

const lfo   = new OscillatorNode(context, { frequency: rateValue });
const depth = new GainNode(context, { gain: cutoff * depthRate });

// Connect nodes
// OscillatorNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
oscillator.connect(lowpass);
lowpass.connect(context.destination);

// Connect nodes for LFO that changes Low-Pass Filter&apos;s frequency periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; delayTime (AudioParam)
lfo.connect(depth);
depth.connect(lowpass.frequency);

// Start oscillator and LFO
oscillator.start(0);
lfo.start(0);

// Stop oscillator and LFO
oscillator.stop(context.currentTime + 10);
lfo.stop(context.currentTime + 10);</code></pre>
            <p>
              以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, ペダルワウに関わるパラメータを制御できるようにしたコード例です.
              一般的なペダルワウでは, ワウペダルのプレッシャーレベルに応じて, カットオフ周波数 (<span class="math-inline">$f_{\mathrm{computed}}$</span>)
              が変化するようになっていますが, LFO を擬似的なワウペダルとした実装なので, 基準となる <span class="math-inline">$f_{\mathrm{computed}}$</span> と
              Depth でワウペダルのプレッシャーレベルを, Rate でペダルの動きを擬似的に実装しています.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-pedal-wah&quot; checked /&gt;
  &lt;span id=&quot;print-checked-pedal-wah&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-pedal-wah-cutoff&quot;&gt;Cutoff Frequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-pedal-wah-cutoff&quot; value=&quot;1000&quot; min=&quot;1000&quot; max=&quot;2000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-pedal-wah-cutoff-value&quot;&gt;1000 Hz&lt;/span&gt;
&lt;label for=&quot;range-pedal-wah-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-pedal-wah-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-pedal-wah-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-pedal-wah-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-pedal-wah-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
&lt;span id=&quot;print-pedal-wah-rate-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-pedal-wah-resonance&quot;&gt;Resonance&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-pedal-wah-resonance&quot; value=&quot;1&quot; min=&quot;1&quot; max=&quot;20&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-pedal-wah-resonance-value&quot;&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let cutoff    = 1000;
let depthRate = 0;
let rateValue = 0;
let resonance = 1;

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });
let lfo        = new OscillatorNode(context, { frequency: rateValue });

let isStop = true;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: cutoff, Q: resonance });
const depth   = new GainNode(context, { gain: lowpass.frequency.value * depthRate });

const buttonElement   = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeCutoffElement    = document.getElementById(&apos;range-pedal-wah-cutoff&apos;);
const rangeDepthElement     = document.getElementById(&apos;range-pedal-wah-depth&apos;);
const rangeRateElement      = document.getElementById(&apos;range-pedal-wah-rate&apos;);
const rangeResonanceElement = document.getElementById(&apos;range-pedal-wah-resonance&apos;);

const spanPrintCheckedElement   = document.getElementById(&apos;print-checked-pedal-wah&apos;);
const spanPrintCutoffElement    = document.getElementById(&apos;print-pedal-wah-cutoff-value&apos;);
const spanPrintDepthElement     = document.getElementById(&apos;print-pedal-wah-depth-value&apos;);
const spanPrintRateElement      = document.getElementById(&apos;print-pedal-wah-rate-value&apos;);
const spanPrintResonanceElement = document.getElementById(&apos;print-pedal-wah-resonance-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);
  lowpass.disconnect(0);
  lfo.disconnect(0);

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    oscillator.connect(lowpass);
    lowpass.connect(context.destination);

    // Connect nodes for LFO that changes Low-Pass Filter&apos;s frequency periodically
    // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
    lfo.connect(depth);
    depth.connect(lowpass.frequency);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
    oscillator.connect(lowpass);
    lowpass.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  } else {
    lowpass.disconnect(0);

    // Connect nodes (Pedal Wah OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  }

  // Connect nodes for LFO that changes Low-Pass Filter&apos;s frequency periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
  lfo.connect(depth);
  depth.connect(lowpass.frequency);

  // Start LFO
  lfo.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeCutoffElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  cutoff = event.currentTarget.valueAsNumber;

  lowpass.frequency.value = cutoff;

  spanPrintCutoffElement.textContent = `${cutoff.toString(10)} Hz`;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = lowpass.frequency.value * depthRate;

  spanPrintDepthElement.textContent = depthRate.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});

rangeResonanceElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  resonance = event.currentTarget.valueAsNumber;

  lowpass.Q.value = resonance;

  spanPrintResonanceElement.textContent = resonance.toString(10);
});</code></pre>
            <div class="app-container app-pedal-wah">
              <div class="app-headline">
                <button type="button" id="button-pedal-wah">start</button>
                <label>
                  <input type="checkbox" id="checkbox-pedal-wah" checked />
                  <span id="print-checked-pedal-wah">ON</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-pedal-wah-cutoff">Cutoff Frequency</label></dt>
                    <dd>
                      <input type="range" id="range-pedal-wah-cutoff" value="1000" min="1000" max="2000" step="1" />
                      <span id="print-pedal-wah-cutoff-value">1000 Hz</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-pedal-wah-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-pedal-wah-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-pedal-wah-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-pedal-wah-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-pedal-wah-rate" value="0" min="0" max="10" step="0.5" />
                      <span id="print-pedal-wah-rate-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-pedal-wah-resonance">Resonance</label></dt>
                    <dd>
                      <input type="range" id="range-pedal-wah-resonance" value="1" min="1" max="20" step="1" />
                      <span id="print-pedal-wah-resonance-value">1</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-vcf-auto-wah">
            <h4>オートワウ</h4>
            <p>
              汎用的なオートワウを実装するためには, <code>WaveShaperNode</code> クラスの理解が必要であったり, ノード接続が複雑であったりするので,
              このセクションでは, すでに解説した <code>BiquadFilterNode</code> の <code>frequency</code> プロパティ (<code>AudioParam</code>)
              のオートメーションメソッドを利用した簡易的な実装を解説します (これは, アナログシンセサイザーの <b>VCF</b> (<b>Voltage Controlled Filter</b>:
              <b>電圧制御フィルタ</b>) の実装でもあります).
            </p>
            <p>
              エンベロープジェネレーターと同様のオートメーションを <code>BiquadFilterNode</code> の <code>frequency</code> プロパティに実装することです. そして,
              それに対応するように, <code>GainNode</code> の <code>gain</code> プロパティにもオートメーションを適用すれば, 同様のスケジューリングで,
              振幅に応じて <span class="math-inline">$f_{\mathrm{computed}}$</span> も変化することになるので, オートワウが実装できます. 現実的には,
              オートワウが有用なのは, このような, ユーザーインタラクティブな操作に応じて, 振幅の変化とともに,
              <span class="math-inline">$f_{\mathrm{computed}}$</span> を変化させればよいケースがほとんどなので, 簡易的なオートワウでも十分なことが多いでしょう.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const cutoff       = 1000;
const targetCutoff = 2000;
const resonance    = 10;

const attack  = 1.0;
const decay   = 0.5;
const sustain = 0.5;
const release = 1.0;

const envelopegenerator = new GainNode(context);

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

let isStop = true;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: cutoff, Q: resonance });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  // Connect nodes
  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(lowpass);
  lowpass.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = lowpass.frequency.value * sustain;

  // Attack -&gt; Decay -&gt; Sustain
  envelopegenerator.gain.cancelScheduledValues(t0);
  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(sustain, t1, t2);

  lowpass.frequency.cancelScheduledValues(t0);
  lowpass.frequency.setValueAtTime(cutoff, t0);
  lowpass.frequency.exponentialRampToValueAtTime(targetCutoff, t1);
  lowpass.frequency.setTargetAtTime(t2Level, t1, t2);

  // Start oscillator
  oscillator.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  // Sustain -&gt; Release
  envelopegenerator.gain.cancelScheduledValues(t3);
  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  lowpass.frequency.cancelScheduledValues(t3);
  lowpass.frequency.setTargetAtTime(cutoff, t3, t4);

  // Stop after Release
  oscillator.stop(t3 + t4);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});</code></pre>
            <p>
              ワウは飛び道具的なエフェクターなので, <code>linearRampToValueAtTime</code> メソッドではなく,
              <code>exponentialRampToValueAtTime</code> メソッドで指数関数的に変化させてみました (もちろん, 線形的に変化させる
              <code>linearRampToValueAtTime</code> メソッドでもオートワウとしては機能します).
            </p>
            <p>
              以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, (パラメータのオートメーションによる)
              オートワウに関わるパラメータを制御できるようにしたコード例です.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label for="range-vcf-auto-wah-attack"&gt;attack&lt;/label&gt;
&lt;input type="range" id="range-vcf-auto-wah-attack" value="1" min="0" max="1" step="0.01" /&gt;
&lt;span id="print-vcf-auto-wah-attack-value"&gt;1&lt;/span&gt;
&lt;label for="range-vcf-auto-wah-decay"&gt;decay&lt;/label&gt;
&lt;input type="range" id="range-vcf-auto-wah-decay" value="0.3" min="0" max="1" step="0.01" /&gt;
&lt;span id="print-vcf-auto-wah-decay-value"&gt;0.3&lt;/span&gt;
&lt;label for="range-vcf-auto-wah-sustain"&gt;sustain&lt;/label&gt;
&lt;input type="range" id="range-vcf-auto-wah-sustain" value="0.5" min="0" max="1" step="0.01" /&gt;
&lt;span id="print-vcf-auto-wah-sustain-value"&gt;0.5&lt;/span&gt;
&lt;label for="range-vcf-auto-wah-release"&gt;release&lt;/label&gt;
&lt;input type="range" id="range-vcf-auto-wah-release" value="1" min="0" max="1" step="0.01" /&gt;
&lt;span id="print-vcf-auto-wah-release-value"&gt;1&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const cutoff       = 1000;
const targetCutoff = 2000;
const resonance    = 10;

let attack  = 1.0;
let decay   = 0.5;
let sustain = 0.5;
let release = 1.0;

const envelopegenerator = new GainNode(context);

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

let isStop = true;

const lowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: cutoff, Q: resonance });

const buttonElement = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);

const rangeAttackElement  = document.getElementById(&apos;range-vcf-auto-wah-attack&apos;);
const rangeDecayElement   = document.getElementById(&apos;range-vcf-auto-wah-decay&apos;);
const rangeSustainElement = document.getElementById(&apos;range-vcf-auto-wah-sustain&apos;);
const rangeReleaseElement = document.getElementById(&apos;range-vcf-auto-wah-release&apos;);

const spanPrintAttackElement  = document.getElementById(&apos;print-vcf-auto-wah-attack-value&apos;);
const spanPrintDecayElement   = document.getElementById(&apos;print-vcf-auto-wah-decay-value&apos;);
const spanPrintSutainElement  = document.getElementById(&apos;print-vcf-auto-wah-sustain-value&apos;);
const spanPrintReleaseElement = document.getElementById(&apos;print-vcf-auto-wah-release-value&apos;);

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  // Connect nodes
  // OscillatorNode (Input) -&gt; GainNode (Envelope Generator) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; AudioDestinationNode (Output)
  oscillator.connect(envelopegenerator);
  envelopegenerator.connect(lowpass);
  lowpass.connect(context.destination);

  const t0 = context.currentTime;
  const t1 = t0 + attack;
  const t2 = decay;

  const t2Level = lowpass.frequency.value * sustain;

  // Attack -&gt; Decay -&gt; Sustain
  envelopegenerator.gain.cancelScheduledValues(t0);
  envelopegenerator.gain.setValueAtTime(0, t0);
  envelopegenerator.gain.linearRampToValueAtTime(1, t1);
  envelopegenerator.gain.setTargetAtTime(sustain, t1, t2);

  lowpass.frequency.cancelScheduledValues(t0);
  lowpass.frequency.setValueAtTime(cutoff, t0);
  lowpass.frequency.exponentialRampToValueAtTime(targetCutoff, t1);
  lowpass.frequency.setTargetAtTime(t2Level, t1, t2);

  // Start oscillator
  oscillator.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  const t3 = context.currentTime;
  const t4 = release;

  // Sustain -&gt; Release
  envelopegenerator.gain.cancelAndHoldAtTime(t3);
  envelopegenerator.gain.setTargetAtTime(0, t3, t4);

  lowpass.frequency.cancelAndHoldAtTime(t3);
  lowpass.frequency.setTargetAtTime(cutoff, t3, t4);

  // Stop after Release (+ until nearly zero audio data)
  oscillator.stop(t3 + t4 + 5);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeAttackElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  attack = event.currentTarget.valueAsNumber;

  spanPrintAttackElement.textContent = attack.toString(10);
});

rangeDecayElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  decay = event.currentTarget.valueAsNumber;

  spanPrintDecayElement.textContent = decay.toString(10);
});

rangeSustainElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  sustain = event.currentTarget.valueAsNumber;

  spanPrintSutainElement.textContent = sustain.toString(10);
});

rangeReleaseElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  release = event.currentTarget.valueAsNumber;

  spanPrintReleaseElement.textContent = release.toString(10);
});</code></pre>
            <div class="app-container app-vcf-auto-wah">
              <div class="app-headline">
                <button type="button" id="button-vcf-auto-wah">start</button>
              </div>
              <div class="ranges-envelopegenerator">
                <dl>
                  <dt><label for="range-vcf-auto-wah-attack">attack</label></dt>
                  <dd>
                    <input type="range" id="range-vcf-auto-wah-attack" value="1" min="0" max="1" step="0.01" />
                    <span id="print-vcf-auto-wah-attack-value">1</span>
                  </dd>
                  <dt><label for="range-vcf-auto-wah-decay">decay</label></dt>
                  <dd>
                    <input type="range" id="range-vcf-auto-wah-decay" value="0.3" min="0" max="1" step="0.01" />
                    <span id="print-vcf-auto-wah-decay-value">0.3</span>
                  </dd>
                  <dt><label for="range-vcf-auto-wah-sustain">sustain</label></dt>
                  <dd>
                    <input type="range" id="range-vcf-auto-wah-sustain" value="0.5" min="0" max="1" step="0.01" />
                    <span id="print-vcf-auto-wah-sustain-value">0.5</span>
                  </dd>
                  <dt><label for="range-vcf-auto-wah-release">release</label></dt>
                  <dd>
                    <input type="range" id="range-vcf-auto-wah-release" value="1" min="0" max="1" step="0.01" />
                    <span id="print-vcf-auto-wah-release-value">1</span>
                  </dd>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effector-auto-wah">
            <h4>汎用的なオートワウ</h4>
            <p>
              汎用的なオートワウを実装するためには, <b><code>WaveShaperNode</code></b> クラスを理解して, <b>エンベロープフォロワー</b>を実装する必要があるので,
              ここでは, とりあえず形式的に (以下のように実装すればオートワウができる程度に) 理解していただければだいじょうぶです (<code>WaveShaperNode</code>
              クラスやエンベロープフォロワーに関しては,
              <a href="#section-effectors-distortion">ディストーション (歪み系エフェクター) のセクション</a>で詳細を解説します).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const depthValue = 0.25;
const rateValue  = 1;

const envelopeFollower = new WaveShaperNode(context, { buffer: new Float32Array([1, 0, 1]) });
const sensitivity      = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 2000, Q: 10 });
const lowpass          = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 2, Q: 1 });

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });
const lfo        = new OscillatorNode(context, { frequency: rateValue });

const depth = new GainNode(context, { gain: 1000 });

const lfoDepth = new GainNode(context, { gain: depthValue });

const amplitude = new GainNode(context, { gain: 0.5 });  // 0.5 +- ${depthValue}

// Connect nodes
// OscillatorNode (Input) -&gt; GainNode (Tremolo) -&gt; BiquadFilterNode (Sensitivity) -&gt; GainNode (Output)
oscillator.connect(amplitude);
amplitude.connect(sensitivity);
sensitivity.connect(context.destination);

// OscillatorNode (Input) -&gt; WaveShaperNode (Envelope Follower) -&gt; WaveShaperNode (Envelope Follower) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
oscillator.connect(envelopeFollower);
envelopeFollower.connect(lowpass);
lowpass.connect(depth);
depth.connect(sensitivity.frequency);

lfo.connect(lfoDepth);
lfoDepth.connect(amplitude.gain);

// Start oscillator
oscillator.start(0);
lfo.start(0);

// Stop oscillator
oscillator.stop(context.currentTime + 5);
lfo.stop(context.currentTime + 5);</code></pre>
            <p>
              音源の振幅の変化をシミュレートするために, トレモロを使っています (したがって, ここでのトレモロは, オートワウの実装には直接的に関係ありません).
            </p>
            <p>
              ノード接続は, 大きく 2 つで構成されています. 1 つは, 音源と振幅に対する感度のための Low-Pass Filter, そして
              <code>AudioDestinationNode</code> の接続と, もう 1 つは, 音源の振幅を<b>エンベロープフォロワー</b>に接続することによって,
              <b>全波整流</b> (端的には, 振幅の絶対値に波形を整形する処理) を施し, それをもとに, 感度の Low-Pass Filter の
              <code>frequency</code> プロパティを振幅に応じて変化させるための (<code>AudioParam</code> への) 接続です.
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-auto-wah" width="1200" height="720" />
              <figcaption>オートワウのノード接続図</figcaption>
            </figure>
            <p>
              全波整流した原音を, カットオフ周波数が非常に低い Low-Pass Filter を通過させることで, 波形 (エンベロープ) ではなく, 振幅の変化を出力とします.
              あとは, 振幅に応じて Depth の値が変化するようにすれば, Sensitivity の Low-Pass Filter の
              <span class="math-inline">$f_{\mathrm{computed}}$</span> が原音の振幅の変化に応じて変化することになり, (汎用的な) オートワウが実装できます.
            </p>
            <p>
              以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, オートワウに関わるパラメータを制御できるようにしたコード例です.
              原音はハードコーディングしていますが, <code>OscillatorNode</code> だけでなく, ワンショットオーディオや
              <code>MediaStreamAudioSourceNode</code> をオーディオソースとして音声に適用するのもおもしろいかもしれません.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-auto-wah&quot; checked /&gt;
  &lt;span id=&quot;print-checked-auto-wah&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-auto-wah-sensitivity&quot;&gt;Sensitivity&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-auto-wah-sensitivity&quot; value=&quot;1000&quot; min=&quot;1000&quot; max=&quot;2000&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-auto-wah-sensitivity-value&quot;&gt;1000 Hz&lt;/span&gt;
&lt;label for=&quot;range-auto-wah-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-auto-wah-depth&quot; value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-auto-wah-depth-value&quot;&gt;0.5&lt;/span&gt;
&lt;label for=&quot;range-auto-wah-resonance&quot;&gt;Resonance&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-auto-wah-resonance&quot; value=&quot;10&quot; min=&quot;1&quot; max=&quot;40&quot; step=&quot;1&quot; /&gt;
&lt;span id=&quot;print-auto-wah-resonance-value&quot;&gt;10&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let sensitivityFrequency = 2000;
let sensitivityDepthRate = 0.5;
let sensitivityResonance = 10;

const tremoloDepthValue = 0.25;
const tremoloRateValue  = 1;

let oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });
let lfo        = new OscillatorNode(context, { frequency: tremoloRateValue });

// for Tremolo (simulates amplitude modulation)
const lfoDepth  = new GainNode(context, { gain: tremoloDepthValue });
const amplitude = new GainNode(context, { gain: 0.5 });  // 0.5 +- ${tremoloDepthValue}

lfo.connect(lfoDepth);
lfoDepth.connect(amplitude.gain);

// Start LFO (simulates amplitude modulation)
lfo.start(0);

const envelopeFollower = new WaveShaperNode(context, { buffer: new Float32Array([1, 0, 1]) });
const sensitivity      = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: sensitivityFrequency, Q: sensitivityResonance });
const lowpass          = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 2, Q: 1 });

const depth = new GainNode(context, { gain: sensitivity.frequency.value * sensitivityDepthRate });

let isStop = true;

const buttonElement   = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeSensitivityFrequencyElement = document.getElementById(&apos;range-auto-wah-sensitivity&apos;);
const rangeSensitivityDepthElement     = document.getElementById(&apos;range-auto-wah-depth&apos;);
const rangeResonanceElement            = document.getElementById(&apos;range-auto-wah-resonance&apos;);

const spanPrintCheckedElement              = document.getElementById(&apos;print-checked-auto-wah&apos;);
const spanPrintSensitivityFrequencyElement = document.getElementById(&apos;print-auto-wah-sensitivity-value&apos;);
const spanPrintDepthElement                = document.getElementById(&apos;print-auto-wah-depth-value&apos;);
const spanPrintResonanceElement            = document.getElementById(&apos;print-auto-wah-resonance-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);
  sensitivity.disconnect(0);
  envelopeFollower.disconnect(0);
  lowpass.disconnect(0);
  depth.disconnect(0);

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Tremolo) -&gt; BiquadFilterNode (Sensitivity) -&gt; GainNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(sensitivity);
    sensitivity.connect(context.destination);

    // OscillatorNode (Input) -&gt; WaveShaperNode (Envelope Follower) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
    oscillator.connect(envelopeFollower);
    envelopeFollower.connect(lowpass);
    lowpass.connect(depth);
    depth.connect(sensitivity.frequency);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; GainNode (Tremolo) -&gt; BiquadFilterNode (Sensitivity) -&gt; GainNode (Output)
    oscillator.connect(amplitude);
    amplitude.connect(sensitivity);
    sensitivity.connect(context.destination);

    // OscillatorNode (Input) -&gt; WaveShaperNode (Envelope Follower) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Depth) -&gt; frequency (AudioParam)
    oscillator.connect(envelopeFollower);
    envelopeFollower.connect(lowpass);
    lowpass.connect(depth);
    depth.connect(sensitivity.frequency);

    // Start oscillator
    oscillator.start(0);
  } else {
    oscillator.disconnect(0);

    // Connect nodes (Auto Wah OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  }

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: 440 });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeSensitivityFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  sensitivityFrequency = event.currentTarget.valueAsNumber;

  sensitivity.frequency.value = sensitivityFrequency;

  spanPrintSensitivityFrequencyElement.textContent = `${sensitivityFrequency.toString(10)} Hz`;
});

rangeSensitivityDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  sensitivityDepthRate = event.currentTarget.valueAsNumber;

  depth.gain.value = sensitivityFrequency * sensitivityDepthRate;

  spanPrintDepthElement.textContent = sensitivityDepthRate.toString(10);
});

rangeResonanceElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  sensitivityResonance = event.currentTarget.valueAsNumber;

  sensitivity.Q.value = sensitivityResonance;

  spanPrintResonanceElement.textContent = sensitivityResonance.toString(10);
});</code></pre>
            <div class="app-container app-auto-wah">
              <div class="app-headline">
                <button type="button" id="button-auto-wah">start</button>
                <label>
                  <input type="checkbox" id="checkbox-auto-wah" checked />
                  <span id="print-checked-auto-wah">ON</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-auto-wah-sensitivity">Sensitivity</label></dt>
                    <dd>
                      <input type="range" id="range-auto-wah-sensitivity" value="1000" min="1000" max="2000" step="1" />
                      <span id="print-auto-wah-sensitivity-value">1000 Hz</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-auto-wah-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-auto-wah-depth" value="0.5" min="0" max="1" step="0.05" />
                      <span id="print-auto-wah-depth-value">0.5</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-auto-wah-resonance">Resonance</label></dt>
                    <dd>
                      <input type="range" id="range-auto-wah-resonance" value="10" min="1" max="40" step="1" />
                      <span id="print-auto-wah-resonance-value">10</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
        </section>
        <section id="section-effectors-distortion">
          <h3>ディストーション (歪み系エフェクター)</h3>
          <p>
            楽器に限らず, オーディオ機器において<b>歪み</b>というのは本来避けるべき現象であり, そのため, 音割れが発生しないように, 振幅の調整 (<a
              href="#section-oscillator-node-synthesize"
              >基本波形の合成</a>セクションのデモなどを参照してください) をしたり, コンプレッサーなどを使って歪みが発生しないようにします. 現在では,
            歪みを積極的に使うエレキギターでも, 元々のアンプはあくまで振幅 (ゲイン) を増幅させる機器であり, クリーンサウンドを使っていたと言われています.
            しかし, あるギタリストが真空管アンプのボリュームを最大にして演奏した歪みが偶然にもよかったことから, 歪み系の音である (広義の)
            <b>ディストーションサウンド</b> (広義というのは, 歪み系のサウンドの総称というニュアンスです) が使われるようになったと言われています
            (このときの歪みは, (狭義の) ディストーションというよりは, ウォームでナチュラルな歪みである<b>オーバードライブ</b> (<b>Overdrive</b>)
            だったそうです).
          </p>
          <p>
            ここから, よりハイゲインなディストーションサウンドを生成できるような真空管アンプが開発されたり,
            ローランド社の伝説的なオーバードライブコンパクトエフェクターとなった BOSS
            <a href="https://www.boss.info/jp/promos/40th_anniversary_compact_pedals/" target="_blank" rel="noopener noreferrer">OD-1 Overdrive</a>
            が開発されたりと (現在でも中古市場で, コンパクトエフェクターにしてはかなりの高額で取引されるほどです), 現在では, 技術の進歩にともなって, アンプ
            (シミュレーター) でも, エフェクターでも, 様々な種類のディストーションサウンドを得ることができます.
          </p>
          <p>
            もっとも, アンプによるディストーションサウンドの生成と, エフェクターによるディストーションサウンドの生成は仕組みとしてはやや異なります. また,
            ギタリストの多くはエフェクターによるディストーションサウンドより, (真空管) アンプのディストーションサウンドを基本に音づくりをする傾向にあるようです.
            歴史的な経緯としても, 先にアンプシミュレーターによるディストーションサウンドの実装を解説したいところですが, Web Audio API においては,
            どちらのディストーションサウンドにおいても, <b><code>WaveShaperNode</code></b> クラスを使うことが基本になるのと, 実装としては,
            エフェクターによるディストーションサウンドのほうが簡潔で理解しやすいと思うので, まずは, <b>Crunch</b> (<b>クランチ</b>),
            <b>Overdrive</b> (<b>オーバードライブ</b>), <b>Distortion</b> (<b>ディストーション</b>), <b>Fuzz</b> (<b>ファズ</b>)
            に分類されるエフェクターによるディストーションサウンドの実装に関して解説します.
          </p>
          <section id="section-effectors-distortion-wave-shaper-node">
            <h4>WaveShaperNode</h4>
            <p>
              ディストーションサウンドの原理は, 周波数領域でみると, 原音には存在しない周波数成分を発生させる<b>非線形処理</b>が原理となっています. これを,
              時間領域の波形でみると, 意図的な音割れを発生させる<b>クリッピング</b>が原理となっています.
            </p>
            <figure>
              <svg id="svg-figure-clipping" width="1200" height="320" />
              <figcaption>クリッピング</figcaption>
            </figure>
            <p>
              波形を単純にクリッピングしただけでは, ディストーションサウンドではなく, 音割れっぽくなってしまうことや, 非線形処理によって折り返し歪み
              (エイリアス歪み) が発生するのを防ぐための<b>オーバーサンプル処理</b>などを抽象化するのが, <b><code>WaveShaperNode</code></b> クラスです.
            </p>
            <section id="section-effectors-distortion-wave-shaper-node-curve">
              <h5>curve プロパティ</h5>
              <p>
                波形のクリッピングを決定するための <code>Float32Array</code> を指定します. <code>curve</code> プロパティで設定される
                <code>Float32Array</code> は, <b>入力の振幅に対する出力の振幅を表しています</b>. つまり, <code>new Float32Array([0, 1])</code> であれば,
                入出力の振幅比は 1 : 1 となり, クリッピングカーブの形状は直線となります. すなわち, これは, <b>線形処理</b>となるので,
                <code>curve</code> プロパティを指定しないのと同じです (<code>curve</code> プロパティのデフォルト値は <code>undefined</code> です).
                <code>curve</code> プロパティに設定する <code>Float32Array</code> のサイズの上限は仕様では決められていませんが,
                <b>下限は <code>2</code></b> です (これは最低でも直線を表現しなければならないことを考慮すると直感的でもあります).
                <b>サンプルとサンプルの間は線形補間が適用される</b>ので, 計算コストも考慮すると現実的な上限は
                <code>8192</code> サンプルぐらいまでで十分と言えます (あまりに少ないと, 線形補間の間が大きくなるので,
                カーブではなく直線をつないだような形状となってしまい, クリッピングの精度が悪くなるので, ディストーションカーブとして使う場合は,
                <code>512</code> サンプル程度は下限としておいたほうがよいでしょう).
              </p>
              <p>
                もっとも, 手動的にクリッピングカーブとなる <code>Float32Array</code> の値を設定することはあまりなく (エンベロープフォロワーなどを除いて),
                ディストーションサウンドとしてのクリッピングカーブを指定する場合, 定石となるようなクリッピングカーブが知られているので,
                それを利用してクリッピングカーブを生成するのがよいでしょう (例えば, シグモイド関数 (<span class="math-inline"
                  >$\sigma_{a}\left(x\right) = \frac{1}{1 + e^{-ax}}$</span>) や<a href="https://stackoverflow.com/questions/7840347/web-audio-api-waveshapernode" target="_blank" rel="noopener noreferrer"
                  >オーバードライブカーブ</a>など).
              </p>
              <p>
                <code>curve</code> プロパティに指定されたクリッピングカーブによって, <b>非線形処理</b>が適用されて, 入出力の振幅比が<b>歪み</b>,
                倍音成分が発生してディストーションサウンドとなります.
              </p>
              <figure>
                <div class="flexbox">
                  <svg id="svg-figure-wave-shaper-node-curve" width="424" height="424" />
                  <dl class="flexbox-column">
                    <dt><label for="svg-figure-wave-shaper-node-curve-select-curve-type">Curve Type</label></dt>
                    <dd>
                      <select id="svg-figure-wave-shaper-node-curve-select-curve-type">
                        <option value="" selected>Linear</option>
                        <option value="sigmoid">Sigmoid Function</option>
                        <option value="overdrive">Overdrive</option>
                        <option value="asymmetrical-overdrive">Overdrive (Asymmetrical)</option>
                        <option value="full-wave-rectifier">Full Wave Rectifier (Envelope Follower)</option>
                        <option value="half-wave-rectifier">Half Wave Rectifier</option>
                        <option value="full-wave-rectifier-and-hard-clipping">Full Wave Rectifier + Hard Clipping</option>
                        <option value="half-wave-rectifier-and-hard-clipping">Half Wave Rectifier + Hard Clipping</option>
                        <option value="bitcrusher">Bit Crusher</option>
                      </select>
                    </dd>
                    <dt>
                      <label for="svg-figure-wave-shaper-node-curve-range-curve-size">Curve Size: </label><span id="svg-figure-wave-shaper-node-curve-range-curve-size-value">4096</span>
                    </dt>
                    <dd>
                      <input type="range" id="svg-figure-wave-shaper-node-curve-range-curve-size" value="4096" min="32" max="8192" step="1" />
                    </dd>
                    <dt>
                      <label for="svg-figure-wave-shaper-node-curve-range-amount">Amount or Clipping Level: </label><span id="svg-figure-wave-shaper-node-curve-range-amount-value">0</span>
                    </dt>
                    <dd>
                      <input type="range" id="svg-figure-wave-shaper-node-curve-range-amount" value="0" min="0" max="0.95" step="0.05" />
                    </dd>
                    <dt>
                      <label for="svg-figure-wave-shaper-node-curve-range-step">Bit Crusher Step: </label><span id="svg-figure-wave-shaper-node-curve-range-step-value">2</span>
                    </dt>
                    <dd>
                      <input type="range" id="svg-figure-wave-shaper-node-curve-range-step" value="2" min="2" max="40" step="2" />
                    </dd>
                  </dl>
                </div>
                <figcaption>クリッピングカーブ</figcaption>
              </figure>
            </section>
            <section id="section-effectors-distortion-wave-shaper-node-oversample">
              <h5>oversample プロパティ</h5>
              <p>
                ディストーションサウンドの生成において必須ではありませんが, クリッピング処理によって原音には存在しない周波数成分が発生しますが,
                <b>ナイキスト周波数以上の周波数成分が発生する可能性, すなわち, 折り返し歪み (エイリアス歪み) が発生してしまう可能性があります</b>. この歪みは,
                ノイズとなって知覚されてしまうので, それを防ぐために,
                <b
                  >プリプロセス処理 (クリッピング前の処理) として, アップサンプリングによってクリッピング対象の信号のサンプル数を増やし, ポストプロセス処理
                  (クリッピング後の処理) として, ダウンサンプリングによって元のサンプル数に戻すという処理</b>
                (<b>オーバーサンプル処理</b>) が知られています.
              </p>
              <p>
                オーバーサンプル処理の, アップサンプリングのサンプリング周波数を設定するのが, <code>oversample</code> プロパティです. もっとも,
                直接アップサンプリング周波数を設定するのではなく, <code>&apos;2x&apos;</code>, または, <code>&apos;4x&apos;</code> の文字列 (<b
                  ><code>OverSampleType</code></b>
                列挙型. デフォルト値は, オーバーサンプル処理をしない <code>&apos;none&apos;</code> です) を指定することによって, サンプリング周波数の 2 倍,
                または, 4 倍にアップサンプリングします. ディストーションサウンドの音質だけを考えるなら, 常に
                <code>&apos;4x&apos;</code> を指定しておくのがよいですが, オーバーサンプル処理によって,
                サンプル数が増えるほどパフォーマンスは低下してしまいます. したがって, Crunch のような軽い歪みやエンベロープフォロワーのような場合には
                <code>&apos;none&apos;</code>, Distortion や Fuzz のような激しい歪みの場合には <code>&apos;4x&apos;</code>, Overdrive の場合には
                <code>&apos;2x&apos;</code> というように, 歪みの深さや用途に応じて設定を変えるほうが, 音質とパフォーマンス (CPU リソースやレイテンシー)
                の観点からは理想と言えます.
              </p>
            </section>
            <p>
              <code>curve</code> プロパティと <code>oversample</code> プロパティが理解できれば, <code>WaveShaperNode</code> インスタンス生成して接続するだけです
              (ファクトリメソッドでインスタンスを生成する場合は, <code>AudioContext</code> インスタンスの
              <b><code>createWaveShaper</code></b> メソッドを呼び出します).
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-wave-shaper-node" width="400" height="520" />
              <figcaption><code>WaveShaperNode</code> のノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const curve = new Float32Array(4096);

const amount = 0.7;

const k = (2 * amount) / (1 - amount);

for (let n = 0, numberOfSamples = curve.length; n &lt; numberOfSamples; n++) {
  const x = (((n - 0) * (1 - (-1))) / (numberOfSamples - 0)) + (-1);
  const y = ((1 + k) * x) / (1 + (k * Math.abs(x)));

  curve[n] = y;
}

const shaper = new WaveShaperNode(context, { curve, oversample: &apos;2x&apos; });

// If use `createWaveShaper`
// const shaper = context.createWaveShaper();
//
// shaper.curve      = curve;
// shaper.oversample = &apos;2x&apos;;

const oscillator = new OscillatorNode(context);

// Connect nodes
// OscillatorNode (Input) -&gt; WaveShaperNode (Clipping) -&gt; AudioDestinationNode (Output)
oscillator.connect(shaper);
shaper.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 10 sec
oscillator.stop(context.currentTime + 10);</code></pre>
            <img src="images/wave-shaper-node.png" alt="WaveShaperNode" width="1232" height="770" loading="lazy" />
          </section>
          <section id="section-effectors-distortion-wave-asymmetrical-clipping">
            <h4>非対称クリッピング</h4>
            <p>
              クリッピングカーブでも紹介した<a
                href="https://stackoverflow.com/questions/7840347/web-audio-api-waveshapernode"
                target="_blank"
                rel="noopener noreferrer"
                >オーバードライブカーブ</a>でも Overdrive を実装することは可能ですが, Overdrive によっては,
              <b>クリッピング後の波形が非対称となる非対称クリッピング</b>で実装されていることも多いのでこのセクションで解説します (ちなみに, BOSS
              <a href="https://www.boss.info/jp/promos/40th_anniversary_compact_pedals/" target="_blank" rel="noopener noreferrer">OD-1 Overdrive</a>
              も非対称クリッピング回路が利用されています).
            </p>
            <p>
              <b>非対称クリッピングすることによって, 奇数次の倍音成分だけではなく, 偶数次の倍音成分も発生するので</b> (端的には, 倍音成分が増えるので),
              ハイゲインよりの Overdrive を生成することが可能となります.
            </p>
            <figure>
              <svg id="svg-figure-asymmetrical-clipping" width="1200" height="320" />
              <figcaption>非対称クリッピング</figcaption>
            </figure>
            <p>
              したがって,
              <a href="https://stackoverflow.com/questions/7840347/web-audio-api-waveshapernode" target="_blank" rel="noopener noreferrer"
                >オーバードライブカーブ</a>をもとに, 非対称のクリッピングになるように, 正負の判定を追加して非対称になるように設定します. 非対称クリッピングカーブの形状は,
              <a href="#svg-figure-wave-shaper-node-curve">クリッピングカーブ</a>で <code>Curve Type</code> を
              <code>Overdrive (Asymmetrical)</code> に選択すると確認できます.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const curve = new Float32Array(4096);

const amount = 0.7;

const k = (2 * amount) / (1 - amount);

for (let n = 0, numberOfSamples = curve.length; n &lt; numberOfSamples; n++) {
  const x = (((n - 0) * (1 - (-1))) / (numberOfSamples - 0)) + (-1);
  const y = ((1 + k) * x) / (1 + (k * Math.abs(x)));

  // Asymmetrical clipping curve
  curve[n] = (y &gt; 0) ? y : ((1 - ((amount &gt; 0.5) ? 0.5 : amount)) * y);
}

const shaper = new WaveShaperNode(context, { curve, oversample: &apos;2x&apos; });

// If use `createWaveShaper`
// const shaper = context.createWaveShaper();
//
// shaper.curve      = curve;
// shaper.oversample = &apos;2x&apos;;

const oscillator = new OscillatorNode(context);

// Connect nodes
// OscillatorNode (Input) -&gt; WaveShaperNode (Clipping) -&gt; AudioDestinationNode (Output)
oscillator.connect(shaper);
shaper.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 10 sec
oscillator.stop(context.currentTime + 10);</code></pre>
            <p>
              また, 対称性に関わらず, Overdrive のクリッピングカーブは滑らかな曲線を設定することが多く, <b>ソフトクリッピング</b>された波形となります (逆に,
              Distortion や Fuzz などは, 急峻に変化する曲線や非線形に直線を組み合わせた設定にすることが多く, <b>ハードクリッピング</b>された波形となります).
            </p>
            <figure>
              <svg id="svg-figure-soft-and-hard-clipping" width="1200" height="320" />
              <figcaption>ソフトクリッピング (左) とハードクリッピング (右)</figcaption>
            </figure>
          </section>
          <section id="section-effectors-distortion-rectification">
            <h4>全波整流・半波整流</h4>
            <p>
              最も激しい歪みに分類される Fuzz では, クリッピングだけでなく, <b>絶対値をとる全波整流</b>, あるいは,
              <b>負数となる振幅を <code>0</code> にする半波整流</b>が実装されていることが多いです (ちなみに,
              「<b>整流器</b>」を英語に訳すと「<b>Rectifier</b>」ですが, ハイゲインアンプで有名な Mesa/Boogie 社の
              <a href="https://www.mesaboogie.com/ja-JP/p/Amp/90s-Dual-Rectifier/2-DR2B-3-A" target="_blank" rel="noopener noreferrer">Dual Rectifier</a>
              などはこれに由来します. ただし, 整流を適用するのは音ではなく, 電源となる電流で, 交流を直流に変換する整流回路として使われます. また,
              その整流回路が, 真空管とダイオードで切り替え可能なことから「Dual」と名称がついています).
            </p>
            <figure>
              <svg id="svg-figure-rectification" width="1200" height="320" />
              <figcaption>整流 (左が全波整流, 右が半波整流)</figcaption>
            </figure>
            <p>
              整流のクリッピングカーブ (<code>curve</code> プロパティの値) は直線の組み合わせなので, 手動的に設定できます. 全波整流であれば
              <code>new Float32Array([1, 0, 1])</code>, 半波整流であれば <code>new Float32Array([0, 0, 1])</code> を設定します (全波整流は,
              オートワウのセクションで解説した, <a href="#section-effector-auto-wah">エンベロープフォロワー</a>でもあります). クリッピングカーブの形状は,
              <a href="#svg-figure-wave-shaper-node-curve">クリッピングカーブ</a>で <code>Curve Type</code> を <code>Full Wave Rectifier</code>, または,
              <code>Half Wave Rectifier</code> に選択すると確認できます.
            </p>
            <p>
              Fuzz では, 整流とハードクリッピングを組み合わせるので, 整流のクリッピングカーブの <code>Float32Array</code> の
              <code>1</code> となっている値をハードクリッピングする値に設定します.
            </p>
            <figure>
              <svg id="svg-figure-fuzz" width="1200" height="320" />
              <figcaption>Fuzz (整流とハードクリッピング)</figcaption>
            </figure>
            <p>
              例えば, <code>0.5</code> でハードクリッピングする場合, 全波整流であれば <code>new Float32Array([0.5, 0, 0.5])</code>, 半波整流であれば
              <code>new Float32Array([0, 0, 0.5])</code>) に設定すれば, 整流とハードクリッピングを組み合わせたクリッピングカーブとなります.
              クリッピングカーブの形状は, <a href="#svg-figure-wave-shaper-node-curve">クリッピングカーブ</a>で <code>Curve Type</code> を
              <code>Full Wave Rectifier + Hard Clipping</code>, または, <code>Half Wave Rectifier + Hard Clipping</code> に選択して,
              <code>Clipping Level</code> の値を変更することで確認できます.
            </p>
            <p>
              しかし, 実際に波形に適用すると, ゲインが <code>1</code> のままでは整流は適用されても, クリッピングされないので
              (振幅がクリッピングの値まで減衰するだけなので), <code>GainNode</code> で振幅を増幅しておきます (以下のコードでは, 2 倍に増幅していますが,
              この増幅率は <code>1</code> より大きいことが重要なので, 必ずしも 2 倍である必要はありません).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const level = 0.5;

// Full Wave Rectifier
const curve = new Float32Array([level, 0, level]);

// If use Half Wave Rectifier
// const curve = new Float32Array([0, 0, level]);

const shaper = new WaveShaperNode(context, { curve, oversample: &apos;4x&apos; });

// If use `createWaveShaper`
// const shaper = context.createWaveShaper();
//
// shaper.curve      = curve;
// shaper.oversample = &apos;4x&apos;;

// for clipping
const gain = new GainNode(context, { gain: 2 });

const oscillator = new OscillatorNode(context);

// Connect nodes
// OscillatorNode (Input) -&gt; GainNode (Amplification) -&gt; WaveShaperNode (Clipping) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(shaper);
shaper.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 10 sec
oscillator.stop(context.currentTime + 10);</code></pre>
          </section>
          <p>
            歪み系エフェクターによるディストーションサウンドのインタラクティブなコード例は, アンプミュレーターと合わせたいので,
            <a href="#section-effectors-amp-simulator-and-effectors-distortion">アンプシミュレーターと歪み系エフェクターによるディストーションサウンド</a>のセクションで記載しています.
          </p>
        </section>
        <section id="section-effectors-amp-simulator">
          <h3>アンプシミュレーター</h3>
          <p>
            ギターアンプにおけるアンプとは, <b>プリアンプ</b>, <b>パワーアンプ</b>, <b>キャビネットとスピーカー</b> (コンボアンプやスモールアンプでは,
            キャビネットはなくて, スピーカーのみであることが多いですが, アンプ本来の音をできるだけ忠実に再現することを目的に解説するので,
            キャビネット内にスピーカーが内蔵されているようなスタックアンプをイメージしていただくのがよいかもしれません) で構成されています.
            パワーアンプは振幅を増幅させるのが主な役割なので,
            <b>アンプ独特のディストーションサウンドは, プリアンプとキャビネットとスピーカーによってつくられます</b>
            (アンプシミュレーターやマルチエフェクターによっては, プリアンプの設定として, キャビネットとスピーカー構成の設定も含まれています).
          </p>
          <p>
            特に, <b>プリアンプの実装が, アンプによるディストーションサウンドにおいて重要となります</b>. アンプシミュレーターを実装するとなると,
            モデリングしたいアンプの数だけ実装が存在することになりますが, エフェクターによるディストーションサウンドと比較して,
            ほとんどのプリアンプで共通して異なる点は, <b>クリッピングの前後にフィルタ処理</b>があること, そして,
            <b>プリアンプの個性となる多段のクリッピングとゲインです</b>.
          </p>
          <p>
            もっとも, モデリングするアンプの数だけ実装を解説することはできないので, このセクションでは,
            <a href="https://lazyecology.web.fc2.com/reverb/special/guitar_tone_process/web_guitar_amp_note.html" target="_blank" rel="noopener noreferrer"
              >WEB AMP like Marshall</a>
            を参考に, ほとんどのスタジオで設置されている Marshall (ライクな) アンプシミュレーターの実装を解説します (さらに, Marshall といっても,
            年代ごとに特色があり,
            <a href="https://www.marshallamps.jp/live_for_music/tips/history/50-years-in-the-making.html" target="_blank" rel="noopener noreferrer"
              >古くは, JTM45, JMP (主に, 1960 年代後半から 70 年代のモデル. 特に, 有名なのが Marshall 1959 Super Lead Plexi), JCM (800, 900, 2000), 最近だと DSL
              や JVM など様々です</a>
            (<a href="https://hookup.co.jp/blog/23844" target="_blank" rel="noopener noreferrer">Marshall 1959 Super Lead Plexi</a> の詳細).
            あくまでこのセクションでの実装は Marshall ライクな実装と捉えてください).
          </p>
          <section id="section-effectors-amp-simulator-preamplifier">
            <h4>プリアンプ</h4>
            <p>
              Marshall ライクなプリアンプの特徴として, <b>2 つのクリッピング処理による歪み</b>と, クリッピングの<b
                >プリプロセスとなる High-Pass Filter と Low-Pass Filter によるフィルタ処理</b>, また, 2 つのクリッピングの間にもフィルタ処理があること, そして, クリッピングのポストプロセスとして<b>ポストイコライザー</b>があることです.
              さらに, 中低域のゲインコントロールと, 高音域のゲインコントロールがあります.
            </p>
            <p>まずは, プリプロセスとなるフィルタ処理を実装します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:  80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const middleAndBassGain = new GainNode(context, { gain: 0.5 });
const highTrebleGain    = new GainNode(context, { gain: 0.5 });

// (AudioSourceNode -&gt;) GainNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Middle and Low Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass1.connect(preLowpass);
preLowpass.connect(middleAndBassGain);
middleAndBassGain.connect(preHighpass3);

// (AudioSourceNode -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; GainNode (High Treble Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass2.connect(highTrebleGain);
highTrebleGain.connect(preHighpass3);

// ...</code></pre>
            <p>次に, 前段の歪みを生成するクリッピング処理を実装します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">function createCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {<!--
    curves[index + i] = peak * ( 1 - c ** (-i) + i * (c ** (-index)) / index );
    curves[index - i] = peak * (-1 + c ** (-i) - i * (c ** (-index)) / index );
-->
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

// If `drive` is about `3`, distortion sound is Crunch.
// If `drive` is about `5`, distortion sound is Overdrive.
// If `drive` is about `8`, distortion sound is Distortion.
const drive   = 5;
const samples = 127;

const curve      = createCurve(drive, samples);
const oversample = &apos;4x&apos;;

const preShaper = new WaveShaperNode(context, { curve, oversample });

// (... -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Pre Clipping)
preHighpass3.connect(preShaper);

// ...</code></pre>
            <p>そして, 前段の歪み後段の歪みの間のフィルタ処理を実装します.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;,  frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:   40, Q: -3 });

// (... -&gt;) WaveShaperNode (Pre Clipping) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; BiquadFilterNode (High-Pass Filter)
preShaper.connect(lowpass);
lowpass.connect(highpass);

// ...</code></pre>
            <p>後段の歪みを生成するクリッピング処理を実装します (このクリッピングカーブは前段と同じです).</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const postShaper = new WaveShaperNode(context, { curve, oversample });

// (... -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Post Clipping)
highpass.connect(postShaper);

// ...</code></pre>
            <p>最後は, ポストイコライザーの実装ですが, これはすでに解説している 3 バンドイコライザーを追加するだけです.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;,  frequency:  240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;,   frequency:  500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

// (... -&gt;) WaveShaperNode (Post Clipping) -&gt; Post Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; AudioDestinationNode (Output)
postShaper.connect(bass);
bass.connect(middle);
middle.connect(treble);
treble.connect(context.destination);</code></pre>
            <p>ここまでのコードを網羅して記載すると, 以下のようになります.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">function createCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

const context = new AudioContext();

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:  80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const middleAndBassGain = new GainNode(context, { gain: 0.5 });
const highTrebleGain    = new GainNode(context, { gain: 0.5 });

// If `drive` is about `3`, distortion sound is Crunch.
// If `drive` is about `5`, distortion sound is Overdrive.
// If `drive` is about `8`, distortion sound is Distortion.
const drive   = 5;
const samples = 127;

const curve      = createCurve(drive, samples);
const oversample = &apos;4x&apos;;

const preShaper  = new WaveShaperNode(context, { curve, oversample });
const postShaper = new WaveShaperNode(context, { curve, oversample });

const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;,  frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:   40, Q: -3 });

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;,  frequency:  240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;,   frequency:  500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

// (AudioSourceNode -&gt;) GainNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Middle and Low Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass1.connect(preLowpass);
preLowpass.connect(middleAndBassGain);
middleAndBassGain.connect(preHighpass3);

// (AudioSourceNode -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; GainNode (High Treble Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass2.connect(highTrebleGain);
highTrebleGain.connect(preHighpass3);

// BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Pre Clipping)
preHighpass3.connect(preShaper);

// WaveShaperNode (Pre Clipping) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; BiquadFilterNode (High-Pass Filter)
preShaper.connect(lowpass);
lowpass.connect(highpass);

// BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Post Clipping)
highpass.connect(postShaper);

// WaveShaperNode (Post Clipping) -&gt; Post Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; AudioDestinationNode (Output)
postShaper.connect(bass);
bass.connect(middle);
middle.connect(treble);
treble.connect(context.destination);</code></pre>
            <p>
              ここまでで, Marshall ライクなプリアンプが実装できたので, エレキギターのクリーントーンのワンショットオーディオを利用して,
              プリアンプで歪ませたディストーションサウンドを聴いてみてください (プリアンプによるディストーションサウンドは,
              多段のフィルタやクリッピングによって, どうしても音が大きくなってしまうので, 最終的な音量を調整できるように, Master Volume となる
              <code>GainNode</code> を出力前に接続しています).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">function createCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

const context = new AudioContext();

const mastervolume = new GainNode(context, { gain: 0.3 });

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:  80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const middleAndBassGain = new GainNode(context, { gain: 0.5 });
const highTrebleGain    = new GainNode(context, { gain: 0.5 });

// If `drive` is about `3`, distortion sound is Crunch.
// If `drive` is about `5`, distortion sound is Overdrive.
// If `drive` is about `8`, distortion sound is Distortion.
const drive   = 5;
const samples = 127;

const curve      = createCurve(drive, samples);
const oversample = &apos;4x&apos;;

const preShaper  = new WaveShaperNode(context, { curve, oversample });
const postShaper = new WaveShaperNode(context, { curve, oversample });

const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;,  frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:   40, Q: -3 });

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;,  frequency:  240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;,   frequency:  500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

fetch(&apos;./assets/one-shots/electric-guitar-clean-chord.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

      // AudioBufferSourceNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Middle and Low Gain) -&gt; BiquadFilterNode (High-Pass Filter)
      source.connect(preHighpass1);
      preHighpass1.connect(preLowpass);
      preLowpass.connect(middleAndBassGain);
      middleAndBassGain.connect(preHighpass3);

      // AudioBufferSourceNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; GainNode (High Treble Gain) -&gt; BiquadFilterNode (High-Pass Filter)
      source.connect(preHighpass2);
      preHighpass2.connect(highTrebleGain);
      highTrebleGain.connect(preHighpass3);

      // BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Pre Clipping)
      preHighpass3.connect(preShaper);

      // WaveShaperNode (Pre Clipping) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; BiquadFilterNode (High-Pass Filter)
      preShaper.connect(lowpass);
      lowpass.connect(highpass);

      // BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Post Clipping)
      highpass.connect(postShaper);

      // WaveShaperNode (Post Clipping) -&gt; Post Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; GainNode (Master Volume)
      postShaper.connect(bass);
      bass.connect(middle);
      middle.connect(treble);
      treble.connect(mastervolume);

      // GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
      mastervolume.connect(context.destination);

      source.start(0);
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <figure>
              <svg id="svg-figure-node-connections-for-pre-amplifier" width="800" height="2050" />
              <figcaption>(Marshall ライクな) プリアンプのノード接続図</figcaption>
            </figure>
          </section>
          <section id="section-effectors-amp-simulator-cabinet-and-speaker-simulator">
            <h4>キャビネットとスピーカーシミュレーター</h4>
            <p>
              プリアンプで生成された音は, 実際のアンプ, 特に, スタックアンプのような,
              キャビネットとそれに内蔵されているスピーカーから発生するような音にはなりません. それを再現するために,
              アンプシミュレーターではプリアンプだけでなく, キャビネットのタイプやスピーカーのサイズや数 (例えば, 12 インチを 4 つなど) を選択できる,
              音楽的な表現だと, いわゆる, <b>箱鳴り感</b>を再現できる, <b>スピーカーシミュレーター</b>も含まれています.
            </p>
            <p>
              実際に存在するキャビネットとスピーカーをシミュレートするには, キャビネット内のインパルス応答を測定したオーディオデータと
              <code>ConvolverNode</code> を利用して実装するのがベストです. しかしながら, RIR と異なって, フリーで使える音源が少ないことも考慮して,
              簡易的なスピーカーシミュレーターの実装を解説します (簡易的なスピーカーシミュレーターでも, 箱鳴り感は十分再現できます).
            </p>
            <p>簡易的なスピーカーシミュレーターの実装であれば, Low-Pass Filter と Notch Filter を接続することで実装できます.</p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const speakerSimulatorNotch   = new BiquadFilterNode(context, { type: &apos;notch&apos;,   frequency: 8000, Q: 1 });
const speakerSimulatorLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: 6 });

// BiquadFilterNode (Notch Filter) -&gt; BiquadFilterNode (Low-Pass Filter)
speakerSimulatorNotch.connect(speakerSimulatorLowpass);</code></pre>
            <p>
              スピーカーシミュレーターのノード接続を, プリアンプの次に接続します. (プリアンプとスピーカーシミュレーターの処理の後の音量を調整できるように,
              Master Volume となる <code>GainNode</code> の接続は, スピーカーシミュレーターの後に変更します. つまり,
              スピーカーシミュレーターの接続先となります).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">function createCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

const context = new AudioContext();

const mastervolume = new GainNode(context, { gain: 0.3 });

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:  80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const middleAndBassGain = new GainNode(context, { gain: 0.5 });
const highTrebleGain    = new GainNode(context, { gain: 0.5 });

// If `drive` is about `3`, distortion sound is Crunch.
// If `drive` is about `5`, distortion sound is Overdrive.
// If `drive` is about `8`, distortion sound is Distortion.
const drive   = 5;
const samples = 127;

const curve      = createCurve(drive, samples);
const oversample = &apos;4x&apos;;

const preShaper  = new WaveShaperNode(context, { curve, oversample });
const postShaper = new WaveShaperNode(context, { curve, oversample });

const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;,  frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:   40, Q: -3 });

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;,  frequency:  240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;,   frequency:  500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

const speakerSimulatorNotch   = new BiquadFilterNode(context, { type: &apos;notch&apos;,   frequency: 8000, Q: 1 });
const speakerSimulatorLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: 6 });

fetch(&apos;./assets/one-shots/electric-guitar-clean-chord.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then((arrayBuffer) =&gt; {
    const successCallback = (audioBuffer) =&gt; {
      const source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

      // Preamp
      // AudioBufferSourceNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Middle and Low Gain) -&gt; BiquadFilterNode (High-Pass Filter)
      source.connect(preHighpass1);
      preHighpass1.connect(preLowpass);
      preLowpass.connect(middleAndBassGain);
      middleAndBassGain.connect(preHighpass3);

      // AudioBufferSourceNode (Input) -&gt; BiquadFilterNode (High-Pass Filter) -&gt; GainNode (High Treble Gain) -&gt; BiquadFilterNode (High-Pass Filter)
      source.connect(preHighpass2);
      preHighpass2.connect(highTrebleGain);
      highTrebleGain.connect(preHighpass3);

      // BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Pre Clipping)
      preHighpass3.connect(preShaper);

      // WaveShaperNode (Pre Clipping) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; BiquadFilterNode (High-Pass Filter)
      preShaper.connect(lowpass);
      lowpass.connect(highpass);

      // BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Post Clipping)
      highpass.connect(postShaper);

      // WaveShaperNode (Post Clipping) -&gt; Post Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; BiquadFilterNode (Speaker Simulator)
      postShaper.connect(bass);
      bass.connect(middle);
      middle.connect(treble);
      treble.connect(speakerSimulatorNotch);

      // Speaker Simulator
      // BiquadFilterNode (Notch Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Master Volume);
      speakerSimulatorNotch.connect(speakerSimulatorLowpass);
      speakerSimulatorLowpass.connect(mastervolume);

      // GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
      mastervolume.connect(context.destination);

      source.start(0);
    };

    const errorCallback = (error) =&gt; {
      // error handling
    };

    context.decodeAudioData(arrayBuffer, successCallback, errorCallback);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <figure>
              <svg id="svg-figure-node-connections-for-amp-simulator" width="800" height="2400" />
              <figcaption>プリアンプとスピーカーシミュレーターのノード接続図</figcaption>
            </figure>
            <article id="section-effectors-amp-simulator-cabinet-and-speaker-simulator-by-convolver-node">
              <h5>ConvolverNode によるスピーカーシミュレーターの実装</h5>
              <p>
                シミューレートしたいキャビネットとスピーカーのインパルス応答を測定したオーディオデータがあれば, リバーブを実装するのと同様に,
                <code>ConvolverNode</code> の <code>buffer</code> プロパティに, そのオーディオデータの <code>AudioBuffer</code> インスタンスを設定することで,
                より忠実にキャビネットとスピーカーをシミュレートすることが可能です.
              </p>
              <p>
                キャビネット内を小さな部屋とみなせば, RIR をコンボリューション積分しているのと同じであり, また, キャビネット内をシステム (系) とみなせば,
                キャビネット内をモデリングする伝達関数を乗算しているのと同じことです.
              </p>
              <p>
                スピーカーシミュレーターとしての <code>ConvolverNode</code> インスタンスも, プリアンプの次に接続します (<code>audioBuffer</code> 変数は,
                <code>Fetch API</code> などから取得した <code>ArrayBuffer</code> インスタンスを <code>AudioContext</code> インスタンスの
                <code>decodeAudioData</code> メソッドの引数に指定して生成した (キャビネットとスピーカーのインパルス応答のオーディオデータの実体となる)
                <code>AudioBuffer</code> インスタンスと考えてください).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">function createCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

const context = new AudioContext();

const mastervolume = new GainNode(context, { gain: 0.3 });

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:  80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const middleAndBassGain = new GainNode(context, { gain: 0.5 });
const highTrebleGain    = new GainNode(context, { gain: 0.5 });

// If `drive` is about `3`, distortion sound is Crunch.
// If `drive` is about `5`, distortion sound is Overdrive.
// If `drive` is about `8`, distortion sound is Distortion.
const drive   = 5;
const samples = 127;

const curve      = createCurve(drive, samples);
const oversample = &apos;4x&apos;;

const preShaper  = new WaveShaperNode(context, { curve, oversample });
const postShaper = new WaveShaperNode(context, { curve, oversample });

const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;,  frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency:   40, Q: -3 });

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;,  frequency:  240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;,   frequency:  500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

const speakerSimulator = new ConvolverNode(context);

// Set instance of `AudioBuffer` that is audio data in cabinet to `buffer` property
// speakerSimulator.buffer = audioBuffer;

// (AudioSourceNode -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; GainNode (Middle and Low Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass1.connect(preLowpass);
preLowpass.connect(middleAndBassGain);
middleAndBassGain.connect(preHighpass3);

// (AudioSourceNode -&gt;) BiquadFilterNode (High-Pass Filter) -&gt; GainNode (High Treble Gain) -&gt; BiquadFilterNode (High-Pass Filter)
preHighpass2.connect(highTrebleGain);
highTrebleGain.connect(preHighpass3);

// BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Pre Clipping)
preHighpass3.connect(preShaper);

// WaveShaperNode (Pre Clipping) -&gt; BiquadFilterNode (Low-Pass Filter) -&gt; BiquadFilterNode (High-Pass Filter)
preShaper.connect(lowpass);
lowpass.connect(highpass);

// BiquadFilterNode (High-Pass Filter) -&gt; WaveShaperNode (Post Clipping)
highpass.connect(postShaper);

// WaveShaperNode (Post Clipping) -&gt; Post Equalizer (Low-Shelving Filter -&gt; Peaking Filter -&gt; High-Shelving Filter) -&gt; ConvolverNode (Speaker Simulator)
postShaper.connect(bass);
bass.connect(middle);
middle.connect(treble);
treble.connect(speakerSimulator);

// ConvolverNode (Speaker Simulator) -&gt; GainNode (Master Volume)
speakerSimulator.connect(mastervolume);

// GainNode (Master Volume) -&gt; AudioDestinationNode (Output)
mastervolume.connect(context.destination);</code></pre>
              <p>
                ちなみに, フラグシップモデルマルチエフェクター (<a href="https://line6.jp/helix/" target="_blank" rel="noopener noreferrer">LINE6 HELIX</a>,
                <a href="https://www.boss.info/jp/products/gt-1000/" target="_blank" rel="noopener noreferrer">BOSS GT-1000</a> など, 価格目安で 10
                万円以上のハイエンドマルチエフェクター) には, インパルス応答 (<abbr title="Impulse Response">IR</abbr>) のインポート機能が搭載されおり,
                実機と遜色のないアンプシミュレートを可能にしています.
              </p>
            </article>
          </section>
          <section id="section-effectors-amp-simulator-and-effectors-distortion">
            <h4>アンプシミュレーターと歪み系エフェクターによるディストーションサウンド</h4>
            <p>
              ここまでで, アンプ (アンプシミュレーター) と歪み系エフェクターによるディストーションサウンドを解説したので, それらを組み合わせて, さらに,
              ハードコーディングしていたパラメータをインタラクティブに制御できるようにしたコード例です.
            </p>
            <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-html line-numbers">&lt;div&gt;
  &lt;button type=&quot;button&quot; id=&quot;button-picking-down&quot; data-index=&quot;0&quot;&gt;Picking Down&lt;/button&gt;
  &lt;button type=&quot;button&quot; id=&quot;button-picking-up&quot;   data-index=&quot;1&quot;&gt;Picking Up&lt;/button&gt;
  &lt;button type=&quot;button&quot; id=&quot;button-chord&quot;        data-index=&quot;2&quot;&gt;Chord&lt;/button&gt;
&lt;/div&gt;
&lt;dl&gt;
  &lt;dt&gt;&lt;label for=&quot;select-distortion-type&quot;&gt;Overdrive / Fuzz&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;select id=&quot;select-distortion-type&quot;&gt;
      &lt;option value=&quot;&quot; selected&gt;none&lt;/option&gt;
      &lt;option value=&quot;overdrive&quot;&gt;Overdrive&lt;/option&gt;
      &lt;option value=&quot;fuzz&quot;&gt;Fuzz&lt;/option&gt;
    &lt;/select&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-distortion-drive&quot;&gt;Drive&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-distortion-drive&quot; value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;0.95&quot; step=&quot;0.05&quot; disabled /&gt;
    &lt;span id=&quot;print-distortion-drive-value&quot;&gt;0.5&lt;/span&gt;
  &lt;/dd&gt;
&lt;/dl&gt;
&lt;dl&gt;
  &lt;dt&gt;
    &lt;label for=&quot;checkbox-preamp&quot;&gt;&lt;span&gt;Preamp&lt;/span&gt;&lt;/label&gt;
  &lt;/dt&gt;
  &lt;dd&gt;&lt;input type=&quot;checkbox&quot; id=&quot;checkbox-preamp&quot; checked /&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;dl&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-normal-gain&quot;&gt;Normal Gain (Middle and Low Gain)&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-normal-gain&quot; value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
    &lt;span id=&quot;print-preamp-normal-gain-value&quot;&gt;0.5&lt;/span&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-high-treble-gain&quot;&gt;High Treble Gain&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-high-treble-gain&quot; value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
    &lt;span id=&quot;print-preamp-high-treble-value&quot;&gt;0.5&lt;/span&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-drive&quot;&gt;Drive&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-drive&quot; value=&quot;5&quot; min=&quot;0&quot; max=&quot;10&quot; step=&quot;0.5&quot; /&gt;
    &lt;span id=&quot;print-preamp-drive-value&quot;&gt;5.0&lt;/span&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-post-equalizer-bass&quot;&gt;Bass&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-post-equalizer-bass&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
    &lt;span id=&quot;print-preamp-post-equalizer-bass-value&quot;&gt;0 dB&lt;/span&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-post-equalizer-middle&quot;&gt;Middle&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-post-equalizer-middle&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
    &lt;span id=&quot;print-preamp-post-equalizer-middle-value&quot;&gt;0 dB&lt;/span&gt;
  &lt;/dd&gt;
  &lt;dt&gt;&lt;label for=&quot;range-preamp-post-equalizer-treble&quot;&gt;Treble&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-preamp-post-equalizer-treble&quot; value=&quot;0&quot; min=&quot;-24&quot; max=&quot;24&quot; step=&quot;1&quot; /&gt;
    &lt;span id=&quot;print-preamp-post-equalizer-treble-value&quot;&gt;0 dB&lt;/span&gt;
  &lt;/dd&gt;
&lt;/dl&gt;
&lt;dl&gt;
  &lt;dt&gt;
    &lt;label for=&quot;checkbox-speaker-simulator&quot;&gt;&lt;span&gt;Speaker Simulator&lt;/span&gt;&lt;/label&gt;
  &lt;/dt&gt;
  &lt;dd&gt;&lt;input type=&quot;checkbox&quot; id=&quot;checkbox-speaker-simulator&quot; checked /&gt;&lt;/dd&gt;
&lt;/dl&gt;
&lt;dl&gt;
  &lt;dt&gt;&lt;label for=&quot;range-distortion-mastervolume&quot;&gt;Master Volume&lt;/label&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;input type=&quot;range&quot; id=&quot;range-distortion-mastervolume&quot; value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
    &lt;span id=&quot;print-distortion-mastervolume-value&quot;&gt;0.5&lt;/span&gt;
  &lt;/dd&gt;
&lt;/dl&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">function createPreampCurve(drive, numberOfSamples) {
  const curves = new Float32Array(numberOfSamples);

  const index = Math.trunc((numberOfSamples - 1) / 2);

  const d = (10 ** ((drive / 5.0) - 1.0)) - 0.1;
  const c = (d / 5.0) + 1.0;

  let peak = 0.4;

  if (c === 1) {
    peak = 1.0;
  } else if ((c &gt; 1) &amp;&amp; (c &lt; 1.04)) {
    peak = (-15.5 * c) + 16.52;
  }

  for (let i = 0; i &lt; index; i++) {
    curves[index + i] = peak * (((+1) - (c ** (-i))) + ((i * (c ** (-index))) / index));
    curves[index - i] = peak * (((-1) + (c ** (-i))) - ((i * (c ** (-index))) / index));
  }

  curves[index] = 0;

  return curves;
}

function createAsymmetricalOverdriveCurve(drive, numberOfSamples) {
  if (drive &lt; 0 || drive &gt;= 1) {
    return null;
  }

  const curves = new Float32Array(numberOfSamples);

  const k = (2 * drive) / (1 - drive);

  for (let n = 0; n &lt; numberOfSamples; n++) {
    const x = (((n - 0) * (1 - (-1))) / (numberOfSamples - 0)) + (-1);
    const y = ((1 + k) * x) / (1 + (k * Math.abs(x)));

    // Asymmetrical clipping curve
    curves[n] = (y &gt; 0) ? y : ((1 - ((drive &gt; 0.5) ? 0.5 : drive)) * y);
  }

  return curves;
}

const context = new AudioContext();

const samples    = 127;
const oversample = &apos;4x&apos;;

const shaper = new WaveShaperNode(context);
const gain   = new GainNode(context, { gain: 1 });

const preLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: -3 });

const preHighpass1 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 80, Q: -3 });
const preHighpass2 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 640, Q: -3 });
const preHighpass3 = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 160, Q: -3 });

const highTrebleGain    = new GainNode(context, { gain: 0.5 });
const middleAndBassGain = new GainNode(context, { gain: 0.5 });

const preShaper  = new WaveShaperNode(context, { curve: createPreampCurve(5.0, samples), oversample });
const postShaper = new WaveShaperNode(context, { curve: createPreampCurve(5.0, samples), oversample });

const lowpass  = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 4000, Q: -3 });
const highpass = new BiquadFilterNode(context, { type: &apos;highpass&apos;, frequency: 40, Q: -3 });

const bass   = new BiquadFilterNode(context, { type: &apos;lowshelf&apos;, frequency: 240 });
const middle = new BiquadFilterNode(context, { type: &apos;peaking&apos;, frequency: 500, Q: 1.5 });
const treble = new BiquadFilterNode(context, { type: &apos;highshelf&apos;, frequency: 1600 });

const speakerSimulatorNotch   = new BiquadFilterNode(context, { type: &apos;notch&apos;, frequency: 8000, Q: 1 });
const speakerSimulatorLowpass = new BiquadFilterNode(context, { type: &apos;lowpass&apos;, frequency: 3200, Q: 6 });

const mastervolume = new GainNode(context, { gain: 0.5 });

const buttonElementPickingDown = document.getElementById(&apos;button-picking-down&apos;);
const buttonElementPickingUp   = document.getElementById(&apos;button-picking-up&apos;);
const buttonElementChord       = document.getElementById(&apos;button-chord&apos;);

const checkboxElementPreamp           = document.getElementById(&apos;checkbox-preamp&apos;);
const checkboxElementSpeakerSimulator = document.getElementById(&apos;checkbox-speaker-simulator&apos;);

const rangePreampNormalGainElement     = document.getElementById(&apos;range-preamp-normal-gain&apos;);
const rangePreampHighTrebleGainElement = document.getElementById(&apos;range-preamp-high-treble-gain&apos;);
const rangePreampDriveElement          = document.getElementById(&apos;range-preamp-drive&apos;);
const rangePreampBassElement           = document.getElementById(&apos;range-preamp-post-equalizer-bass&apos;);
const rangePreampMiddleElement         = document.getElementById(&apos;range-preamp-post-equalizer-middle&apos;);
const rangePreampTrebleElement         = document.getElementById(&apos;range-preamp-post-equalizer-treble&apos;);

const selectEffectorDistortionTypeElement = document.getElementById(&apos;select-distortion-type&apos;);
const rangeEffectorDistortionDriveElement = document.getElementById(&apos;range-distortion-drive&apos;);

const rangeDistortionMasterVolumeElement = document.getElementById(&apos;range-distortion-mastervolume&apos;);

const spanPrintPreampNormalGainElement     = document.getElementById(&apos;print-preamp-normal-gain-value&apos;);
const spanPrintPreampHighTrebleGainElement = document.getElementById(&apos;print-preamp-high-treble-value&apos;);
const spanPrintPreampDriveElement          = document.getElementById(&apos;print-preamp-drive-value&apos;);
const spanPrintPreampBassElement           = document.getElementById(&apos;print-preamp-post-equalizer-bass-value&apos;);
const spanPrintPreampMiddleElement         = document.getElementById(&apos;print-preamp-post-equalizer-middle-value&apos;);
const spanPrintPreampTrebleElement         = document.getElementById(&apos;print-preamp-post-equalizer-treble-value&apos;);

const spanPrintEffectorDistortionDriveElement = document.getElementById(&apos;print-distortion-drive-value&apos;);

const spanPrintDistortionMasterVolumeElement = document.getElementById(&apos;print-distortion-mastervolume-value&apos;);

Promise
  .all([
    fetch(&apos;./assets/one-shots/electric-guitar-clean-picking-down.mp3&apos;),
    fetch(&apos;./assets/one-shots/electric-guitar-clean-picking-up.mp3&apos;),
    fetch(&apos;./assets/one-shots/electric-guitar-clean-chord.mp3&apos;)
  ])
  .then(async (responses) =&gt; {
    const audioBuffers = await Promise.all(responses.map(async (response) =&gt; {
      const arrayBuffer = await response.arrayBuffer();
      const audioBuffer = await context.decodeAudioData(arrayBuffer);

      return audioBuffer;
    }));

    const onDown = async (event) =&gt; {
      const index = Number(event.target.getAttribute(&apos;data-index&apos;));

      const buffer = audioBuffers[index];
      const source = new AudioBufferSourceNode(context, { buffer });

      gain.disconnect(0);
      shaper.disconnect(0);
      treble.disconnect(0);
      speakerSimulatorLowpass.disconnect(0);

      source.connect(gain);
      gain.connect(shaper);

      if (checkboxElementPreamp.checked &amp;&amp; checkboxElementSpeakerSimulator.checked) {
        shaper.connect(preHighpass1);
        preHighpass1.connect(preLowpass);
        preLowpass.connect(middleAndBassGain);
        middleAndBassGain.connect(preHighpass3);

        shaper.connect(preHighpass2);
        preHighpass2.connect(highTrebleGain);
        highTrebleGain.connect(preHighpass3);

        preHighpass3.connect(preShaper);

        preShaper.connect(lowpass);
        lowpass.connect(highpass);

        highpass.connect(postShaper);

        postShaper.connect(bass);
        bass.connect(middle);
        middle.connect(treble);
        treble.connect(speakerSimulatorNotch);

        speakerSimulatorNotch.connect(speakerSimulatorLowpass);
        speakerSimulatorLowpass.connect(mastervolume);
      } else {
        if (checkboxElementPreamp.checked) {
          shaper.connect(preHighpass1);
          preHighpass1.connect(preLowpass);
          preLowpass.connect(middleAndBassGain);
          middleAndBassGain.connect(preHighpass3);

          shaper.connect(preHighpass2);
          preHighpass2.connect(highTrebleGain);
          highTrebleGain.connect(preHighpass3);

          preHighpass3.connect(preShaper);

          preShaper.connect(lowpass);
          lowpass.connect(highpass);

          highpass.connect(postShaper);

          postShaper.connect(bass);
          bass.connect(middle);
          middle.connect(treble);
          treble.connect(mastervolume);
        } else if (checkboxElementSpeakerSimulator.checked) {
          shaper.connect(speakerSimulatorNotch);

          speakerSimulatorNotch.connect(speakerSimulatorLowpass);
          speakerSimulatorLowpass.connect(mastervolume);
        } else {
          shaper.connect(mastervolume);
        }
      }

      mastervolume.connect(context.destination);

      source.start(0);
    };

    buttonElementPickingDown.addEventListener(&apos;mousedown&apos;, onDown);
    buttonElementPickingDown.addEventListener(&apos;touchstart&apos;, onDown);
    buttonElementPickingUp.addEventListener(&apos;mousedown&apos;, onDown);
    buttonElementPickingUp.addEventListener(&apos;touchstart&apos;, onDown);
    buttonElementChord.addEventListener(&apos;mousedown&apos;, onDown);
    buttonElementChord.addEventListener(&apos;touchstart&apos;, onDown);

    checkboxElementPreamp.addEventListener(&apos;change&apos;, (event) =&gt; {
      if (event.currentTarget.checked) {
        rangePreampNormalGainElement.removeAttribute(&apos;disabled&apos;);
        rangePreampHighTrebleGainElement.removeAttribute(&apos;disabled&apos;);
        rangePreampDriveElement.removeAttribute(&apos;disabled&apos;);
        rangePreampBassElement.removeAttribute(&apos;disabled&apos;);
        rangePreampMiddleElement.removeAttribute(&apos;disabled&apos;);
        rangePreampTrebleElement.removeAttribute(&apos;disabled&apos;);
      } else {
        rangePreampNormalGainElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
        rangePreampHighTrebleGainElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
        rangePreampDriveElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
        rangePreampBassElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
        rangePreampMiddleElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
        rangePreampTrebleElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
      }
    });

    selectEffectorDistortionTypeElement.addEventListener(&apos;change&apos;, (event) =&gt; {
      const drive = rangeEffectorDistortionDriveElement.valueAsNumber;

      rangeEffectorDistortionDriveElement.removeAttribute(&apos;disabled&apos;);

      switch (event.currentTarget.value) {
        case &apos;overdrive&apos;: {
          shaper.curve      = createAsymmetricalOverdriveCurve(drive, samples);
          shaper.oversample = &apos;2x&apos;;

          gain.gain.value = 1;
          break;
        }

        case &apos;fuzz&apos;: {
          shaper.curve      = new Float32Array([drive, 0, drive]);
          shaper.oversample = &apos;4x&apos;;

          gain.gain.value = 2;
          break;
        }

        default: {
          shaper.curve      = null;
          shaper.oversample = &apos;none&apos;;

          gain.gain.value = 1;

          rangeEffectorDistortionDriveElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);
          break;
        }
      }
    });

    rangeEffectorDistortionDriveElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const drive = event.currentTarget.valueAsNumber;

      switch (selectEffectorDistortionTypeElement.value) {
        case &apos;overdrive&apos;: {
          shaper.curve      = createAsymmetricalOverdriveCurve(drive, samples);
          shaper.oversample = &apos;2x&apos;;
          break;
        }

        case &apos;fuzz&apos;: {
          shaper.curve      = new Float32Array([drive, 0, drive]);
          shaper.oversample = &apos;4x&apos;;
          break;
        }

        default: {
          shaper.curve      = null;
          shaper.oversample = &apos;none&apos;;
          break;
        }
      }

      spanPrintEffectorDistortionDriveElement.textContent = drive.toFixed(1);
    });

    rangePreampNormalGainElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      middleAndBassGain.gain.value = gain;

      spanPrintPreampNormalGainElement.textContent = gain.toString(10);
    });

    rangePreampHighTrebleGainElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      highTrebleGain.gain.value = gain;

      spanPrintPreampHighTrebleGainElement.textContent = gain.toString(10);
    });

    rangePreampDriveElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const drive = event.currentTarget.valueAsNumber;

      preShaper.curve  = createPreampCurve(drive, samples);
      postShaper.curve = createPreampCurve(drive, samples);

      spanPrintPreampDriveElement.textContent = drive.toFixed(1);
    });

    rangePreampBassElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      bass.gain.value = gain;

      spanPrintPreampBassElement.textContent = `${gain} dB`;
    });

    rangePreampMiddleElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      middle.gain.value = gain;

      spanPrintPreampMiddleElement.textContent = `${gain} dB`;
    });

    rangePreampTrebleElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      treble.gain.value = gain;

      spanPrintPreampTrebleElement.textContent = `${gain} dB`;
    });

    rangeDistortionMasterVolumeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
      const gain = event.currentTarget.valueAsNumber;

      mastervolume.gain.value = gain;

      spanPrintDistortionMasterVolumeElement.textContent = gain.toString(10);
    });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            <p>音源はエレキギターのクリーンサウンドのワンショットオーディオを 3 つ用意しました (単音ピッキング 2 つ (音高ダウンとアップ) とコード弾き).</p>
            <p>
              制御可能なパラメータが多いので, インタラクティブに制御できる部分は最小限にしました. クリッピングカーブの <code>Float32Array</code> のサイズ,
              および, 歪み系エフェクターのオーバードライブカーブは非対称クリッピング, 整流は全波整流に固定していますが, アプリケーションの要件次第では,
              これらもインタラクティブに制御可能にすることもあるでしょう. インタラクティブな部分が多く, コードが長くなっていますが, 分解すれば,
              これまで解説してきた, 歪み系エフェクターやアンプシミュレーターの実装です.
            </p>
            <p>
              追加で解説すべきなのは, アンプシミュレーターと歪み系エフェクターの<b>接続順</b>です. 一般的には
              (マルチエフェクターなどエフェクターの接続順が固定されている場合でも),
              <b>アンプシミュレーターの前に歪み系エフェクターを接続します</b> (あとのセクションで解説しますが,
              エフェクターの接続順は出力されるサウンドに大きく影響します). UI も, 接続順がイメージしやすいように左から順に並べています.
            </p>
            <p>
              サンプルコードにおける注意点ですが, 歪み系エフェクター, プリアンプ, スピーカーシミュレーターの有効・無効に応じて, 再生時のノード接続が変わるので,
              ワンショットオーディオを再生するイベントリスナーで, 歪み系エフェクターのためのノード (コードでの変数名は <code>shaper</code>),
              プリアンプの最後の接続ノード (コードでの変数名は <code>treble</code>), スピーカーシミュレーターの最後の接続ノード (コードでの変数名は
              <code>speakerSimulatorLowpass</code>) それぞれで <code>disconnect</code> メソッドを呼び出して, 前回再生時のノード接続をクリアしていることです
              (この処理がないと, 例えば, スピーカーシミュレーターつきで再生後, チェックボックスを外して, スピーカーシミュレーターなしで再生すると,
              スピーカシミュレーターつきのノード接続が残るので,
              スピーカーシミュレーターつきの音とスピーカーシミュレーターなしの音が合成されて再生されてしまいます). サンプルコードではありますが,
              <code>disconnect</code> メソッドの明示的な呼び出しが必要な, 比較的よくあるユースケースです.
            </p>
            <div class="app-container app-amp-simulator-and-effectors-distortion">
              <div class="flexbox">
                <button type="button" id="button-picking-down" data-index="0">Picking Down</button>
                <button type="button" id="button-picking-up" data-index="1">Picking Up</button>
                <button type="button" id="button-chord" data-index="2">Chord</button>
              </div>
              <div class="flexbox">
                <div>
                  <dl>
                    <div>
                      <dt><label for="select-distortion-type">Overdrive / Fuzz</label></dt>
                      <dd>
                        <select id="select-distortion-type">
                          <option value="" selected>none</option>
                          <option value="overdrive">Overdrive</option>
                          <option value="fuzz">Fuzz</option>
                        </select>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-distortion-drive">Drive</label></dt>
                      <dd>
                        <input type="range" id="range-distortion-drive" value="0.5" min="0" max="0.95" step="0.05" disabled />
                        <span id="print-distortion-drive-value">0.5</span>
                      </dd>
                    </div>
                  </dl>
                </div>
                <div>
                  <dl>
                    <div class="checkbox-container">
                      <dt>
                        <label for="checkbox-preamp"><span>Preamp</span></label>
                      </dt>
                      <dd><input type="checkbox" id="checkbox-preamp" checked /></dd>
                    </div>
                  </dl>
                </div>
                <div>
                  <dl>
                    <div>
                      <dt><label for="range-preamp-normal-gain">Normal Gain (Middle and Low Gain)</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-normal-gain" value="0.5" min="0" max="1" step="0.05" />
                        <span id="print-preamp-normal-gain-value">0.5</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-preamp-high-treble-gain">High Treble Gain</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-high-treble-gain" value="0.5" min="0" max="1" step="0.05" />
                        <span id="print-preamp-high-treble-value">0.5</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-preamp-drive">Drive</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-drive" value="5" min="0" max="10" step="0.5" />
                        <span id="print-preamp-drive-value">5.0</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-preamp-post-equalizer-bass">Bass</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-post-equalizer-bass" value="0" min="-24" max="24" step="1" />
                        <span id="print-preamp-post-equalizer-bass-value">0 dB</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-preamp-post-equalizer-middle">Middle</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-post-equalizer-middle" value="0" min="-24" max="24" step="1" />
                        <span id="print-preamp-post-equalizer-middle-value">0 dB</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-preamp-post-equalizer-treble">Treble</label></dt>
                      <dd>
                        <input type="range" id="range-preamp-post-equalizer-treble" value="0" min="-24" max="24" step="1" />
                        <span id="print-preamp-post-equalizer-treble-value">0 dB</span>
                      </dd>
                    </div>
                  </dl>
                </div>
                <div>
                  <dl>
                    <div class="checkbox-container">
                      <dt>
                        <label for="checkbox-speaker-simulator"><span>Speaker Simulator</span></label>
                      </dt>
                      <dd><input type="checkbox" id="checkbox-speaker-simulator" checked /></dd>
                    </div>
                  </dl>
                </div>
                <div>
                  <dl>
                    <div>
                      <dt><label for="range-distortion-mastervolume">Master Volume</label></dt>
                      <dd>
                        <input type="range" id="range-distortion-mastervolume" value="0.5" min="0" max="1" step="0.05" />
                        <span id="print-distortion-mastervolume-value">0.5</span>
                      </dd>
                    </div>
                  </dl>
                </div>
              </div>
            </div>
          </section>
          <article id="section-effectors-amp-simulator-tube-guitar-amplifier-history">
            <h4>真空管ギターアンプの歴史</h4>
            <p>
              モデリングしたいアンプの数だけ解説することは難しいので, Marshall ライクなアンプシミュレーターの実装のみを解説しましたが,
              真空管ギターアンプの歴史を知ることで, モデリングしたいアンプの実装のヒントになるかもしれません.
            </p>
            <p>
              <a href="https://lazyecology.web.fc2.com/reverb/special/preamp_design/index.html" target="_blank" rel="noopener noreferrer"
                >真空管ギタープリアンプの回路設計</a>で詳細は記載されていますが, 真空管ギターアンプのルーツは大きく分類すると, <b>Fender</b> (<b>フェンダー</b>)
              をルーツとする真空管ギターアンプとそれ以外に分離できます. Marshall (マーシャル), Mesa/Boogie (メサ・ブギー), Soldano (ソルダーノ), Bogner
              (ボグナー) など, ハイゲインな真空管ギターアンプのルーツは, Fender にあるので, Fender のギターアンプ設計を理解することで,
              真空管ギターアンプの定石を知って, モデリングしたいアンプに近い実装が可能になるかもしれません (参考として, Marshall
              ライクな実装のみを解説しましたが, 同じ Web ページに, Fender (ライク) な実装も解説されています).
            </p>
            <p>
              Fender をルーツとしないギターアンプとしては, 有名なのは VOX (ヴォックス), HIWATT (ハイワット), Orange (オレンジ), ENGL (エングル) などがあります.
            </p>
            <p>
              また, 公開されているプロダクトの実装を参考にするのも 1 つのヒントになるでしょう (例:
              <a href="https://mainline.i3s.unice.fr/AmpSim3/" target="_blank" rel="noopener noreferrer">Guitar Amp Simulation using WebAudio</a>).
            </p>
          </article>
        </section>
        <section id="section-effectors-compressor">
          <h3>コンプレッサー</h3>
          <p>
            コンプレッサーは, 振幅の大きな音を (相対的に) 小さく, 振幅の小さな音を (相対的に) 大きくすることによって, 振幅値のオーバーフロー (例として,
            量子化ビット <code>8 bit</code> で, 2 の補数表現の場合, <code>127</code> (<code>0b0111111</code>) が正の値の上限で, <code>1</code> 増えると,
            <code>-128</code> (<code>0b1000000</code>) で負数となって波形が大きくゆがんでしまいます) や (振幅値のオーバーフローを防ぐための)
            クリッピング処理による意図しない歪みを防止しつつ, 迫力のあるサウンドに変化させるエフェクターです. 音楽的な表現だと,
            <b>音の粒をそろえる</b>エフェクターと言えます.
          </p>
          <p>
            音楽ジャンルにもよりますが, ロックやポップスのレコーディングでは一般的に利用されるエフェクターです. ロックやポップスでは, ギター数パート, ベース,
            ドラム, そして, ボーカルと定石の構成でも楽音数が多く, ミキシングをある程度オートマティックにおこなう必要があるからです (逆に, クラシックなどでは
            (例えば, ピアノ演奏のみの場合), 演奏時の強弱 (フォルテやピアノなど) も忠実に再現することが理想とされます).
          </p>
          <section id="section-effectors-compressor-dynamics-compressor-node">
            <h4>DynamicsCompressorNode</h4>
            <p>
              Web Audio API でコンプレッサーを実装するのはおそらくもっとも簡単です.
              <b><code>DynamicsCompressorNode</code></b> インスタンスを生成して接続するだけです. これだけで, それなりのコンプレッサーとして機能します (<code
                >DynamicsCompressorNode</code>
              インスタンスの各 <code>AudioParam</code> のデフォルト値が機能するような値になっていますが, コンストラクタの第 2 引数の
              <b><code>DynamicsCompressorOptions</code></b> でインスタンス生成時に値を設定したり, <code>AudioParam</code> のセッターで値を設定したりはできます).
            </p>
            <figure>
              <svg id="svg-figure-node-connections-for-dynamics-compressor-node" width="400" height="520" />
              <figcaption><code>DynamicsCompressorNode</code> のノード接続図</figcaption>
            </figure>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

const compressor = new DynamicsCompressorNode(context);

// If use `createDynamicsCompressor`
// const compressor = context.createDynamicsCompressor();

// OscillatorNode (Input) -&gt; DynamicsCompressorNode (Compressor) -&gt; AudioDestinationNode (Output)
oscillator.connect(compressor);
compressor.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 10 sec
oscillator.stop(context.currentTime + 10);</code></pre>
            <img src="images/dynamics-compressor-node.png" alt="DynamicsCompressorNode" width="1232" height="770" loading="lazy" />
            <p>
              以下は, <a href="#section-oscillator-node-synthesize">基本波形の合成セクション</a>でコード例として記載した, 和音となる
              <code>OscillatorNode</code> の合成で, ゲインの調整ではなく, コンプレッサーを利用することで (<code>DynamicsCompressorNode</code>
              インスタンスを接続することで), 意図しない音割れを防止するコードです.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

// C major chord
const oscillatorC = new OscillatorNode(context, { frequency: 261.6255653005991 });
const oscillatorE = new OscillatorNode(context, { frequency: 329.6275569128705 });
const oscillatorG = new OscillatorNode(context, { frequency: 391.9954359817500 });

const compressor = new DynamicsCompressorNode(context);

// If use `createDynamicsCompressor`
// const compressor = context.createDynamicsCompressor();

// OscillatorNode (Input) -&gt; DynamicsCompressorNode (Compressor) -&gt; AudioDestinationNode (Output)
oscillatorC.connect(compressor);
oscillatorE.connect(compressor);
oscillatorG.connect(compressor);
compressor.connect(context.destination);

// Start oscillators immediately
oscillatorC.start(0);
oscillatorE.start(0);
oscillatorG.start(0);

// Stop oscillators after 10 sec
oscillatorC.stop(context.currentTime + 10);
oscillatorE.stop(context.currentTime + 10);
oscillatorG.stop(context.currentTime + 10);</code></pre>
            <p>
              「おそらくもっとも簡単」と表現したのは, <code>AudioNode</code> 単体の接続とデフォルト値でエフェクターとして機能することが理由です. すでに,
              解説したように <code>DelayNode</code> や <code>BiquadFilterNode</code> 単体としては現実世界の (音楽表現として意味をもつ)
              エフェクターとして機能することはないからです.
            </p>
          </section>
          <section id="section-effectors-compressor-parameters">
            <h4>コンプレッサーの仕組み</h4>
            <p>
              <code>DynamicsCompressorNode</code> で制御可能なパラメータ (<code>AudioParam</code> インスタンス) としては, <b><code>threshold</code></b>, <b><code>knee</code></b>, <b><code>ratio</code></b>, <b><code>attack</code></b>, <b><code>release</code></b> がありますが, コンプレッサーの効果に大きく影響するのは, <code>threshold</code> (<b>閾値</b>) と
              <code>ratio</code> (<b>レシオ値</b>) です.
            </p>
            <p>
              <code>threshold</code> は, コンプレッサーによる振幅の圧縮がかかる閾値を指定します. 逆に言うと,
              <code>threshold</code> 以下の振幅には影響を与えません. 単位はデシベルで, <code>-100 dB</code> から
              <code>0 dB</code> デシベルまでの範囲で設定可能です. <code>threshold</code> が <code>0</code> (<code>dB</code>) の場合は,
              コンプレッサーが効いていないのと同じで, <code>-100</code> (<code>dB</code>) に近づくほど,
              より小さな振幅に対してもコンプレッサーが効くことになります (デフォルト値は, <code>-24</code> (<code>dB</code>) です).
            </p>
            <p>
              <code>ratio</code> は, <code>threshold</code> を超えた振幅を圧縮する割合を指定します (デフォルト値は <code>12</code> なので,
              <code>threshold</code> を超えた振幅値を <span class="math-inline">$\frac{1}{12}$</span> 倍に圧縮します). したがって, <code>ratio</code> が
              <code>1</code> (最小値) の場合は, まったく圧縮をしないので, コンプレッサーが効いてないのと同じで, ここから
              <span class="math-inline">$\infty$</span> (ただし, 実際の最大値は <code>20</code> です) に近づくほど,
              <code>threshold</code> に近い値に圧縮されていきます. 圧縮後の振幅値は, <code>threshold</code> の振幅値と圧縮された振幅値を加算した値となります.
              <b>リミッター</b>は, <code>ratio</code> が <span class="math-inline">$\infty$</span> のコンプレッサーであり,
              <code>threshold</code> を超える振幅はすべて <code>threshold</code> にまで圧縮します.
            </p>
            <p>
              <code>knee</code> は <code>threshold</code> を超えた振幅の圧縮を緩やかに処理する (<b>Soft knee</b>) か, 急峻に処理する (<b>Hard knee</b>)
              かを決定します. デフォルト値は <code>30</code> (<code>dB</code>) ですが, 最大値の <code>40</code> (<code>dB</code>) に近づくほど,
              より緩やかな振幅の圧縮となり, 最小値の <code>0</code> (<code>dB</code>) に近づくほど, 急峻な圧縮となります (音楽的には,
              コンプレッサーが急激に効いてしまうと, アナログ感のあるスムーズな圧縮にならず, いわゆる, 「<b>デジタルくさい音</b>」と言われることもあるので,
              それを緩和するパラメータと考えるとよいかもしれません (また, <code>knee</code> の値とコンプレッションカーブの関係 (関数) は,
              単調増加する関数であることが唯一の仕様となっているので, その詳細はレンダリングエンジンの実装に依存することになります).
            </p>
            <figure>
              <div class="flexbox">
                <svg id="svg-figure-compressor-parameters" width="424" height="424" />
                <dl class="flexbox-column">
                  <dt>
                    <label for="svg-figure-compressor-range-threshold">threshold: </label><span id="svg-figure-compressor-range-threshold-value">-24 dB</span>
                  </dt>
                  <dd>
                    <input type="range" id="svg-figure-compressor-range-threshold" value="-24" min="-100" max="0" step="1" />
                  </dd>
                  <dt><label for="svg-figure-compressor-range-ratio">ratio: </label><span id="svg-figure-compressor-range-ratio-value">12</span></dt>
                  <dd>
                    <input type="range" id="svg-figure-compressor-range-ratio" value="12" min="1" max="20" step="1" />
                  </dd>
                  <dt><label for="svg-figure-compressor-range-knee">knee: </label><span id="svg-figure-compressor-range-knee-value">30 dB</span></dt>
                  <dd>
                    <input type="range" id="svg-figure-compressor-range-knee" value="30" min="0" max="40" step="1" />
                  </dd>
                </dl>
              </div>
              <figcaption><code>threshold</code> (閾値), <code>ratio</code> (レシオ値), <code>knee</code></figcaption>
            </figure>
            <p>
              また, 振幅を圧縮した結果, 振幅を大きくする余地が発生します. これによって, 相対的に振幅の小さい音もある程度大きくすることが可能になります (ただし,
              <code>DynamicsCompressorNode</code> の仕様上, どのようなアルゴリズムよって増幅するかは記載されていないので,
              レンダリングエンジンの実装に依存することになります).
            </p>
            <figure>
              <div><svg id="svg-figure-compressor-lower-volume-and-raise-volume-by-compressor-curve" width="800" height="400" /></div>
              <div><svg id="svg-figure-compressor-lower-volume-and-raise-volume" width="800" height="280" /></div>
              <figcaption>コンプレッサーによる圧縮と増幅</figcaption>
            </figure>
            <p>
              <code>attack</code> と <code>release</code> はイメージとしては, エンベロープジェネレーターの <code>attack</code> と <code>release</code> と同じで,
              コンプレッサーが効き始めと終わりの時間的変化を指定することになります. 仕様としては, <code>attack</code> は, コンプレッサーが効き始めて,
              <code>10 dB</code> 圧縮されるまでの時間 (秒単位. デフォルト値は, <code>0.003 sec</code>), <code>release</code> は,
              コンプレッサーが効き終わってから, <code>10 dB</code> 増幅されるまでの時間 (秒単位. デフォルト値は, <code>0.25 sec</code>) となっています. また,
              <code>attack</code> と <code>release</code> による, コンプレッサーの時間的な圧縮レベルの変化は, 読み取り専用の
              <b><code>reduction</code></b> プロパティ (単位は <code>dB</code>)で参照することが可能です.
            </p>
          </section>
        </section>
        <section id="section-effectors-auto-panner">
          <h3>オートパン</h3>
          <p>
            <b>オートパン</b>とは, <b>音像</b>を周期的に左右のチャンネルに振るエフェクターです. 楽器演奏でのエフェクターとして使われることはほとんどなく, DAW
            (Digital Audio Workstation) を利用した音楽制作・音源編集で使われることが多いエフェクターです. Web Audio API において,
            オートパンを実装する方法はいくつかありますが, もっとも簡単な実装は <b><code>StereoPannerNode</code></b> クラスを利用する実装です.
            オーディオ信号処理としては, <b>左右のチャンネルが互いに逆位相となるトレモロ</b>をかけることで実装する方法が一般的でもあるので,
            こちらの実装も解説したいと思います (さらに, Web Audio API において, チャンネルごとに独立したオーディオ信号処理を適用するための,
            <b><code>ChannelSplitterNode</code></b> クラス, および,
            <b><code>ChannelMergerNode</code></b> クラスの基本を理解するためにも適切なエフェクターだからです).
          </p>
          <section id="section-effectors-auto-panner-sound-image">
            <h4>音像</h4>
            <p>
              人間の聴覚は, 音を聴いただけで, 音源の位置や音源の (物理的な) 大きさや形などを知覚することができます. この感覚的に (実際の音源はどうであれ)
              知覚した音を, <b>音像</b>と呼びます. すなわち, <b>聴覚上の音源</b>とも言えます. Web Audio API の音像に関しては,
              <code>StereoPannerNode</code> クラスもそうですが, あとのセクションで解説する <code>PannerNode</code> クラスも音像を変化させることが可能ですが,
              <b>音源の位置</b>のみの設定です (<code>AudioListener</code> クラスで, リスナーの位置も設定可能で, これによって音像を変化させることもできます).
              したがって, Web Audio API のスコープの中では, 音像, イコール, 音源の位置と理解しても問題ありません. また, 音像を知覚することを,
              <b>音像定位</b>と呼びます.
            </p>
            <p>
              音像を適切に設定したり, 変化させることで, 音に立体感や臨場感を与えることができます. 特に, CG (WebGL) と音楽を利用するような Web
              アプリケーションでは (理論上は) 重要なエフェクターと言えます (しかしながら, 現状の Web Audio API における <code>PannerNode</code> クラス, および,
              <code>AudioListener</code> クラスは, それらのユースケースに十分に応えられるほど精度のよいものではありません).
            </p>
          </section>
          <section id="section-effectors-auto-panner-stereo-panner-node">
            <h4>StereoPannerNode</h4>
            <p>
              <b><code>StereoPannerNode</code></b> クラスは, 音像を左右に移動させることだけに特化した <code>AudioNode</code> です (<code>PannerNode</code>
              クラスの <code>positionX</code> プロパティ (<code>AudioParam</code>) のみを設定可能なクラスで,
              <code>PannerNode</code> クラスのサブセット的なクラスと言えます). 音楽制作・音源編集などのユースケースでは,
              左右のチャンネルに音像を振ることができれば十分要件を満たすことができるので, すなわち, オートパンにおいても
              <code>StereoPannerNode</code> で実装可能です (<code>PannerNode</code> クラスは左右のチャンネルに音像を設定するというよりは,
              あくまで立体音響でのユースケースのために定義されています).
            </p>
            <p>
              <code>StereoPannerNode</code> には, <code>AudioParam</code> の <b><code>pan</code></b> プロパティのみが定義されています. デフォルト値は
              <code>0</code> で, 音像は変化しません. <code>pan</code> プロパティの値は, 最大値 (<code>maxValue</code>) が <code>1</code>, 最小値
              (<code>minValue</code>) が <code>-1</code> ですが,
              <b>最大値に近いほど, 右チャンネルに音像が振られ, 最小値に近いほど, 左チャンネルに音像が振られます</b>.
            </p>
            <p>
              以下のコードは, <code>pan</code> プロパティの初期値を <code>-1</code> (つまり, 左チャンネルに完全に音像を振った状態) に設定して
              (コンストラクタの第 2 引数 <b><code>StereoPannerOptions</code></b> で指定しています. ファクトリメソッドを利用する場合は,
              <code>AudioParam</code> の <code>value</code> プロパティで設定してください). タイマーによって, 右チャンネル,
              左チャンネルに交互に音像を移動させます (<b>ヘッドフォンやイヤフォンを着用して確認することを推奨します</b>).
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);

const panner = new StereoPannerNode(context, { pan: -1 });

// If use `createStereoPanner`
// const panner = context.createStereoPanner();
//
// panner.pan.value = -1;

// OscillatorNode (Input) -&gt; StereoPannerNode (pan) -&gt; AudioDestinationNode (Output)
oscillator.connect(panner);
panner.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 10 sec
oscillator.stop(context.currentTime + 10);

const intervalid = window.setInterval(() =&gt; {
  panner.pan.value *= -1;
}, 2000);

oscillator.onended = () =&gt; {
  window.clearInterval(intervalid);
};</code></pre>
            <p>オートパンを実装するには, <code>AudioParam</code> である <code>pan</code> に, LFO を接続して周期的に変化させることで可能になります.</p>
            <figure>
              <svg id="svg-figure-node-connections-for-auto-panner" width="800" height="520" />
              <figcaption>オートパンのノード接続図</figcaption>
            </figure>
            <p>
              以下は, 実際のアプリケーションを想定して, ユーザーインタラクティブに, オートパンに関わるパラメータを制御できるようにしたコード例です. Depth
              が大きいほど, より大きく左右に音像が振れて, Rate を高くするほど, より速く左右に音像が振られるようになります.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;button type=&quot;button&quot;&gt;start&lt;/button&gt;
&lt;label&gt;
  &lt;input type=&quot;checkbox&quot; id=&quot;checkbox-auto-panner&quot; checked /&gt;
  &lt;span id=&quot;print-checked-auto-panner&quot;&gt;ON&lt;/span&gt;
&lt;/label&gt;
&lt;label for=&quot;range-auto-panner-depth&quot;&gt;Depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-auto-panner-depth&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-auto-panner-depth-value&quot;&gt;0&lt;/span&gt;
&lt;label for=&quot;range-auto-panner-rate&quot;&gt;Rate&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-auto-panner-rate&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;5&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-auto-panner-rate-value&quot;&gt;0&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

let rateValue = 0;

let oscillator = new OscillatorNode(context);
let lfo        = new OscillatorNode(context, { frequency: rateValue });

let isStop = true;

const depth = new GainNode(context);

const panner = new StereoPannerNode(context);

const buttonElement   = document.querySelector(&apos;button[type=&quot;button&quot;]&apos;);
const checkboxElement = document.querySelector(&apos;input[type=&quot;checkbox&quot;]&apos;);

const rangeDepthElement = document.getElementById(&apos;range-auto-panner-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-auto-panner-rate&apos;);

const spanPrintCheckedElement = document.getElementById(&apos;print-checked-auto-panner&apos;);
const spanPrintDepthElement   = document.getElementById(&apos;print-auto-panner-depth-value&apos;);
const spanPrintRateElement    = document.getElementById(&apos;print-auto-panner-rate-value&apos;);

checkboxElement.addEventListener(&apos;click&apos;, () =&gt; {
  oscillator.disconnect(0);
  lfo.disconnect(0);

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; StereoPannerNode (Auto Panner) -&gt; AudioDestinationNode (Output)
    oscillator.connect(panner);
    panner.connect(context.destination);

    // Connect nodes for LFO that changes pan periodically
    // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; pan (AudioParam)
    lfo.connect(depth);
    depth.connect(panner.pan);

    spanPrintCheckedElement.textContent = &apos;ON&apos;;
  } else {
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    spanPrintCheckedElement.textContent = &apos;OFF&apos;;
  }
});

buttonElement.addEventListener(&apos;mousedown&apos;, () =&gt; {
  if (!isStop) {
    return;
  }

  if (checkboxElement.checked) {
    // Connect nodes
    // OscillatorNode (Input) -&gt; StereoPannerNode (Auto Panner) -&gt; AudioDestinationNode (Output)
    oscillator.connect(panner);
    panner.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  } else {
    panner.disconnect(0);

    // Connect nodes (Auto Panner OFF)
    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
    oscillator.connect(context.destination);

    // Start oscillator
    oscillator.start(0);
  }

  // Connect nodes for LFO that changes pan periodically
  // OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; pan (AudioParam)
  lfo.connect(depth);
  depth.connect(panner.pan);

  lfo.start(0);

  isStop = false;

  buttonElement.textContent = &apos;stop&apos;;
});

buttonElement.addEventListener(&apos;mouseup&apos;, () =&gt; {
  if (isStop) {
    return;
  }

  // Stop immediately
  oscillator.stop(0);
  lfo.stop(0);

  oscillator = new OscillatorNode(context);
  lfo        = new OscillatorNode(context, { frequency: rateValue });

  isStop = true;

  buttonElement.textContent = &apos;start&apos;;
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const depthValue = event.currentTarget.valueAsNumber;

  depth.gain.value = depthValue;

  spanPrintDepthElement.textContent = depthValue.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  rateValue = event.currentTarget.valueAsNumber;

  if (lfo) {
    lfo.frequency.value = rateValue;
  }

  spanPrintRateElement.textContent = rateValue.toString(10);
});</code></pre>
            <div class="app-container app-auto-panner">
              <div class="app-headline">
                <button type="button" id="button-auto-panner">start</button>
                <label>
                  <input type="checkbox" id="checkbox-auto-panner" checked />
                  <span id="print-checked-auto-panner">ON</span>
                </label>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-auto-panner-depth">Depth</label></dt>
                    <dd>
                      <input type="range" id="range-auto-panner-depth" value="0" min="0" max="1" step="0.05" />
                      <span id="print-auto-panner-depth-value">0</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-auto-panner-rate">Rate</label></dt>
                    <dd>
                      <input type="range" id="range-auto-panner-rate" value="0" min="0" max="5" step="0.05" />
                      <span id="print-auto-panner-rate-value">0</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
          </section>
          <section id="section-effectors-auto-panner-tremolo">
            <h4>トレモロによる実装</h4>
            <p>
              人間の聴覚は, (実際に音源があるかは無関係に) 音が大く聴こえてくる方向に音源があると判断する仕組みがあります. これを,
              <b>インテンシティ効果</b>と呼びます. つまり, 実際に音源がなくても, 左側から聴こえてくる音が右側から聴こえてくる音より, より大きいほど,
              音源が左側にあると知覚します. パンの変更やオートパンはこのインテンシティ効果を利用したエフェクターと言えます.
            </p>
            <p>
              トレモロによるオートパンの実装は, インテンシティ効果を直接コードに落とし込んだ実装と言えます. 左右の位相が互いに逆位相になることにより,
              徐々に音像が左右の間で移動することになります. 左右の位相が互いに逆位相になることにより, 徐々に音像が左右の間で移動することになります.
              位相差が大きいほど, 左右のどちらかに, 小さいほど中央に音像が移動します.
            </p>
            <section id="section-effectors-auto-panner-tremolo-channel-splitter-node">
              <h5>ChannelSplitterNode</h5>
              <p>
                <b><code>ChannelSplitterNode</code></b> は, 指定のチャンネル数に分割するためのクラスです. デフォルト値としては,
                <code>6</code> チャンネルに分割します (おそらく, このデフォルト値は, 5.1 チャンネルサラウンド方式を考慮しての値と思われます). つまり, ステレオ
                (2 チャンネル) に分割するのであれば, コンストラクタの第 2 引数 <b><code>ChannelSplitterOptions</code></b> 型の
                <b><code>numberOfOutputs</code></b> プロパティに <code>2</code> を指定します (ファクトリメソッドを利用する場合は, 第 1 引数に指定します).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const splitter = new ChannelSplitterNode(context, { numberOfOutputs: 2 });

// If use `createChannelSplitter`
// const splitter = context.createChannelSplitter(2);</code></pre>
              <img src="images/channel-splitter-node.png" alt="ChannelSplitterNode" width="1232" height="770" loading="lazy" />
            </section>
            <section id="section-effectors-auto-panner-tremolo-channel-merger-node">
              <h5>ChannelMergerNode</h5>
              <p>
                <b><code>ChannelMergerNode</code></b> は, <code>ChannelSplitterNode</code> によって分割された複数のチャンネルを 1 つのチャンネル (ストリーム)
                にまとめるためのクラスです (したがって, <code>ChannelSplitterNode</code> と併用するケースがほとんどです).
                <code>ChannelSplitterNode</code> のデフォルト値にしたがって, デフォルトの入力チャンネル数は <code>6</code> チャンネルです. つまり, ステレオ (2
                チャンネル) を 1 つのチャンネルにまとめるのであれば, コントラクタの第 2 引数 <b><code>ChannelMergerOptions</code></b> 型の
                <b><code>numberOfInputs</code></b> プロパティに <code>2</code> を指定します (ファクトリメソッドを利用する場合は, 第 1 引数に指定します).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const merger = new ChannelMergerNode(context, { numberOfInputs: 2 });

// If use `createChannelMerge`
// const merger = context.createChannelMerger(2);</code></pre>
              <img src="images/channel-merger-node.png" alt="ChannelMergerNode" width="1232" height="770" loading="lazy" />
            </section>
            <section id="section-effectors-auto-panner-tremolo-connect-channel">
              <h5>connect メソッドの接続先チャンネル指定</h5>
              <p>
                <code>connect</code> メソッドには, いくつかのオーバーロードされたシグネチャがありますが, <code>ChannelSplitterNode</code> /
                <code>ChannelMergerNode</code> を利用する場合に, 第 2 引数と第 3 引数にチャンネルを指定する形式が実質的に有用になります (ちなみに, 第 2 引数, 第
                3 引数はオプショナルです).
              </p>
              <p>
                第 2 引数には, <code>ChannelSplitterNode</code> で分割した出力先のチャンネルナンバーを指定します. 例えば, ステレオなら, 左チャンネルであれば
                <code>0</code>, 右チャンネルであれば <code>1</code> を指定します.
              </p>
            </section>
            <section id="section-effectors-auto-panner-tremolo-code">
              <h5>実装</h5>
              <p>
                トレモロによるオートパンの実装で, <code>ChannelSplitterNode</code> / <code>ChannelMergerNode</code>, および, <code>connect</code> メソッドの第 2
                引数, 第 3 引数の指定を解説します.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-html line-numbers">&lt;audio src=&quot;https://korilakkuma.github.io/Web-Music-Documentation/assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&quot; crossorigin="anonymous" controls /&gt;</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const audioElement = document.querySelector(&apos;audio&apos;);

const source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

const amplitudeL = new GainNode(context, { gain: +1.0 });  // +1.0 +- ${depthValue}
const amplitudeR = new GainNode(context, { gain: -1.0 });  // -1.0 +- ${depthValue} Inverse Phase

const lfo   = new OscillatorNode(context, { frequency: 0 });
const depth = new GainNode(context, { gain: 0 });

const splitter = new ChannelSplitterNode(context, { numberOfOutputs: 2 });
const merger   = new ChannelMergerNode(context, { numberOfInputs: 2 });

const lfoSplitter = new ChannelSplitterNode(context, { numberOfOutputs: 2 });

// Connect nodes
//                                                              -&gt; GainNode (Amplitude) Output Channel Number is `0` (Left Channel)  -&gt; Input Channel Number is `0` (Left Channel)  -
// MediaElementAudioSourceNode (Input) -&gt; ChannelSplitterNode -|                                                                                                                    | -&gt; ChannelMergerNode -&gt; AudioDestinationNode (Output)
//                                                              -&gt; GainNode (Amplitude) Output Channel Number is `1` (Right Channel) -&gt; Input Channel Number is `1` (Right Channel) -
source.connect(splitter);
splitter.connect(amplitudeL, 0, 0);
splitter.connect(amplitudeR, 1, 0);
amplitudeL.connect(merger, 0, 0);
amplitudeR.connect(merger, 0, 1);
merger.connect(context.destination);

// Connect nodes for LFO that changes gain periodically
// OscillatorNode (LFO) -&gt; GainNode (Depth) -&gt; ChannelSplitterNode
lfo.connect(depth);
depth.connect(lfoSplitter);

// ChannelSplitterNode (Left Channel) -&gt;  gain (AudioParam) (Left Channel)
lfoSplitter.connect(amplitudeL.gain, 0);

// ChannelSplitterNode (Right Channel) -&gt;  gain (AudioParam) (Right Channel)
lfoSplitter.connect(amplitudeR.gain, 1);

lfo.start(0);

const rangeDepthElement = document.getElementById(&apos;range-auto-panner-by-tremolo-depth&apos;);
const rangeRateElement  = document.getElementById(&apos;range-auto-panner-by-tremolo-rate&apos;);

const spanPrintDepthElement = document.getElementById(&apos;print-auto-panner-by-tremolo-depth-value&apos;);
const spanPrintRateElement  = document.getElementById(&apos;print-auto-panner-by-tremolo-rate-value&apos;);

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const depthValue = event.currentTarget.valueAsNumber;

  depth.gain.value = depthValue;

  spanPrintDepthElement.textContent = depthValue.toString(10);
});

rangeRateElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const rateValue = event.currentTarget.valueAsNumber;

  lfo.frequency.value = rateValue;

  spanPrintRateElement.textContent = rateValue.toString(10);
});</code></pre>
              <p>
                左右のチャンネルに, 互いに逆位相となるトレモロをかけるので, <code>ChannelSplitterNode</code> の <code>numberOfOutputs</code> プロパティ,
                <code>ChannelMergerNode</code> の <code>numberOfInputs</code> プロパティのチャンネル数はそれぞれ, <code>2</code> (以上) を指定します. また,
                振幅変調させるための <code>GainNode</code> インスタンスも, それぞれのチャンネルに必要なので 2 つ生成します. また,
                <b
                  >左右の <code>gain</code> プロパティの値を互いに逆, つまり, 絶対値を同じにして, 一方を負数にすることで,
                  左右のチャンネルが逆位相となるトレモロが実装可能です</b>.
                <b
                  ><code>GainNode</code> インスタンスの <code>gain</code> プロパティの値を負数 (<code>-1</code>) にして, 逆位相にする実装は Web Audio API
                  におけるテクニックの 1 つです</b>
                (もっとも, このような Web Audio API 特有のテクニックが, 他のオーディオ API 経験者からすると, 奇怪な API に思われる一因かもしれません).
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{flalign}
                    &y_{L}\left(n\right) = \left(1 + \mathrm{depth} \cdot \sin\left(\frac{2\pi \cdot \mathrm{rate} \cdot n}{f_{s}}\right)\right) \cdot x_{L}\left(n\right) \\
                    &y_{R}\left(n\right) = \left(1 - \mathrm{depth} \cdot \sin\left(\frac{2\pi \cdot \mathrm{rate} \cdot n}{f_{s}}\right)\right) \cdot x_{R}\left(n\right) \\
                  \end{flalign}
                $
              </div>
              <p>
                <code>ChannelSplitterNode</code> によって分割された出力は, <code>numberOfOutputs</code> で指定したチャンネル数と同じです.
                それぞれのチャンネルを<b>接続先の <code>AudioNode</code>, または, <code>AudioParam</code> の入力チャンネル</b>として (<b
                  ><code>ChannelSplitterNode</code> の出力チャンネル</b>として), <code>connect</code> メソッドの第 2 引数に, チャンネルナンバーを指定します (第 3 引数はデフォルトで <code>0</code> なので,
                不要であれば省略可能です)
              </p>
              <p>
                <code>ChannelSplitterNode</code> でチャンネルを分割して, それぞれにオーディオ処理を適用したあと, 1 つのチャンネル (ストリーム) にまとめるために,
                <code>ChannelMergerNode</code> へ接続します. <code>ChannelMergerNode</code> へまとめられるチャンネルの入力は,
                <code>numberOfInputs</code> で指定したチャンネル数と同じです. それぞれのチャンネルを<b>接続元の <code>AudioNode</code> の出力チャンネル</b>として (<b><code>ChannelMergerNode</code> の入力チャンネル</b>として), <code>connect</code> メソッドの第 3 引数に,
                チャンネルナンバーを指定します (また, <code>AudioParam</code> の場合, 分割したチャンネル (ストリーム) をまとめる必要はないので,
                <code>ChannelMergerNode</code> へ接続するのは, 分割された <code>AudioNode</code> のみです).
              </p>
              <figure>
                <svg id="svg-figure-node-connections-for-auto-panner-by-tremolo" width="1150" height="940" />
                <figcaption>トレモロによるオートパンのノード接続図</figcaption>
              </figure>
              <p>以下は, トレモロによるオートパンのデモです.</p>
              <div class="app-container">
                <div class="app-headline">
                  <audio
                    id="audio-auto-panner-by-tremolo"
                    src="https://korilakkuma.github.io/Web-Music-Documentation/assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3"
                    crossorigin="anonymous"
                    controls
                  ></audio>
                </div>
                <div>
                  <dl>
                    <div>
                      <dt><label for="range-auto-panner-by-tremolo-depth">Depth</label></dt>
                      <dd>
                        <input type="range" id="range-auto-panner-by-tremolo-depth" value="0" min="0" max="1" step="0.05" disabled />
                        <span id="print-auto-panner-by-tremolo-depth-value">0</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-auto-panner-by-tremolo-rate">Rate</label></dt>
                      <dd>
                        <input type="range" id="range-auto-panner-by-tremolo-rate" value="0" min="0" max="5" step="0.05" disabled />
                        <span id="print-auto-panner-by-tremolo-rate-value">0</span>
                      </dd>
                    </div>
                  </dl>
                </div>
              </div>
              <p>
                <code>OscillatorNode</code> は 1 チャンネルしかもたないモノラルな入力なので, 入力ノードはステレオ入力となるオーディオデータをもつ
                <code>MediaElementAudioSourceNode</code> を利用しています. したがって, オートパンを実装するには,
                <code>StereoPannerNode</code> を利用したほうが実装も簡潔で汎用性も高いですが, <code>ChannelSplitterNode</code> と
                <code>ChannelMergerNode</code> の解説として好例なのであえて解説しました.
              </p>
            </section>
          </section>
        </section>
        <section id="section-effectors-glide">
          <h3>グライド (ポルタメント)</h3>
          <p>
            <b>グライド</b> (<b>ポルタメント</b>) とは, シンセサイザー特有のエフェクターで, 周波数の変化をなめらかにすることによって,
            シンセサイザーらしい音色にすることができます. また, Web Audio API においては, すでに解説した, パラメータのオートメーション (<code>AudioParam</code>
            のスケジューリング) のみで実装可能です. シンセサイザー特有のエフェクターでありますが, 同様の実装で, 演奏記号の<b>スラー</b>,
            ギターやベースの<b>スライド奏法</b>を実現することも可能です.
          </p>
          <section id="section-effectors-glide-parameter-automation">
            <h4>グライドのパラメータのオートメーション</h4>
            <p>
              グライドに必要となるパラメータのオートメーションは, <code>OscillatorNode</code> インスタンスの <code>frequency</code> プロパティ (または
              <code>detune</code> プロパティ) で, グライドタイムの経過後に, 次の <code>frequency</code> プロパティの値になるように変化させます. 以下の実装では,
              <code>linearRampToValueAtTime</code> メソッドを利用して線形的に変化させていますが,
              <code>exponentialRampToValueAtTime</code> メソッドを利用して指数関数的に変化させてもよいでしょう. グライドのアルゴリズム上,
              次に発音する音の周波数が同じ場合, 効果はありません.
            </p>
            <p>
              <code>OscillatorNode</code> インスタンスの <code>frequency</code> プロパティのオートメーションと同様に,
              <code>AudioBufferSourceNode</code> インスタンスの <code>detune</code> プロパティ (または <code>playbackRate</code> プロパティ) を変化させれば,
              スラーやスライド奏法の実現となります.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const defaultFrequency = 440;
const nextFrequency    = 880;

const glide = 1.5;

const oscillator = new OscillatorNode(context);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

const t0 = context.currentTime;
const t1 = t0 + glide;

// Schedule `frequency` (`AudioParam`) for Glide
oscillator.frequency.setValueAtTime(defaultFrequency, t0);
oscillator.frequency.linearRampToValueAtTime(nextFrequency, t1);

// Start oscillator immediately
oscillator.start(t0);

// Stop oscillator after 2.5 sec
oscillator.stop(t1 + 2.5);</code></pre>
            <p>
              以下は, シンセサイザーで利用することを想定して, 鍵盤の UI でインタラクティブに操作できるようにしたコード例です (グライドタイムとメソッド (<code
                >linearRampToValueAtTime</code>
              メソッド, または, <code>exponentialRampToValueAtTime</code> メソッド) もインタラクティブに制御できるようにしています.
            </p>
            <p>また, 鍵盤の UI の実装はいくつかアプローチ (SVG, Canvas ... etc) がありますが, ここではシンプルに HTML と CSS で実装しています).</p>
            <p>
              HTML や CSS のコードまで記載すると, グライドの本質的な解説からそれてしまうので, ディベロッパーツールなどを使いながら, HTML や CSS
              をリーディングしていただくとして, その UI がある前提で, グライドのコードを記載します.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const keyboards = document.querySelectorAll(&apos;.piano button[type=&quot;button&quot;]&apos;);

const frequencyRatio = 2 ** (1 / 12);

let glideTime = 0;
let glideType = &apos;linear&apos;;

// Invalid frequency
let prevFrequency = -1;

keyboards.forEach((keyboard) =&gt; {
  let oscillator = null;

  const onDown = async (event) =&gt; {
    if (context.state !== &apos;running&apos;) {
      await context.resume();
    }

    const t0 = context.currentTime;

    if (oscillator !== null) {
      // If starts next oscillator during glide, should cancel scheduling
      oscillator.frequency.cancelScheduledValues(t0);
      oscillator.stop(t0);
    }

    const pianoIndex    = Number(keyboard.getAttribute(&apos;data-index&apos;));
    const nextFrequency = 27.5 * (frequencyRatio ** pianoIndex);

    oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos;, frequency: (prevFrequency === -1 ? nextFrequency : prevFrequency) });

    oscillator.connect(context.destination);

    if (prevFrequency !== -1) {
      const t1 = t0 + glideTime;

      oscillator.frequency.setValueAtTime(prevFrequency, t0);

      switch (glideType) {
        case &apos;linear&apos;: {
          oscillator.frequency.linearRampToValueAtTime(nextFrequency, t1);
          break;
        }

        case &apos;exponential&apos;: {
          oscillator.frequency.exponentialRampToValueAtTime(nextFrequency, t1);
          break;
        }
      }
    }

    // Update frequency
    prevFrequency = nextFrequency;

    oscillator.start(t0);

    keyboard.classList.add(&apos;pressed&apos;);
  };

  const onUp = () =&gt; {
    if (oscillator === null) {
      return;
    }

    oscillator.frequency.cancelScheduledValues(context.currentTime);
    oscillator.stop(0);

    oscillator = null;

    keyboard.classList.remove(&apos;pressed&apos;);
  };

  keyboard.addEventListener(&apos;mousedown&apos;, onDown);
  keyboard.addEventListener(&apos;touchstart&apos;, onDown);
  keyboard.addEventListener(&apos;mouseup&apos;, onUp);
  keyboard.addEventListener(&apos;touchend&apos;, onUp);
});

const rangeGlideTimeElement = document.getElementById(&apos;range-glide-time&apos;);
const formGlideTypeElement  = document.getElementById(&apos;form-glide-type&apos;);

const spanPrintGlideTimeElement = document.getElementById(&apos;print-glide-time-value&apos;);

rangeGlideTimeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  glideTime = event.currentTarget.valueAsNumber;

  spanPrintGlideTimeElement.textContent = glideTime.toFixed(2);
});

formGlideTypeElement.addEventListener(&apos;change&apos;, () =&gt; {
  const radios = formGlideTypeElement.elements[&apos;radio-glide-type&apos;];

  for (const radio of radios) {
    if (radio.checked) {
      glideType = radio.value;
      break;
    }
  }
});</code></pre>
            <p>
              UI と関連するイベントリスナーのコードも混在していますが, 以下の 3 つが実用的なグライドを実装する場合に重要となります
              (コードのコメントとしても記載しています).
            </p>
            <ul>
              <li>
                グライドの原理上, 初回の発音では無効なので, 初回の発音が判定できるフラグなどを定義します. 上記のコードでは,
                <code>prevFrequency</code> 変数を周波数としては無効な <code>-1</code> に設定することで初回の発音であることを判定して分岐させています (また,
                このほうが, 初回の発音時は, 前回の発音はないので, より原理を実装に落とし込んだコードと言えそうです. もちろん,
                <code>boolean</code> 型の変数などで判定しても実装としては問題ありません)
              </li>
              <li>
                発音のたびに, 現在発音している周波数 (コードでは <code>nextFrequency</code> 変数) を次回の発音まで参照できるようにします. 実装としては,
                <code>prevFrequency</code> 変数を更新してしまうのがよいでしょう (1 つ前より以前の周波数を参照する必要はないので)
              </li>
              <li>
                上記の 2 つが実装できていれば, グライドとしてはほぼ問題なく機能しますが, 鍵盤を連打した場合など, 前回の
                <code>frequency</code> プロパティのスケジューリングが残っている可能性があるので, 発音のたびに,
                <code>OscillatorNode</code> インスタンスが存在していれば, <code>cancelScheduledValues</code> メソッドでクリアして, 即時停止します (ちなみに,
                Firefox の対応が不要であれば, <code>cancelAndHoldAtTime</code> メソッドを利用してもよいでしょう. その場合,
                <code>setValueAtTime</code> メソッドが不要になります)
              </li>
            </ul>
            <p>
              また, グライドの実装とは関係ありませんが, このような鍵盤の UI を利用して発音する場合, 周波数の算出式として,
              <span class="math-inline">$27.5 \cdot \mathrm{pow}\left(\mathrm{pow}\left(2, \left(\frac{1}{12}\right)\right), \mathrm{index}\right)$</span>
              を利用すると便利です. <span class="math-inline">$\mathrm{index}$</span> は, ピアノ 88 鍵を配列に見立てて, 最も左側にある鍵盤 (<code>27.5 Hz</code>) のインデックスを <code>0</code> とした場合の値です. また, そのインデックスは, それぞれの鍵盤を構成する HTML
              のカスタムーデータ属性として定義しておくことで, イベントリスナーで簡単に取得することが可能です (もちろん, 他のよりよい実装があるかもしれませんが).
            </p>
            <div class="app-container app-glide">
              <div class="piano">
                <ul class="white-keyboards">
                  <li><button type="button" data-index="0">A0</button></li>
                  <li><button type="button" data-index="2">B0</button></li>
                  <li><button type="button" data-index="3">C1</button></li>
                  <li><button type="button" data-index="5">D1</button></li>
                  <li><button type="button" data-index="7">E1</button></li>
                  <li><button type="button" data-index="8">F1</button></li>
                  <li><button type="button" data-index="10">G1</button></li>
                  <li><button type="button" data-index="12">A1</button></li>
                  <li><button type="button" data-index="14">B1</button></li>
                  <li><button type="button" data-index="15">C2</button></li>
                  <li><button type="button" data-index="17">D2</button></li>
                  <li><button type="button" data-index="19">E2</button></li>
                  <li><button type="button" data-index="20">F2</button></li>
                  <li><button type="button" data-index="22">G2</button></li>
                  <li><button type="button" data-index="24">A2</button></li>
                  <li><button type="button" data-index="26">B2</button></li>
                  <li><button type="button" data-index="27">C3</button></li>
                  <li><button type="button" data-index="29">D3</button></li>
                  <li><button type="button" data-index="31">E3</button></li>
                  <li><button type="button" data-index="32">F3</button></li>
                  <li><button type="button" data-index="34">G3</button></li>
                  <li><button type="button" data-index="36">A3</button></li>
                  <li><button type="button" data-index="38">B3</button></li>
                  <li><button type="button" data-index="39">C4</button></li>
                  <li><button type="button" data-index="41">D4</button></li>
                  <li><button type="button" data-index="43">E4</button></li>
                  <li><button type="button" data-index="44">F4</button></li>
                  <li><button type="button" data-index="46">G4</button></li>
                  <li><button type="button" data-index="48">A4</button></li>
                  <li><button type="button" data-index="50">B4</button></li>
                  <li><button type="button" data-index="51">C5</button></li>
                  <li><button type="button" data-index="53">D5</button></li>
                  <li><button type="button" data-index="55">E5</button></li>
                  <li><button type="button" data-index="56">F5</button></li>
                  <li><button type="button" data-index="58">G5</button></li>
                  <li><button type="button" data-index="60">A5</button></li>
                  <li><button type="button" data-index="62">B5</button></li>
                  <li><button type="button" data-index="63">C6</button></li>
                  <li><button type="button" data-index="65">D6</button></li>
                  <li><button type="button" data-index="67">E6</button></li>
                  <li><button type="button" data-index="68">F6</button></li>
                  <li><button type="button" data-index="70">G6</button></li>
                  <li><button type="button" data-index="72">A6</button></li>
                  <li><button type="button" data-index="74">B6</button></li>
                  <li><button type="button" data-index="75">C7</button></li>
                  <li><button type="button" data-index="77">D7</button></li>
                  <li><button type="button" data-index="79">E7</button></li>
                  <li><button type="button" data-index="80">F7</button></li>
                  <li><button type="button" data-index="82">G7</button></li>
                  <li><button type="button" data-index="84">A7</button></li>
                  <li><button type="button" data-index="86">B7</button></li>
                  <li><button type="button" data-index="87">C7</button></li>
                </ul>
                <ul class="black-keyboards">
                  <li><button type="button" data-index="1">A0#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="4">C1#</button></li>
                  <li><button type="button" data-index="6">D1#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="9">F1#</button></li>
                  <li><button type="button" data-index="11">G1#</button></li>
                  <li><button type="button" data-index="13">A1#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="16">C2#</button></li>
                  <li><button type="button" data-index="18">D2#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="21">F2#</button></li>
                  <li><button type="button" data-index="23">G2#</button></li>
                  <li><button type="button" data-index="25">A2#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="28">C3#</button></li>
                  <li><button type="button" data-index="30">D3#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="33">F3#</button></li>
                  <li><button type="button" data-index="35">G3#</button></li>
                  <li><button type="button" data-index="37">A3#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="40">C4#</button></li>
                  <li><button type="button" data-index="42">D4#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="45">F4#</button></li>
                  <li><button type="button" data-index="47">G4#</button></li>
                  <li><button type="button" data-index="49">A4#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="52">C5#</button></li>
                  <li><button type="button" data-index="54">D5#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="57">F5#</button></li>
                  <li><button type="button" data-index="59">G5#</button></li>
                  <li><button type="button" data-index="61">A5#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="64">C6#</button></li>
                  <li><button type="button" data-index="66">D6#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="69">F6#</button></li>
                  <li><button type="button" data-index="71">G6#</button></li>
                  <li><button type="button" data-index="73">A6#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="76">C7#</button></li>
                  <li><button type="button" data-index="78">D7#</button></li>
                  <li class="hidden"></li>
                  <li><button type="button" data-index="81">F7#</button></li>
                  <li><button type="button" data-index="83">G7#</button></li>
                  <li><button type="button" data-index="85">A7#</button></li>
                </ul>
              </div>
              <dl>
                <div>
                  <dt><label for="range-glide-time">Glide Time</label></dt>
                  <dd>
                    <input type="range" id="range-glide-time" value="0" min="0" max="2" step="0.05" />
                    <span id="print-glide-time-value">0.00</span>
                  </dd>
                </div>
                <div>
                  <dt>Glide Type</dt>
                  <dd>
                    <form id="form-glide-type" class="form-glide-type">
                      <label><input type="radio" name="radio-glide-type" value="linear" checked /><span>Linear</span></label>
                      <label><input type="radio" name="radio-glide-type" value="exponential" /><span>Exponential</span></label>
                    </form>
                  </dd>
                </div>
              </dl>
            </div>
          </section>
        </section>
        <section id="section-effectors-by-audio-worklet">
          <h3>AudioWorklet が必要なエフェクター</h3>
          <p>
            ここまでのセクションで解説したエフェクターは, <code>AudioNode</code> (のサブクラス) と <code>AudioNode</code> のサブクラス特有に定義されている
            <code>AudioParam</code> のパラメータの設定, また, それらの接続を駆使する (LFO の接続やフィードバック接続など) ことで実装可能でした. 言い換えると,
            直接サウンドデータにアクセスすることなく, すなわち, <b>AudioWorklet を必要とすることなく実装可能なエフェクターです</b>.
          </p>
          <p>
            しかしながら, 現実世界に存在するエフェクターのなかには, そのアルゴリズムの複雑度に関わらず, 直接サウンドデータにアクセスしないと実装できない,
            すなわち, AudioWorklet を必要とするエフェクターもあります. 例えば, すでに解説したボーカルキャンセラは, 言ってしまえば,
            オーディオデータを減算するだけの単純な処理ですが, 現状の Web Audio API の仕様では, AudioWorklet を利用する以外に実装する手段はありません. 他にも,
            ピッチシフターやノイズサプレッサーといったエフェクターも, 現状の Web Audio API の仕様では, AudioWorklet を利用しないと実装できません
            (エフェクターの可能性を考えると, AudioWorklet を必要とするエフェクターのほうが圧倒的に多いと考えることもできます).
          </p>
          <p>
            もっとも,
            <b>Web Audio API の仕様設計の思想に従うと, <code>AudioNode</code> や <code>AudioParam</code> の組み合わせで実装できないかを検討することが大事です</b>. すでに解説した, オートワウやアンプシミュレーターなどは比較的, 複雑な実装をしていますが, AudioWorklet を必要とすることなく実装できました.
            <b>アルゴリズムの複雑性と AudioWorklet の必要性はあまり関係ありません</b> (研究目的であれば高い相関があるかもしれませんが, 研究用途の場合, そもそも
            Web Audio API を使うことが適さないと思われます (MATLAB や Python を使うのが一般的です). リアルタイム性自体が研究目的であっても,
            その場合はプラットフォーム (ハードウェアや OS) に依存するので, 研究対象のプラットフォームで研究することになるでしょう).
          </p>
          <p>
            それでも実装不可能な場合, <b>最終手段として</b> AudioWorklet で実装します (ただし, 他のオーディオ API 経験者からすると, AudioWorklet
            ですべてのオーディオ信号処理を実装してしまうほうが学習コストが低いかもしれません. 特に, ビジネスの現場は,
            教条的に思想に従うのはあまり意味がないので, そのあたりは柔軟な判断をしてください. このドキュメントでは, あくまで技術的な理想として, まずは, Web
            Audio API の仕様設計の思想に従うことを最優先しました).
          </p>
          <section id="section-effectors-by-audio-worklet-time-and-frequency-resolution">
            <h4>時間分解能と周波数分解能</h4>
            <p>
              ところで, 直接サウンドデータにアクセスしなければ実装できないオーディオ信号処理でも, <b>時間領域の演算のみで実装できる場合は</b>,
              <code>AudioWorkletProcessCallback</code> メソッドで, <code>renderQuantumSize</code> (デフォルトは <code>128</code> サンプル)
              ごとに処理を適用すれば問題ありません. 例えば, 時間領域でのボーカルキャンセラやノイズゲート, また, エフェクターではありませんが,
              ノイズ生成もこれに該当します.
            </p>
            <p>
              問題となるのは, <b>周波数領域での演算を必要とする</b>, つまり, <b>高速フーリエ変換を必要とする場合は</b>,
              <b>周波数分解能の低さ</b>が問題となります. 結論から解説すると, デフォルトの <code>renderQuantumSize</code>, つまり,
              <code>128</code> サンプルごとの処理では, 周波数分解が非常に低く, 周波数領域での演算精度が悪くなり, 結果として,
              エフェクターが想定より効いていなかったり, 場合によってはノイズとなってしまっりといった諸問題が発生します. さらに,
              <b>直接サウンドデータにアクセスしなければ実装できないオーディオ信号処理のほとんどは, 周波数領域での演算を必要とします</b>. 例えば,
              ピッチシフターやノイズサプレッサー, 周波数領域でのボーカルキャンセラなどです (Web Audio API 1.1 では, 必ずしも
              <code>128</code> サンプル固定ではないものの, 適用される <code>renderQuantumSize</code> はブラウザやプラットフォーム依存なので,
              周波数分解能の低さを根本的に解決はできません).
            </p>
            <p>
              AudioWorklet における (Web Audio API における) 周波数分解能の低さを解決する手法として, <b>オーバーラップアド</b> (<b>Overlap-Add method</b>:
              <b>重畳加算法</b>) があります. イメージとしては, 過去の数フレーム (<code>renderQuantumSize</code> のオーディオデータ) をバッファに格納しておき,
              過去のデータとつなぎ合わせて, (入出力以外の) オーディオの処理単位を (一般的には 2 の冪乗で) 任意のサイズで制御することができる手法です.
              これによって, 高速フーリエ変換のサイズも <code>renderQuantumSize</code> に依存しない, つまり,
              <code>128</code> サンプルよりも大きなサンプル数を指定できるので周波数分解能の問題を解決することができます.
            </p>
            <p>
              では, 周波数分解能の説明をします. 周波数分解能 (<span class="math-inline">$f_{\mathrm{resolution}}$</span>) は以下の数式で定義されます. 分子の
              <span class="math-inline">$f_{s}$</span> はサンプリング周波数, 分母の <span class="math-inline">$N$</span> は, 高速フーリエ変換のサイズです.
            </p>
            <div class="math-block">$f_{\mathrm{resolution}} = \frac{f_{s}}{N}$</div>
            <p>
              サンプリング周波数を一定, つまり, 定数とすると, 周波数分解能は, 高速フーリエ変換のサイズによって決まることになります. そして,
              高速フーリエ変換のサイズである <span class="math-inline">$N$</span> の値を大きくしていくと, 周波数分解能 (<span class="math-inline"
                >$f_{\mathrm{resolution}}$</span>) はより小さい値になっていきます. <b>周波数分解能の値が小さいほど, より精度の高い周波数演算ができるので周波数分解能が高くなります</b>.
            </p>
            <p>
              具体的な値で算出すると, サンプリング周波数 <span class="math-inline">$f_{s}$</span> を <code>48000</code> で定数とします.
              高速フーリエ変換のサイズを, デフォルトの <code>renderQuantumSize</code> の <code>128</code> サンプルとすると,
              <span class="math-inline">$f_{\mathrm{resolution}} = \frac{48000}{128} = 375$</span> となり, 周波数領域での演算を
              <code>375 Hz</code> 単位の粗い単位でしか実行できなくなり, 先に解説した諸問題が発生します. 一方で,
              高速フーリエ変換のサイズをオーバーラップアドによって, <code>2048</code> サンプル (過去 15 フレーム (<code>renderQuantumSize</code>) 分と現在の 1
              フレーム (<code>renderQuantumSize</code>) 分のサンプル (ただし, 厳密には, オーバーラップ, つまり, 重なるサンプルがあるので, 過去の必要なフレームは
              15 フレームより多いです. あくまで, 理解のための概算と考えてください. <code>2048</code> サンプルは,
              時間分解能とのバランスを考慮したよく利用されるサンプル数です) とすると,
              <span class="math-inline">$f_{\mathrm{resolution}} = \frac{48000}{2048} = 23.4375$</span> となり, 周波数領域での演算を約
              <code>24 Hz</code> 単位の細かい精度で実行できるので, 先に解説した諸問題は実用上発生しません (理論上は, デジタル信号である以上,
              少なからず丸め誤差によるノイズなどはありますが).
            </p>
            <p>
              ピアノ 88 鍵の音域で考えると, 高速フーリエ変換のサイズがデフォルトの <code>renderQuantumSize</code> の <code>128</code> サンプルでは, 低音域の 44
              鍵がまったく演算できないことになりますが, <code>2048</code> サンプルにすると, 最も低い <code>27.5 Hz</code> から 88
              鍵の音域をすべて演算可能できることになります. また, すでに解説したグラフィックイコライザーで考えると, 低音の帯域が <code>32 Hz</code> から
              <code>250 Hz</code> となるので, 高速フーリエ変換のサイズがデフォルトの <code>renderQuantumSize</code> の <code>128</code> サンプルでは,
              低音域をまったく演算できないことになりますが, <code>2048</code> サンプルにすると, 十分に低音域も演算できることになります.
            </p>
            <p>
              ところで, 周波数分解能の低さが問題になるのであれば, なぜ, デフォルトの
              <code>renderQuantumSize</code> をもっと大きな値に仕様策定していないのでしょうか ? 実は,
              <b>周波数分解を高くすると時間分解能が低くなるというトレードオフの関係があるからです</b>. 時間分解能は, 1 フレームあたりのサンプル数です. Web Audio
              API では, (デフォルトの実装で) <code>renderQuantumSize</code> が時間分解能です. また, 上記の周波数分解能の数式では, 高速フーリエ変換サイズである
              <span class="math-inline">$N$</span> が時間分解能です. すなわち,
              <b>単位時間あたりに処理するサンプル数が小さいほど時間分解能は高くなります</b> (高速フーリエ変換のサイズ (時間分解能である)
              <span class="math-inline">$N$</span> を小さくするほど, 周波数分解能である
              <span class="math-inline">$f_{\mathrm{resolution}}$</span> はより大きな値となります).
              <b>時間分解能が高いほど</b> (単位時間あたりに処理するサンプル数が小さいほど), CPU の負荷を減らせるので, グリッチ (glitch) や遅延 (latency)
              を軽減することができます. すなわち, <b>Web Audio API の設計では, 時間分解能を優先</b>しているということです (これは, 非推奨となった仕様である
              <code>ScriptProcessorNode</code> のフィードバックから影響を受けています. また, このことからも, Web Audio API では, AudioWorklet
              による実装を最終手段的に利用するべきという暗黙的な設計思想がうかがえます).
            </p>
            <p>
              先ほどの周波数分解能の算出の例として, <b>時間分解能とのバランスを考慮した</b>のも, トレードオフの関係があるからです.
              実用的によく利用される高速フーリエ変換のサイズは <code>512</code>, <code>1024</code>, <code>2048</code>, <code>4096</code> あたりです.
              これより小さい, もしくは大きいと, 周波数分解能が低すぎる, または, 時間分解能が低すぎてそれぞれのケースにおける諸問題を発生させてしまいます.
              参考までに, <code>AnalyserNode</code> の <code>fftSize</code> プロパティのデフォルト値も <code>2048</code> となっています.
            </p>
            <p>
              視覚的にも, 時間分解能と周波数分解能を理解してみましょう. 以下の波形 (<code>OscillatorNode</code>) は, いずれも, ノコギリ波
              (偶奇の倍音成分をもつので視覚的に理解しやすくなります) で, ピアノ 88 鍵盤の最も低い周波数である <code>27.5 Hz</code> です.
            </p>
            <p>
              デフォルトの <code>renderQuantumSize</code> である <code>128</code> サンプルの場合, 時間分解能が高く 1 フレームあたり, 約
              <code>2.5 msec</code> の高い精度で処理されています (Gibbs の現象の詳細を確認できるほどの時間分解能です). 一方で, 周波数分解能が
              <code>375 Hz</code> で, <code>27.5 Hz</code> には程遠い帯域なので, まったく処理できていません (<code>128</code>
              サンプルのスペクトルの描画はバグではなく, 周波数分解能が低すぎて描画されていません).
            </p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-time-and-frequency-high-resolution-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-time-and-frequency-low-resolution-spectrum" width="720" height="240" /></dd>
              </dl>
              <figcaption>
                <span>時間分解能と周波数分解能</span>
                <span
                  >(<span class="math-inline">$N = 128$</span>, <span class="math-inline">$f_{s} = 48000 \space \mathrm{Hz}$</span>,
                  <span class="math-inline">$f_{\mathrm{resolution}} = \frac{f_{s}}{N} = 375 \space \mathrm{Hz}$</span>)</span>
                <button type="button" id="button-time-and-frequency-resolution-128-fft-size-animation">start</button>
              </figcaption>
            </figure>
            <p>
              <code>2048</code> サンプルの場合, 周波数分解能が高く, <code>27.5 Hz</code> の帯域と倍音成分が高い精度で処理されています. 一方で,
              時間分解能が低くなるので, 1 フレームあたり, 約 <code>40 msec</code> と低い精度になっています (それにしたがって, レイテンシーも大きくなります).
            </p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-time-and-frequency-low-resolution-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-time-and-frequency-high-resolution-spectrum" width="720" height="240" /></dd>
              </dl>
              <figcaption>
                <span>時間分解能と周波数分解能</span>
                <span
                  >(<span class="math-inline">$N = 2048$</span>, <span class="math-inline">$f_{s} = 48000 \space \mathrm{Hz}$</span>,
                  <span class="math-inline">$f_{\mathrm{resolution}} = \frac{f_{s}}{N} = 23.4375 \space \mathrm{Hz}$</span>)</span>
                <button type="button" id="button-time-and-frequency-resolution-2048-fft-size-animation">start</button>
              </figcaption>
            </figure>
          </section>
          <section id="section-effectors-by-audio-worklet-overlap-add">
            <h4>オーバーラップアド</h4>
            <p>
              <b>オーバーラップアド</b> (<b>Overlap-Add method</b>: <b>重畳加算法</b>) とは, その命名のとおり,
              フレームごとに区切ったオーディオデータのサンプルを 「<b>重ねて</b>」「<b>加算</b>」することです. 特に, リアルタイム処理においては,
              過去のフレームをバッファに格納しておき, それぞれのフレームに対して, 1 つあとのフレームをスライディングして重ねて, 加算します.
            </p>
            <figure>
              <svg id="svg-figure-overlap-add-without-window-function" width="720" height="400" />
              <figcaption>オーバーラップアド</figcaption>
            </figure>
            <p>
              ところで, 周波数分解能の問題だけであれば, わざわざ「重ねて」「加算」する必要はない, すなわち,
              オーバーラップアドをわざわざ実装する必要はないように思われます. しかしながら, 周波数領域で演算する場合,
              <b
                >離散フーリエ変換後の信号は周期が <span class="math-inline">$N$</span> (離散フーリエ変換のサイズ. 実用上は, 高速フーリエ変換のサイズ)
                であることを仮定しています</b>
              (<span class="math-inline">$-\frac{f_{s}}{2}$</span> (負のナイキスト周波数) から
              <span class="math-inline">$\frac{f_{s}}{2}$</span> (ナイキスト周波数) までが 1 周期で, その間のサンプル数は
              <span class="math-inline">$N$</span> (<span class="math-inline">$0 \lt k \leq \frac{f_{s}}{2}$</span> の範囲で
              <span class="math-inline">$\frac{N}{2}$</span>) となります). したがって,
              <span class="math-inline">$N$</span> が対象の信号の周期の整数倍となっていない場合, <b>時間領域の波形に不連続点が発生してしまい</b>,
              その波形を離散フーリエ変換することで, <b>本来は存在しない周波数成分が発生してしまいます</b>. そして, 現実のオーディオデータにおいて,
              離散フーリエ変換のサイズが, 周期の整数倍になることは稀です.
            </p>
            <figure>
              <svg id="svg-figure-dft-size-and-period" width="720" height="400" />
              <figcaption>
                <span class="math-inline">$N$</span> が周期の整数倍となる場合 (上) と周期の整数倍とならない場合 (下: 時間領域の波形で不連続点が発生しています)
              </figcaption>
            </figure>
            <p>
              この問題を緩和するために, (次のセクションで解説する) <b>窓関数</b>を利用しますが, 窓関数は, ほとんどが両端で <code>0</code> となるので (あるいは,
              ハミング窓でもかなり減衰するので), フレーム両端のオーディオデータが失われてしまいます. そこで, オーバーラップアドによって,
              <b>フレーム両端のオーディオデータを平滑化して保ちながら</b>, 本来は存在しない周波数成分が発生してしまう問題も緩和することができます. 特に,
              <a href="#section-effectors-delay-and-reverb-cyclic-convolution">巡回畳み込み</a>で不可欠な処理となります.
            </p>
            <p>
              また, オーバーラップアドの計算量を改良した, <b>オーバーラップセーブ</b> (<b>Overlap-Save method</b>: <b>重畳保留法</b>)
              が利用されることもありますが, その処理の目的は同じです.
            </p>
          </section>
          <section id="section-effectors-by-audio-worklet-overlap-add-window-function">
            <h4>窓関数</h4>
            <p>
              理論上, フレームのサイズを十分に大きくしていけば, 離散フーリエ変換のサイズが周期の整数倍になります. しかしながら,
              フレームサイズを大きくしすぎると, 時間分解能が低下します. さらに, 高速フーリエ変換を適用するとなると 2 の冪乗単位でフレームを大きくする必要があり,
              離散フーリエ変換のサイズが周期の整数倍 (かつ, 2 の冪乗) になるようにフレームサイズを大きくするというのは, 現実的には難しい解決手段です.
            </p>
            <p>
              そこで, 周波数分解能の低下をある程度を許容して, 本来存在しない周波数成分 (不連続点が原因なので, <b>段差成分</b>と呼ばれることもあります)
              を除去する (緩和する) ために, <b>窓関数</b>を<b>時間領域で乗算</b> (<b>周波数領域ではコンボリューション積分</b>) します. 窓関数によって,
              フレーム両端の不連続点を除去 (緩和) して, 離散フーリエ変換のサイズが周期の整数倍になるように処理を適用します.
            </p>
            <p>
              窓関数の振幅スペクトルの形状は, <b>メインローブ</b>と<b>サイドローブ</b>に分類できます. <b>メインローブの幅が狭いほど周波数分解能が高くなり</b>,
              <b>サイドローブのピーク値が低いほど段差成分の周波数成分が発生するのを防ぐことができます</b>. 例えば, 矩形窓の場合,
              その振幅スペクトルの形状はシンク関数 (<span class="math-inline"
                >$\left|H_{rect}\left({\omega}\right)\right| = \left|\frac{\sin\left(\omega\right)}{\omega}\right|$</span>) になります.
            </p>
            <p></p>
            <figure>
              <svg id="svg-figure-window-function-spectrum" width="720" height="405" />
              <figcaption>
                窓関数の振幅スペクトルの形状 (周期を <span class="math-inline">$T$</span> とすると, 周波数分解能は
                <span class="math-inline">$\omega = \frac{2 \pi}{T}$</span> となります)
              </figcaption>
            </figure>
            <p></p>
            <p>
              矩形窓の振幅スペクトル (青色のパス) はメインローブの幅 (半透明の青) は狭くて, 周波数分解能は高くなりますが, サイドローブのピーク値
              (青色の破線のパス) が高くなってしまうので, 段差成分の周波数成分を除去することができません. そのために, ハニング窓やハミング窓など (狭義での)
              窓関数を利用します (ハニング窓の振幅スペクトルはマゼンタのパス, メインローブの幅を半透明のマゼンタ,
              サイドローブのピーク値をマゼンタの破線のパスで描画しています).
            </p>
            <table class="nowrap-table">
              <caption>
                主な窓関数
              </caption>
              <thead>
                <tr>
                  <th scope="col">Window Function</th>
                  <th scope="col">Formula <span class="math-inline">$\left(0 \leq n \leq N \right)$</span></th>
                  <th scope="col">Description</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>矩形窓</td>
                  <td><span class="math-inline">$w\left(n\right) = 1$</span></td>
                  <td>
                    広義の窓関数で, 周波数分解能が最も高い. 一方で, サイドローブのピーク値が最も高く段差成分をまったく除去できません. コンテキストによっては
                    (狭義には), 窓関数に含めていない場合もあるので注意.
                  </td>
                </tr>
                <tr>
                  <td>ハニング窓</td>
                  <td><span class="math-inline">$w\left(n\right) = 0.5 - 0.5\cos\left(\frac{2 \pi n}{N}\right)$</span></td>
                  <td>
                    オーディオ信号処理において最もよく利用される窓関数で, 周波数分解能は矩形窓より低くなりますが, サイドローブのピーク値が他の窓関数より低く,
                    両端においては段差成分を完全に除去できます. トレードオフである, 周波数分解能と段差成分の除去のバランスがとれていることが,
                    オーディオ信号処理において最もよく利用される理由です.
                  </td>
                </tr>
                <tr>
                  <td>ハミング窓</td>
                  <td><span class="math-inline">$w\left(n\right) = 0.54 - 0.46\cos\left(\frac{2 \pi n}{N}\right)$</span></td>
                  <td>
                    <b>両端に不連続点が存在するのが特徴</b>です. ハニング窓より周波数分解能は高くなりますが, サイドローブのピーク値は高くなります.
                    イメージ的には, 矩形窓とハニング窓の間のような性能の窓関数です.
                  </td>
                </tr>
                <tr>
                  <td>ブラックマン窓</td>
                  <td>
                    <span class="math-inline">
                      <!-- prettier-ignore -->
                      $
                        \begin{flalign}
                          &\alpha = 0.16 \\
                          &w\left(n\right) = \frac{1 - \alpha}{2} - \frac{1}{2}\cos\left(\frac{2 \pi n}{N}\right) + \frac{\alpha}{2}\cos\left(\frac{4 \pi n}{N}\right) \\
                          & = 0.42 - 0.5\cos\left(\frac{2 \pi n}{N}\right) + 0.08\cos\left(\frac{4 \pi n}{N}\right) \\
                        \end{flalign}
                      $
                    </span>
                  </td>
                  <td>
                    Web Audio API の
                    <a href="https://www.w3.org/TR/webaudio/#blackman-window" target="_blank" rel="noopener noreferrer"
                      ><code>AnalyserNode</code> インスタンスの <code>getByteFrequencyData</code> メソッドや
                      <code>getFloatFrequencyData</code> メソッドで利用されている窓関数</a>. ハニング窓より周波数分解能は低くなりますが, そのぶん, サイドローブのピーク値も低くなります. したがって,
                    より正確なスペクトルを描画するために, 最適な窓関数と言えます.
                  </td>
                </tr>
                <tr>
                  <td>sine 窓, Vorbis 窓</td>
                  <td>
                    <span class="math-inline"
                      >$w\left(n\right) = \sin\left(\pi n\right), \space w\left(n\right) = \sin\left(\frac{\pi}{2}\sin^{2}\left(\pi n\right)\right)$</span>
                  </td>
                  <td>
                    <abbr title="MPEG-1 Audio Layer 3">MP3</abbr> や <abbr title="Advanced Audio Coding">AAC</abbr> など, オーディオコーデック
                    (オーディオ圧縮アルゴリズム. <b>修正離散コサイン変換</b> (<b>MDCT</b>: <b>Modified Discrete Cosine Transform</b>)) で利用される窓関数です.
                  </td>
                </tr>
              </tbody>
            </table>
            <p>
              上記の表で記載した以外にも, 窓関数がいくつも定義されているのは,
              <b>メインローブの幅 (周波数分解能) とサイドローブのピーク値 (段差成分の有無) がトレードオフ</b>の関係にあるからです. したがって,
              <b>ユースケースに応じて最適な窓関数を選択する必要があります</b>.
            </p>
            <figure>
              <dl>
                <dt>Time Domain</dt>
                <dd><svg id="svg-animation-window-functions-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                <dt>Frequency Domain (Spectrum)</dt>
                <dd><svg id="svg-animation-window-functions-spectrum" width="720" height="240" /></dd>
                <dt><label for="select-window-functions">Window Function</label></dt>
                <dd>
                  <select id="select-window-functions">
                    <option value="rect" selected>Rectangular Window</option>
                    <option value="hanning">Hanning Window</option>
                    <option value="hamming">Hamming Window</option>
                    <option value="blackman">Blackman Window</option>
                  </select>
                </dd>
              </dl>
              <figcaption class="flexbox-column">
                <div>
                  <span>窓関数とスペクトル</span>
                  <button type="button" id="button-window-functions">start</button>
                </div>
                <p>矩形窓とハミング窓は両端の不連続点が残るので, 描画フレームごとに, スペクトルの端が異なった値になっています</p>
              </figcaption>
            </figure>
            <article id="section-effectors-by-audio-worklet-overlap-add-window-function-as-system">
              <h5>システム (系) としての窓関数</h5>
              <p>
                現実世界のオーディオ信号処理システムは, 十分に長い時間, 入力と出力を観測すると, 多くのシステム (系) は, <b>非線形時変システム</b> (<b
                  >Non-Linear Time-Variant system</b>) となります. 非線形, かつ, 時変システムをモデリングする (数式として表現する) のは一般的に複雑になります. しかしながら,
                非線形時変システムであっても, ごく短時間だけを観測すれば, ほとんどは, <b>線形時不変システム</b> (<b>Linear Time-Invariant system</b>)
                となりモデリングが容易となります. 窓関数は, 本来, 非線形時変システムであるオーディオの入出力を, ごく短時間で区切って,
                線形時不変システムとしてモデリングすることを可能にしています (信号の定常性やエルゴード過程なども担保します).
                周波数分解能と段差成分の除去のトレードオフの判断の必要はありますが, システムを単純にする役割も果たしています.
              </p>
              <dl>
                <dt>定常性</dt>
                <dd>確率的な信号において, その統計的性質 (期待値 (平均) や分散など) が時刻によらず一定であるような性質 (つまり, 時不変)</dd>
                <dt>エルゴード過程</dt>
                <dd>観測信号の期待値と時間平均が一致すると仮定して, 期待値を時間平均で代替可能な性質をもつ信号系列</dd>
              </dl>
              <p>
                一方で, 人工知能の発展にともなって, 機械学習, 特に, ニューラルネットワークでは, データセットを元に, 非線形時変システムのまま,
                モデリングする手法もすでに存在しています.
              </p>
            </article>
          </section>
          <section id="section-effectors-by-audio-worklet-overlap-add-audio-worklet-processor-and-window-function">
            <h4>オーバーラップアド (AudioWorkletProcessor クラスの拡張) と窓関数の実装</h4>
            <p>
              一般的な (Web Audio API に限らない), オーディオ信号処理におけるオーバーラップアドと窓関数は, 以下のようなイメージで, フレームごとに,
              オーバーラップする部分を最適な窓関数で平滑化します.
            </p>
            <figure>
              <svg id="svg-figure-overlap-add-with-window-function" width="720" height="400" />
              <figcaption>オーバーラップアドと窓関数 (矩形窓 (上) とハニング窓 (下))</figcaption>
            </figure>
            <p>
              このセクションでは,
              <a href="https://github.com/olvb/phaze" target="_blank" rel="noopener noreferrer">Phaze: a real-time web audio pitch-shifter</a> で利用されている,
              <a href="https://github.com/olvb/phaze/blob/master/src/ola-processor.js" target="_blank" rel="noopener noreferrer"><code>OLAProcessor</code></a>
              クラスの実装を参考に, 理解のために, 少し簡素化して, Web Audio API におけるオーバーラップアドの実装と, そのサブクラスの利用例
              (窓関数と高速フーリエ変換の適用) を解説します.
            </p>
            <section id="section-effectors-by-audio-worklet-overlap-add-audio-worklet-processor-and-window-function-overlap-add-processor">
              <h5>OverlapAddProcessor クラスの実装 (AudioWorkletProcessor クラスの拡張)</h5>
              <p>
                <a href="https://github.com/olvb/phaze/blob/master/src/ola-processor.js" target="_blank" rel="noopener noreferrer"><code>OLAProcessor</code></a>
                クラスの実装を参考にして, 簡素化したオーバーラップアドのための <code>AudioWorkletProcessor</code> のサブクラスの実装は以下のようになります.
                簡素化した実装は, <code>AudioWorkletNodeOptions</code> 型の <code>numberOfInputs</code> プロパティと <code>numberOfOutputs</code> プロパティは,
                <b>指定しない</b>という制約 (コーディングルールレベルの制約ですが, TypeScript であれば型レベルで制約を実装することも可能です) の代わりに,
                <code>AudioWorkletProcessCallback</code> の第 1 引数と第 2 引数に渡される, <code>FrozenArray</code> の
                <code>0</code> 番目を固定で取得することで, ループ処理を減らして, 理解しやすい実装にしています (この理由は,
                <code>AudioWorkletNodeOptions</code> 型の <code>numberOfInputs</code> プロパティと
                <code>numberOfOutputs</code> プロパティを明示的に指定してしまうと, <code>AudioWorkletProcessCallback</code> の第 1 引数と第 2
                引数の配列の構造が変わってしまうからです).
              </p>
              <p>
                この制約と簡素化したループ処理を除けば, 本質的な実装は
                <a href="https://github.com/olvb/phaze/blob/master/src/ola-processor.js" target="_blank" rel="noopener noreferrer"><code>OLAProcessor</code></a>
                と同じです.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/overlap-add.js&apos;

/**
 * This class extends `AudioWorkletProcessor`.
 */
class OverlapAddProcessor extends AudioWorkletProcessor {
  static RENDER_QUANTUM_SIZE = 128;

  constructor(options) {
    super(options);

    this.frameSize = 2048;
    this.hopSize   = 128;

    this.numberOfOverlaps = this.frameSize / this.hopSize;

    this.inputBuffers       = [[]];  /** @type {[Float32Array[]]} */
    this.inputBuffersHead   = [[]];  /** @type {[Float32Array[]]} */
    this.inputBuffersToSend = [[]];  /** @type {[Float32Array[]]} */

    this.outputBuffers           = [[]];  /** @type {[Float32Array[]]} */
    this.outputBuffersToRetrieve = [[]];  /** @type {[Float32Array[]]} */

    if (options.processorOptions) {
      this.frameSize = options.processorOptions.frameSize ?? 2048;
    }

    this.allocateInputChannels(1);
    this.allocateOutputChannels(1);
  }

  /**
   * @param {[Float32Array[]]} inputs
   * @param {[Float32Array[]]} outputs
   * @param {{ [parameterName: string]: Float32Array }} parameters
   * @abstract
   */
  processOverlapAdd(inputs, outputs, parameters) {}

  /**
   * @param {[Float32Array[]]} inputs
   * @param {[Float32Array[]]} outputs
   * @param {{ [parameterName: string]: Float32Array }} parameters
   * @override
   */
  process(inputs, outputs, parameters) {
    this.reallocateChannelsIfNeeded(inputs, outputs);

    this.readInputs(inputs);
    this.shiftInputBuffers();
    this.prepareInputBuffersToSend();
    this.processOverlapAdd(this.inputBuffersToSend, this.outputBuffersToRetrieve, parameters);
    this.handleOutputBuffersToRetrieve();
    this.writeOutputs(outputs);
    this.shiftOutputBuffers();

    return true;
  }

  reallocateChannelsIfNeeded(inputs, outputs) {
    const inputNumberOfChannels = inputs[0].length;

    if (inputNumberOfChannels !== this.inputBuffers[0].length) {
      this.allocateInputChannels(inputNumberOfChannels);
    }

    const outputNumberOfChannels = outputs[0].length;

    if (outputNumberOfChannels !== this.outputBuffers[0].length) {
      this.allocateOutputChannels(outputNumberOfChannels);
    }
  }

  allocateInputChannels(numberOfChannels) {
    this.inputBuffers = [[]];

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      this.inputBuffers[0][channelNumber] = new Float32Array(this.frameSize + OverlapAddProcessor.RENDER_QUANTUM_SIZE);
    }

    this.inputBuffersHead   = [[]];
    this.inputBuffersToSend = [[]];

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      this.inputBuffersHead[0][channelNumber]   = this.inputBuffers[0][channelNumber].subarray(0, this.frameSize);
      this.inputBuffersToSend[0][channelNumber] = new Float32Array(this.frameSize);
    }
  }

  allocateOutputChannels(numberOfChannels) {
    this.outputBuffers = [[]];

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      this.outputBuffers[0][channelNumber] = new Float32Array(this.frameSize);
    }

    this.outputBuffersToRetrieve = [[]];

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      this.outputBuffersToRetrieve[0][channelNumber] = new Float32Array(this.frameSize);
    }
  }

  readInputs(inputs) {
    if (inputs[0].length &amp;&amp; (inputs[0][0].length === 0)) {
      for (let channelNumber = 0; channelNumber &lt; this.inputBuffers[0].length; channelNumber++) {
        this.inputBuffers[0][channelNumber].fill(0, this.frameSize);
      }

      return;
    }

    for (let channelNumber = 0; channelNumber &lt; this.inputBuffers[0].length; channelNumber++) {
      this.inputBuffers[0][channelNumber].set(inputs[0][channelNumber], this.frameSize);
    }
  }

  writeOutputs(outputs) {
    for (let channelNumber = 0; channelNumber &lt; this.inputBuffers[0].length; channelNumber++) {
      outputs[0][channelNumber].set(this.outputBuffers[0][channelNumber].subarray(0, OverlapAddProcessor.RENDER_QUANTUM_SIZE));
    }
  }

  shiftInputBuffers() {
    for (let channelNumber = 0; channelNumber &lt; this.inputBuffers[0].length; channelNumber++) {
      this.inputBuffers[0][channelNumber].copyWithin(0, OverlapAddProcessor.RENDER_QUANTUM_SIZE);
    }
  }

  shiftOutputBuffers() {
    for (let channelNumber = 0; channelNumber &lt; this.outputBuffers[0].length; channelNumber++) {
      this.outputBuffers[0][channelNumber].copyWithin(0, OverlapAddProcessor.RENDER_QUANTUM_SIZE);
      this.outputBuffers[0][channelNumber].subarray(this.frameSize - OverlapAddProcessor.RENDER_QUANTUM_SIZE).fill(0);
    }
  }

  prepareInputBuffersToSend() {
    for (let channelNumber = 0; channelNumber &lt; this.inputBuffers[0].length; channelNumber++) {
      this.inputBuffersToSend[0][channelNumber].set(this.inputBuffersHead[0][channelNumber]);
    }
  }

  handleOutputBuffersToRetrieve() {
    for (let channelNumber = 0; channelNumber &lt; this.outputBuffers[0].length; channelNumber++) {
      for (let n = 0; n &lt; this.frameSize; n++) {
        this.outputBuffers[0][channelNumber][n] += this.outputBuffersToRetrieve[0][channelNumber][n] / this.numberOfOverlaps;
      }
    }
  }
}</code></pre>
              <p>
                実装を理解するには, <code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>)
                が呼び出しているメソッドをそれぞれ理解するのがよいでしょう. <code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>)
                が必要なメソッドを順に呼び出すようになっています (GoF の 23 のデザインパターンにあてはめると, Adapter (オーバーラップアドの抽象化) と Facade
                (必要なメソッドの呼び出しのみの責務) のような役割を果たします).
              </p>
              <p>
                <code>OverlapAddProcessor</code> クラスにおいて, 入力のための 3 つの多次元配列と, 出力のための 2 つの多次元配列は独立しています.
                入力と出力をつなげるのは <code>processOverlapAdd</code> メソッドで, これはサブクラスの責務です.
              </p>
              <p>
                入力処理は, <code>128</code> サンプルを <code>frameSize</code> サンプルのオーディオデータを格納する
                <code>Float32Array</code> のバッファに格納するだけです. そのとき, サンプル数が不足していれば <code>0</code> 埋めして,
                サンプル数が超過していれば, 最も古い <code>128</code> サンプルのデータを破棄します (つまり, オーバーラップアド処理のためのプリプロセス処理です).
              </p>
              <dl>
                <dt><code>readInputs</code> メソッド</dt>
                <dd>
                  実際の入力オーディオデータ (<code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>) の第 1 引数) を読み込んで,
                  <code>inputBuffers</code> の末尾に追加するメソッドです.
                </dd>
                <dt><code>shiftInputBuffers</code> メソッド</dt>
                <dd>
                  <code>inputBuffers</code> に格納されているオーディオデータすべてを <code>128</code> サンプル先頭にシフトします. この時点で, 最も古い
                  <code>128</code> サンプルのオーディオデータは破棄され, 直近の <code>readInputs</code> メソッドで読み込まれた
                  <code>128</code> サンプルのオーディオデータが, <code>inputBuffersHead</code> の参照する末尾の <code>128</code> サンプルにシフトします.
                </dd>
                <dt><code>prepareInputBuffersToSend</code> メソッド</dt>
                <dd>
                  <code>inputBuffersHead</code> が参照する, <code>inputBuffers</code> の先頭から <code>frameSize</code> サンプル数のオーディオデータを
                  <code>inputBuffersToSend</code> に格納して, <code>processOverlapAdd</code> メソッドで処理できるようにします.
                </dd>
              </dl>
              <p>出力処理に関するメソッドは以下の 3 つで, <code>handleOutputBuffersToRetrieve</code> メソッドがオーバーラップアド処理を適用しています.</p>
              <dl>
                <dt><code>handleOutputBuffersToRetrieve</code> メソッド</dt>
                <dd>
                  <code>processOverlapAdd</code> メソッドで, <code>outputBuffersToRetrieve</code> に格納された
                  <code>frameSize</code> サンプル数のオーディオデータを, 前の実行時までのオーバーラップアドを適用した, 出力オーディオデータを格納している
                  <code>outputBuffers</code> に加算します. この加算は, オーバーラップ数 (<code>numberOfOverlaps</code>) だけ実行されるので,
                  その平均を取るように, オーバーラップ数で除算します.
                </dd>
                <dt><code>writeOutputs</code> メソッド</dt>
                <dd>
                  実際の出力オーディオデータ (<code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>) の第 2 引数) を読み込んで,
                  <code>outputBuffers</code> の先頭から <code>128</code> サンプルを参照して出力します.
                </dd>
                <dt><code>shiftOutputBuffers</code> メソッド</dt>
                <dd>
                  <code>outputBuffers</code> に格納されているオーディオデータすべてを <code>128</code> サンプル先頭にシフトします. この時点で, 直近の
                  <code>writeOutputs</code> メソッドで出力した <code>128</code> サンプルのオーディオデータは破棄されます. また, シフトした結果,
                  <code>outputBuffers</code> の末尾の <code>128</code> サンプルは <code>0</code> 埋めしておきます.
                </dd>
              </dl>
              <p>
                説明の順序が逆転的ですが, コンストラクタでは, 各プロパティの初期化と, チャンネルごとの
                <code>Float32Array</code> を格納する多次元配列の初期化メソッドを呼び出します.
              </p>
              <dl>
                <dt><code>allocateInputChannels</code> メソッド</dt>
                <dd>コンストラクタ呼び出し時と, <code>reallocateChannelsIfNeeded</code> メソッドから呼び出されます.</dd>
                <dt><code>allocateOutputChannels</code> メソッド</dt>
                <dd>コンストラクタ呼び出し時と, <code>reallocateChannelsIfNeeded</code> メソッドから呼び出されます.</dd>
              </dl>
              <p>
                また, <code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>) 実行ごとに, チャンネル数がリセットされたり,
                変更されたりしていれば, 同様に配列の初期化を実行します (この処理は, <code>AudioWorkletProcessor</code> の仕様上必要な処理で,
                オーバーラップアドの本質的な処理ではないので, 説明を最後にしました).
              </p>
              <dl>
                <dt><code>reallocateChannelsIfNeeded</code> メソッド</dt>
                <dd>
                  <code>AudioWorkletProcessor</code> の仕様上, オーディオデータがない場合などは, チャンネル数が <code>0</code> になって,
                  <code>FrozenArray</code> の構造が変わるので, <code>process</code> メソッドの最初で, 必要があれば (チャンネル数が異なっていれば),
                  各チャンネルを <code>Float32Array</code> で初期化します (<code>allocateInputChannels</code> メソッドと<code>allocateOutputChannels</code>
                  メソッドを呼び出します)
                </dd>
              </dl>
              <p>上記の <code>OverlapAddProcessor</code> クラスによるオーバーラップアドは以下のようなイメージになります.</p>
              <figure>
                <svg id="svg-figure-overlap-add-by-overlap-add-processor" width="1080" height="300" />
                <figcaption><code>OverlapAddProcessor</code> クラスによるオーバーラップアド</figcaption>
              </figure>
              <p>
                <code>frameSize</code> を <code>1024</code> サンプル (8 フレーム) とした場合のイメージです. もし詳細が理解しにくければ,
                <code>128</code> サンプル末尾に enqueue (入力) しつつ, <code>128</code> サンプルを dequeue (出力) するリングバッファのようなデータ構造をもち,
                オーバーラップするオーディオデータは加算してその平均をとるという理解で十分です. また, オーディオデータが不足している場合,
                <code>0</code> 埋めされたオーディオデータを加算するので, 加算回数はどのフレームも同じになります (上記のイラストだと,
                前半のフレームは薄くなっていますが, 実際には <code>0</code> 埋めされたフレームが加算されてその平均が出力されています).
              </p>
            </section>
            <section id="section-effectors-by-audio-worklet-overlap-add-audio-worklet-processor-and-window-function-extends-overlap-add-processor">
              <h5>OverlapAddProcessor サブクラスの実装 (OverlapAddProcessor クラスの拡張)</h5>
              <p>
                <code>OverlapAddProcessor</code> クラスは直接 <code>registerProcessor</code> メソッドに指定するのではなく,
                <code>OverlapAddProcessor</code> クラスを拡張 (継承) したサブクラスを指定します. そして, このサブクラスで
                <code>processOverlapAdd</code> メソッドをオーバーライドします (したがって, TypeScript を使う場合は,
                <code>processOverlapAdd</code> メソッドを抽象メソッドにして, <code>OverlapAddProcessor</code> クラスを抽象クラスにしておくと言語処理系レベルで,
                この仕様を矯正できます).
              </p>
              <p>
                具体的な解説として, 実用性はありませんが, <code>processOverlapAdd</code> メソッドで窓関数 (ハニング窓) を適用して, FFT したあと,
                周波数領域では何もせず (Bypass), IFFT して, 再度, 時間領域で窓関数を適用して出力するだけの <code>OverlapAddProcessor</code> クラスの拡張である
                <code>BypassOverlapAddProcessor</code> クラスの実装を記載します.
              </p>
              <p>
                <code>processOverlapAdd</code> メソッドで呼び出している, <code>FFT</code> / <code>IFFT</code> 関数は,
                <a href="#section-fast-fourier-transform-code">高速フーリエ変換の実装</a>セクションで記載している実装を
                <code>AudioWorkletGlobalScope</code> 内で定義していると仮定してください.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/bypass-overlap-add.js&apos;

/**
 * This class extends `OverlapAddProcessor`.
 */
class BypassOverlapAddProcessor extends OverlapAddProcessor {
  static createHanningWindow(size) {
    const w = new Float32Array(size);

    for (let n = 0; n &lt; size; n++) {
      w[n] = 0.5 - 0.5 * Math.cos((2 * Math.PI * n) / (size - 1));
    }

    return w;
  }

  constructor(options) {
    super(options);

    this.hanningWindow = BypassOverlapAddProcessor.createHanningWindow(this.frameSize);
  }

  /** @overdrive */
  processOverlapAdd(inputs, outputs, parameters) {
    const input  = inputs[0];
    const output = outputs[0];

    const numberOfChannels = input.length;

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const reals = new Float32Array(this.frameSize);
      const imags = new Float32Array(this.frameSize);

      for (let n = 0; n &lt; this.frameSize; n++) {
        reals[n] = this.hanningWindow[n] * input[channelNumber][n];
      }

      FFT(reals, imags, this.frameSize);

      // Bypass

      IFFT(reals, imags, this.frameSize);

      for (let n = 0; n &lt; this.frameSize; n++) {
        output[channelNumber][n] = this.hanningWindow[n] * reals[n];
      }
    }
  }
}

registerProcessor(&apos;BypassOverlapAddProcessor&apos;, BypassOverlapAddProcessor);</code></pre>
              <p>
                実装としては, <code>OverlapAddProcessor</code> クラスを <code>extends</code> するのと,
                <code>processOverlapAdd</code> メソッドをオーバーライドするだけで, 本質的なオーディオ処理の実装は, <code>AudioWorkletProcessor</code> クラスの
                <code>process</code> メソッド (<code>AudioWorkletProcessCallback</code>) と同様に実装できます. 引数に渡されるチャンネルごとの
                <code>Float32Array</code> のサイズが, <code>frameSize</code> となっていますが, 実装上, その違い意識することはあまりないと思います.
              </p>
              <p>
                窓関数 (ハニング窓) を適用した, オーバーラップアドは, 以下ようなイメージになります. 1 フレームごとではなく, 指定したフレーム単位で適用されます
                (以下のイラストは 8 フレーム (<code>1024</code> サンプル) 単位).
              </p>
              <figure>
                <svg id="svg-figure-overlap-add-by-overlap-add-processor-with-window-function" width="1080" height="300" />
                <figcaption><code>OverlapAddProcessor</code> サブクラスによるオーバーラップアドと窓関数</figcaption>
              </figure>
              <p>
                あとは, メインスレッドからフレームサイズ (<code>frameSize</code>) を指定します. <code>AudioWorkletNode</code> インスタンスの第 3 引数は,
                <b><code>AudioWorkletNodeOptions</code></b> 型のオブジェクトを指定できるようになっているので, その
                <b><code>processorOptions</code></b> プロパティに, <code>frameSize</code> プロパティとしてフレームサイズを指定します (仕様では,
                <code>processorOptions</code> プロパティは, プレインオブジェクトなので, 他にもメインスレッドから渡したい値があれば,
                <code>AudioWorkletProcessor</code> クラスに渡すことができます).
              </p>
              <p>
                また, すでに解説しましたが, <code>OverlapAddProcessor</code> クラスを使う場合には, <code>AudioWorkletNodeOptions</code> 型の
                <code>numberOfInputs</code> プロパティと <code>numberOfOutputs</code> プロパティは指定しないように制約をつけてください.
              </p>
              <p>デフォルトのフレームサイズを利用するのであれば, <code>AudioWorkletNode</code> インスタンスの第 3 引数での指定は不要です.</p>
              <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">/**
 * This class extends `AudioWorkletProcessor`.
 */
class OverlapAddProcessor extends AudioWorkletProcessor {
  static RENDER_QUANTUM_SIZE = 128;

  constructor(options) {
    super(options);

    // ...

    if (options.processorOptions) {
      this.frameSize = options.processorOptions.frameSize ?? 2048;
    }

    // ...
  }

  // ...
}</code></pre>
              <p>
                そして, <code>OverlapAddProcessor</code> コンストラクタで, <code>processorOptions</code> プロパティが truthy な値であれば,
                <code>frameSize</code> プロパティが設定されているかを判定して, 設定されていれば, そのフレームサイズで
                <code>frameSize</code> プロパティをオーバーライドします.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-html line-numbers">&lt;button type=&quot;button&quot; id=&quot;button-bypass-overlap-add-processor&quot;&gt;start&lt;/button&gt;</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

// &apos;./audio-worklets/bypass-overlap-add.js&apos; is URL that has subclass that extends `OverlapAddProcessor`
context.audioWorklet.addModule(&apos;./audio-worklets/bypass-overlap-add.js&apos;)
  .then(() =&gt; {
    let oscillator = null;
    let processor  = null;

    const onDown = () =&gt; {
      if ((oscillator !== null) || (processor !== null)) {
        return;
      }

      oscillator = new OscillatorNode(context);

      processor = new AudioWorkletNode(context, &apos;BypassOverlapAddProcessor&apos;, {
        processorOptions: {
          frameSize: 1024
        }
      });

      // OscillatorNode (Input) -&gt; AudioWorkletNode (Bypass) -&gt; AudioDestinationNode (Output)
      oscillator.connect(processor);
      processor.connect(context.destination);

      oscillator.start(0);

      buttonElement.textContent = &apos;stop&apos;
    };

    const onUp = () =&gt; {
      if ((oscillator === null) || (processor === null)) {
        return;
      }

      oscillator.stop(0);

      processor.disconnect(context.destination);

      oscillator = null;
      processor  = null;

      buttonElement.textContent = &apos;start&apos;
    };

    const buttonElement = document.getElementById(&apos;button-bypass-overlap-add-processor&apos;);

    buttonElement.addEventListener(&apos;mousedown&apos;, onDown);
    buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
    buttonElement.addEventListener(&apos;mouseup&apos;, onUp);
    buttonElement.addEventListener(&apos;touchend&apos;, onUp);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
            </section>
          </section>
          <section id="section-effectors-by-audio-worklet-noise-suppressor">
            <h4>ノイズサプレッサー</h4>
            <section id="section-effectors-by-audio-worklet-noise-suppressor-noise-gate">
              <h5>ノイズゲート</h5>
              <p>
                ノイズを除去するためのエフェクターとしては, 単純なエフェクターだと<b>ノイズゲート</b>があります.
                ノイズゲートは時間領域のみの演算で実装可能ですが, 一定レベルの振幅以下の信号を無音 (振幅が <code>0</code>) にするので, ノイズでなくても,
                信号の振幅が指定したレベル以下であれば, 除去されてしまいます. また, 目的の信号と重なった (目的の信号に加算された)
                ノイズを除去することができません. (ノイズゲートは, 単純なアルゴリズムでありますが, 現状の Web Audio API の仕様では, AudioWorklet
                を利用しないと実装できません. 以下に, ノイズゲートの <code>AudioWorkletProcessor</code> のサブクラスの実装例を記載します).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/noise-gate.js&apos;

class NoiseGateProcessor extends AudioWorkletProcessor {
  constructor(options) {
    super(options);

    this.level = 0;

    this.port.onmessage = (event) =&gt; {
      if ((event.data.level &gt;= 0) &amp;&amp; (event.data.level &lt;= 1))  {
        this.level = event.data.level;
      }
    };
  }

  process(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    const numberOfChannels = input.length;

    for (let channelNumber = 0; channelNumber &lt; input.length; channelNumber++) {
      const bufferSize = input[channelNumber].length;

      for (let n = 0; n &lt; bufferSize; n++) {
        output[channelNumber][n] = (Math.abs(input[channelNumber][n]) &gt; this.level) ? input[channelNumber][n] : 0;
      }
    }

    return true;
  }
}

registerProcessor(&apos;NoiseGateProcessor&apos;, NoiseGateProcessor);</code></pre>
            </section>
            <section id="section-effectors-by-audio-worklet-noise-suppressor-by-spectrum-subtraction">
              <h5>スペクトルサブトラクション法によるノイズサプレッサー</h5>
              <p>
                <b>ノイズサプレッサー</b>では, ノイズのスペクトルの特性に着目して, 振幅スペクトルで演算を実行して, <b>ノイズを目的に除去します</b>. そして,
                よく利用されるノイズサプレッサーのアルゴリズムとして, <b>スペクトルサブトラクション法</b>があります. スペクトルサブトラクション法では,
                ノイズをホワイトノイズ (白色雑音) としてモデリングして, その振幅スペクトル (あるいは, その 2 乗のパワースペクトル) が,
                <b>すべての周波数成分を一様に含んでいる</b> (<b>すべての周波数成分が同じパワーとなる</b>) ことを利用して, 振幅スペクトルからノイズの振幅の差分
                (サブトラクション: subtraction) を, ノイズ除去後の信号の振幅スペクトルとするアルゴリズムです. また, 周波数領域での演算において,
                <b>人間の聴覚は位相スペクトルの違いに鈍感</b>という特性も利用して, 位相スペクトルに対しては原音そのままに,
                時間領域の信号に戻すときに利用するだけです.
              </p>
              <p>
                これを形式的に, 数式で表現すると以下のようになります.
                <span class="math-inline">$w\left(n\right), x\left(n\right), y\left(n\right)$</span> をそれぞれ, ノイズ信号, 入力信号, そして,
                ノイズを含んだ出力信号とします.
              </p>
              <div class="math-block">$y\left(n\right) = x\left(n\right) + w\left(n\right)$</div>
              <p>
                これらの信号を, FFT (<span class="math-inline">$F$</span>) した複素関数をそれぞれ,
                <span class="math-inline">$W\left(k\right), X\left(k\right), Y\left(k\right)$</span> とします.
              </p>
              <div class="math-block">
                $F\left[w\left(n\right)\right] = W\left(k\right), \quad F\left[x\left(n\right)\right] = X\left(k\right), \quad F\left[y\left(n\right)\right] =
                Y\left(k\right)$
              </div>
              <p>よって, 振幅スペクトル (絶対値) と位相スペクトル (偏角) の信号は以下のように表現できます.</p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{align}
                    \begin{cases}
                      &\left|Y\left(k\right)\right| = \left|X\left(k\right)\right| + \left|W\left(k\right)\right| \\
                      &\tan^{-1}\left(\frac{\mathrm{imag}\left(Y\left(k\right)\right)}{\mathrm{real}\left(Y\left(k\right)\right)}\right) = \tan^{-1}\left(\frac{\mathrm{imag}\left(X\left(k\right)\right)}{\mathrm{real}\left(X\left(k\right)\right)}\right) + \tan^{-1}\left(\frac{\mathrm{imag}\left(W\left(k\right)\right)}{\mathrm{real}\left(W\left(k\right)\right)}\right) \\
                    \end{cases}
                  \end{align}
                $
              </div>
              <p>
                ここで, 極系式でこの振幅スペクトル (絶対値) の信号 (関数) と位相スペクトル (偏角) の信号 (関数) を 1 つにまとめて表現すると,
                以下のように表現できます (<span class="math-inline">$j$</span> は <span class="math-inline">$j^{2} = -1$</span> となる虚数単位,
                <span class="math-inline">$A_{W|X|Y}\left(k\right)$</span> は, 各信号の振幅スペクトの関数,
                <span class="math-inline">$\theta_{W|X|Y}\left(k\right)$</span> は, 各信号のスペクトの関数を表しています.
              </p>
              <div class="math-block">
                $A_{Y}\left(k\right)e^{j\theta_{Y}\left(k\right)} = A_{X}\left(k\right)e^{j\theta_{X}\left(k\right)} +
                A_{W}\left(k\right)e^{j\theta_{W}\left(k\right)}$
              </div>
              <p>したがって, ノイズを除去した, 元の入力信号 (<span class="math-inline">$X\left(k\right)$</span>) のスペクトルは以下のように表現できます.</p>
              <div class="math-block">
                $X\left(k\right) = A_{Y}\left(k\right)e^{j\theta_{Y}\left(k\right)} - A_{W}\left(k\right)e^{j\theta_{W}\left(k\right)}$
              </div>
              <p>
                厳密な数式の定義に基づくと, 位相スペクトルも演算対象に含める必要がありますが, ここで, 人間の聴覚特性を利用して位相スペクトルの演算は無視すると,
                以下のような, 振幅スペクトルのみの差分演算となります (差分結果が, 負数となる場合は, 無音 (振幅スペクトルが <code>0</code>) とします.
              </p>
              <div class="math-block">
                <!-- prettier-ignore -->
                $
                  \begin{align}
                    A_{X}\left(k\right) =
                    \begin{cases}
                      &A_{Y}\left(k\right) - A_{W}\left(k\right)  &\left(A_{Y}\left(k\right) - A_{W}\left(k\right) \ge 0 \right) \\
                      &0                                          &\left(A_{Y}\left(k\right) - A_{W}\left(k\right) \lt 0 \right) \\
                    \end{cases}
                  \end{align}
                $
              </div>
              <p>
                あとは, オイラーの公式に従って, 絶対値と偏角をもとに, 実部の関数と虚部の関数に分解して, IFFT すれば, ノイズを除去した信号を得ることができます.
              </p>
              <div class="math-block">
                $x_{\mathrm{subtraction}}\left(n\right) = F^{-1}\left[X\left(k\right)\right] =
                F^{-1}\left[A_{X}\left(k\right)e^{j\theta_{X}\left(k\right)}\right] = F^{-1}\left[A_{X}\left(k\right)\cos\left(\theta_{X}\left(k\right)\right) +
                jA_{X}\left(k\right)\sin\left(\theta_{X}\left(k\right)\right)\right]$
              </div>
              <p>
                これを実装した, <code>OverlapAddProcessor</code> クラスを拡張した <code>NoiseSuppressorProcessor</code> サブクラスの実装です
                (周波数分解能をデフォルトより高くするために, <code>OverlapAddProcessor</code> クラスを拡張します). また,
                <code>processOverlapAdd</code> メソッドで呼び出している, <code>FFT</code> / <code>IFFT</code> 関数は,
                <a href="#section-fast-fourier-transform-code">高速フーリエ変換の実装</a>セクションで記載している実装を
                <code>AudioWorkletGlobalScope</code> 内で定義していると仮定してください.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/noise-suppressor.js&apos;

class NoiseSuppressorProcessor extends OverlapAddProcessor {
  static createHanningWindow(size) {
    const w = new Float32Array(size);

    for (let n = 0; n &lt; size; n++) {
      w[n] = 0.5 - 0.5 * Math.cos((2 * Math.PI * n) / (size - 1));
    }

    return w;
  }

  constructor(options) {
    super(options);

    this.threshold = 0;

    this.hanningWindow = NoiseSuppressorProcessor.createHanningWindow(this.frameSize);

    this.port.onmessage = (event) =&gt; {
      if ((event.data.threshold &gt;= 0) &amp;&amp; (event.data.threshold &lt;= 1))  {
        this.threshold = event.data.threshold;
      }
    };
  }

  /** @overdrive */
  processOverlapAdd(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    const numberOfChannels = input.length;

    const fftSize = this.frameSize;

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const reals = new Float32Array(fftSize);
      const imags = new Float32Array(fftSize);

      for (let n = 0; n &lt; fftSize; n++) {
        reals[n] = this.hanningWindow[n] * input[channelNumber][n];
      }

      FFT(reals, imags, fftSize);

      const amplitudes = new Float32Array(fftSize);
      const phases     = new Float32Array(fftSize);

      for (let k = 0; k &lt; fftSize; k++) {
        amplitudes[k] = Math.sqrt((reals[k] ** 2) + (imags[k] ** 2));

        if ((reals[k] !== 0) &amp;&amp; (imags[k] !== 0)) {
          phases[k] = Math.atan2(imags[k], reals[k]);
        }
      }

      for (let k = 0; k &lt; fftSize; k++) {
        amplitudes[k] -= this.threshold;

        if (amplitudes[k] &lt; 0) {
          amplitudes[k] = 0;
        }
      }

      // Euler&apos;s formula
      for (let k = 0; k &lt; fftSize; k++) {
        reals[k] = amplitudes[k] * Math.cos(phases[k]);
        imags[k] = amplitudes[k] * Math.sin(phases[k]);
      }

      IFFT(reals, imags, fftSize);

      for (let n = 0; n &lt; fftSize; n++) {
        output[channelNumber][n] = this.hanningWindow[n] * reals[n];
      }
    }
  }
}

registerProcessor(&apos;NoiseSuppressorProcessor&apos;, NoiseSuppressorProcessor);</code></pre>
              <p>
                ちなみに, <code>BypassOverlapAddProcessor</code> クラスでも, 同様の処理をしていますが, 窓関数の適用は, FFT の前だけでなく, IFFT
                した後の時間領域の信号にも乗算します.
              </p>
              <p>最後に, メインスレッドの実装例です.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-html line-numbers">&lt;button type=&quot;button&quot; id=&quot;button-noise-suppressor&quot;&gt;start&lt;/button&gt;
&lt;label for=&quot;range-noise-suppressor-threshold&quot;&gt;threshold&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-noise-suppressor-threshold&quot; value=&quot;0&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-noise-suppressor-threshold-value&quot;&gt;0.00&lt;/span&gt;</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.getElementById(&apos;button-noise-suppressor&apos;);
const rangeElement  = document.getElementById(&apos;range-noise-suppressor-threshold&apos;);
const spanElement   = document.getElementById(&apos;print-noise-suppressor-threshold-value&apos;);

let processor   = null;
let mediaStream = null;

buttonElement.addEventListener(&apos;click&apos;, async () =&gt; {
  buttonElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);

  if (processor === null) {
    // &apos;./audio-worklets/noise-suppressor.js&apos; is URL that has subclass that extends `OverlapAddProcessor`
    await context.audioWorklet.addModule(&apos;./audio-worklets/noise-suppressor.js&apos;)
  }

  if (mediaStream) {
    const audioTracks = mediaStream.getAudioTracks();

    for (const audioTrack of audioTracks) {
      audioTrack.stop();
    }

    mediaStream = null;

    buttonElement.textContent = &apos;start&apos;;
    buttonElement.removeAttribute(&apos;disabled&apos;);
    return;
  }

  if (mediaStream === null) {
    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  }

  const source = new MediaStreamAudioSourceNode(context, { mediaStream });

  processor = new AudioWorkletNode(context, &apos;NoiseSuppressorProcessor&apos;, {
    processorOptions: {
      frameSize: 512
    }
  });

  // MediaStreamAudioSourceNode (Input) -&gt; AudioWorkletNode (Noise Suppressor) -&gt; AudioDestinationNode (Output)
  source.connect(processor);
  processor.connect(context.destination);

  buttonElement.textContent = &apos;stop&apos;;
  buttonElement.removeAttribute(&apos;disabled&apos;);
});

rangeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const threshold = event.currentTarget.valueAsNumber;

  if (processor) {
    processor.port.postMessage({ threshold });
  }

  spanElement.textContent = threshold.toFixed(2);
});</code></pre>
              <div class="app-container app-noise-suppressor">
                <div class="app-headline">
                  <button type="button" id="button-noise-suppressor">start</button>
                </div>
                <div>
                  <dl>
                    <div>
                      <dt><label for="range-noise-suppressor-threshold">threshold</label></dt>
                      <dd>
                        <input type="range" id="range-noise-suppressor-threshold" value="0" min="0" max="1" step="0.05" />
                        <span id="print-noise-suppressor-threshold-value">0.00</span>
                      </dd>
                    </div>
                  </dl>
                </div>
              </div>
              <article id="section-effectors-by-audio-worklet-noise-suppressor-white-noise-and-impulse-spectrum">
                <h6>ホワイトノイズとインパルス音のスペクトル</h6>
                <p>
                  ホワイトノイズの振幅スペクトルはすべての周波数成分を一様に含んでいる説明しましたが, 実は,
                  インパルス音の振幅スペクトルもすべての周波数成分を一様に含んでいます. これは, インパルス音のモデリング関数である, デルタ関数のフーリエ変換が
                  <code>1</code> と定数になることから導出できます. したがって, インパルス音の位相スペクトルも <code>1</code> と定数であり,
                  すべての周波数成分を一様に含んでいます (<a href="#section-effectors-delay-and-reverb-cyclic-convolution">巡回畳み込み</a>セクションを参照).
                  一方, ホワイトノイズの位相スペクトルは, すべての周波数成分を含んでいますが, 一様ではなくランダムな値 (位相) となります.
                  人間の聴覚は位相スペクトルの違いに鈍感ではありますが, ホワイトノイズとインパルス音の違いは, 時間領域の波形と位相スペクトルにあらわれており,
                  人間の聴覚の不思議さとまだまだ明かされていない知覚能力があるのではないかと思います.
                </p>
                <figure>
                  <dl>
                    <dt>Time Domain</dt>
                    <dd><svg id="svg-animation-white-noise-time" width="720" height="240" data-parameters="true" data-a="1" /></dd>
                    <dt>Amplitude Spectrum</dt>
                    <dd><svg id="svg-animation-white-noise-amplitude-spectrum" width="720" height="240" /></dd>
                    <dt>Phase Spectrum</dt>
                    <dd><svg id="svg-animation-white-noise-phase-spectrum" width="720" height="240" /></dd>
                  </dl>
                  <figcaption>
                    <span>ホワイトノイズの振幅スペクトルと位相スペクトル</span>
                    <button type="button" id="button-white-noise-spectrums">start</button>
                  </figcaption>
                </figure>
              </article>
            </section>
          </section>
          <section id="section-effectors-by-audio-worklet-pitch-shifter">
            <h4>ピッチシフター</h4>
            <p>
              <b>ピッチシフター</b>とは, <b>再生速度 (音長) を変化させずに, 周波数を変化させる</b>エフェクターです (結果的に,
              ピッチを変化させるエフェクターとなります). オーディオ処理を適用せずに, 再生速度を変化させると周波数も変化しますが, これは物理的な関係があり,
              <b>再生速度と周波数は比例関係にあるからです</b>. 実際, <code>AudioBufferSourceNode</code> の <code>playbackRate</code> プロパティで再生速度を
              <code>2</code> 倍にするとピッチも <code>2</code> 倍に, 逆に, 再生速度を <code>0.5</code> 倍にするとピッチも <code>0.5</code> 倍になって聴こえます.
              一般的に, ピッチシフターは, <b>再生速度を変化させずに</b>, 周波数のみを変化させるエフェクターを意味します.
            </p>
            <p>
              ピッチシフターは, <b>原音とオクターブ違いの音を合成するオクターバー</b>, <b>ピッチベンド</b> (エレキギターでは,
              <b>ペダルでピッチを変化させるワーミー (Whammy)</b> と呼ばれることが多いです.
              <a href="https://digitech.com/dp/whammy/" target="_blank" rel="noopener noreferrer">DigiTech の Whammy</a>),
              <b>原音と 3 度音程の音 (ダイアトニック・スケール上のハーモニー) を合成するハーモナイザー</b>など, 派生するエフェクターが多くあります.
            </p>
            <p>ピッチシフターのアルゴリズムもいくつかあります. そのすべてを解説することはできないので, このセクションでは, 以下の 2 つを解説します.</p>
            <ul>
              <li>
                時間領域の演算だけで実装できる<b>リサンプリング</b>と<b>タイムストレッチ</b>を利用した実装 (これは, Adobe Audition で利用されている
                <b>SOLA</b> (<b>Synchronized Overlap-Add</b>) や, <b>PSOLA</b> (<b>Pitch Synchronous Overlap-Add</b>) の原始的なアルゴリズムです)
              </li>
              <li>周波数領域での演算が必要となりますが, より精度の高いピッチシフターが実現できる, <b>フェーズボコーダ</b>を利用した実装</li>
            </ul>
            <p>
              周波数領域での演算による, ピッチシフターのアルゴリズムとしては, <b>正弦波モデル</b> (<b>スペクトルモデル</b>) による,
              <b>McAulay &amp; Quatieri モデル</b> (<b>Sinusoidal Spectral Modeling</b>) や <b>SMS</b> (<b>Spectral Modeling Synthesis</b>) などがあります
              (あるいは, これらのアルゴリズムが, フェーズボコーダに利用されている場合もあります).
            </p>
            <section id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch">
              <h5>リサンプリングとタイムストレッチ</h5>
              <p>
                <b>リサンプリング</b>を利用することによって, ピッチを変化させるることができます. リサンプリングとは,
                <b>サンプリング周波数を変える処理のことで</b>, <b>サンプル数を増やす場合, アップサンプリング</b>,
                <b>サンプル数を減らす場合, ダウンサンプリング</b>と呼びます. もちろん, 先に解説したように, リサンプリングによって,
                サンプリング周波数を変えるだけでは, 再生速度も変化してしまうので, <b>タイムストレッチ</b>を合わせて利用することで, 再生速度を変化させずに,
                ピッチを変化させることが可能になります.
              </p>
              <figure>
                <svg id="svg-figure-resampling" width="1200" height="600" data-parameters="true" data-a="1" />
                <figcaption>
                  <p>
                    左はダウンサンプリングで, ダウンサンプリングしたあと, 元のサンプリング周波数で再生すると, 周波数 (ピッチ) が高く, かつ,
                    再生速度が速くなる<br />
                    右はアップサンプリングで, アップサンプリングしたあと, 元のサンプリング周波数で再生すると, 周波数 (ピッチ) が低く, かつ, 再生速度が遅くなる
                  </p>
                </figcaption>
              </figure>
              <p>
                <b>タイムストレッチ</b>とは, ピッチシフターとは対となるエフェクターで,
                <b>周波数 (ピッチ) を変化させずに, 再生速度 (音長) を変化させる</b>エフェクターです.
              </p>
              <p>
                しかしながら, Web Audio API の仕様上, リサンプリングとタイムストレッチの組み合わせでピッチシフターを実装する場合は,
                リアルタイム処理はできないことに注意してください. 再生前のピッチシフト, つまり, <b>オフラインピッチシフター</b>となります. したがって, Web Audio
                API でリアルタイムでピッチシフターが必要な場合は, やはり, 次のセクションで解説する, フェーズボコーダなど周波数領域での演算をする必要があります
                (ワンショットオーディオなど, ごくサイズの小さいオーディオデータであれば, 体感上はリアルタイム処理の速さがあるかもしれませんが,
                厳密なリアルタイム処理ではありません).
              </p>
              <p>
                あえて解説をするのは, オーディオ信号処理では一般的に利用されるアルゴリズムでもあり, リサンプリングやタイムストレッチ (波形の伸縮処理) は,
                ピッチシフター以外でも頻繁に利用するオーディオ信号処理だからです.
              </p>
              <p>
                さらに追加すると, タイムストレッチでは, 事前にオーディオデータすべてを取得する必要があるので (これがリアルタイム処理できない理由です),
                楽曲データの再生でも <b><code>AudioBufferSourceNode</code> を利用する必要があります</b> (厳密には, オーディオデータの実体となる,
                <code>AudioBuffer</code> インスタンスから, チャンネルごとのオーディオデータをすべて取得して演算します).
              </p>
              <p>
                <code>AudioBufferSourceNode</code> を利用する場合, リサンプリングは <code>playbackRate</code> プロパティ (<code>AudioParam</code>)
                の値を変更するだけです. <code>playbackRate</code> プロパティを <code>1</code> より大きくすると, ダウンサンプリングとなって,
                再生速度が速くなるとともに, ピッチも高くなります. <code>playbackRate</code> プロパティを <code>1</code> より小さくすると,
                アップサンプリングとなって, 再生速度が遅くなるとともに, ピッチも低くなります (もちろん, 同様のことは
                <code>detune</code> プロパティでも可能ですが, リサンプリングが目的であれば, <code>playbackRate</code> プロパティのほうが直感的です).
              </p>
              <p>
                あとは, タイムストレッチが実装できれば, オフラインピッチシフターが実装できます. タイムストレッチのアルゴリズムの本質は,
                再生速度に関わらず同じですが, 再生速度が <code>1</code> より大きい場合と, 小さい場合では, 実装は少し異なるので, まずは,
                それぞれのケースに対してシンプルに解説します.
              </p>
              <section id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch-fast">
                <h6>再生速度を速くする場合</h6>
                <p>
                  まずは, 再生速度を速くする場合から解説します. 再生速度を速くする場合とは, 再生速度を
                  <span class="math-inline">$\mathrm{rate}$</span> で表した場合, <span class="math-inline">$\mathrm{rate} \gt 1$</span> となる場合です.
                  具体例として, <span class="math-inline">$\mathrm{rate} = 1.5 \left(= \frac{3}{2}\right)$</span> (<code>1.5</code> 倍速)
                  でタイムストレッチする場合を考えます. この場合, 以下のイラストで表すように, 3 周期分の波形を 2
                  周期に縮小する処理をオーディオデータ全体に適用します (すなわち, タイムストレッチ適用後のオーディオデータのサイズは,
                  <span class="">$\frac{2}{3} = 0.66 \cdots$</span> 倍となります). ただし, 単純に波形を重ねて縮小するだけでは,
                  <span class="math-inline">$n$</span> 番目の周期と <span class="math-inline">$n + 1$</span> 番目の周期が同時に再生されるだけなので,
                  <span class="math-inline">$n$</span> 番目の周期には, <b>単調減少関数を乗算した波形</b>と,
                  <span class="math-inline">$n + 1$</span> 番目の周期には, <b>単調増加関数を乗算した波形</b>を<b>オーバーラップアド</b>することによって,
                  縮小部分における不連続点を除去, あるいは, 緩和しつつ, 再生速度を速くすることができます.
                </p>
                <p>
                  この周期の縮小処理を, イラスト中の <span class="math-inline">$\mathrm{offset}$</span> を更新して, 逐次的に実行することで,
                  オーディオデータ全体の再生速度を速くすることができます.
                </p>
                <figure>
                  <svg id="svg-figure-time-stretch-fast" width="720" height="600" data-parameters="true" data-a="1" />
                  <figcaption>タイムストレッチ (再生速度が <code>1</code> より大きい場合)</figcaption>
                </figure>
                <p>
                  ところで, 説明を簡単にするために, あえてわかりやすい再生速度に設定しましたが, <code>1</code> より大きい
                  <span class="math-inline">$\mathrm{rate}$</span> において, <span class="math-inline">$m$</span> 周期分の波形を,
                  <span class="math-inline">$n$</span> 周期分の波形に縮小するというのは, 以下の関係式が定義できます.
                </p>
                <div class="math-block">$n = \mathrm{rate} \cdot m \quad \left(\mathrm{rate} \gt 1\right)$</div>
                <p>
                  <span class="math-inline">$\mathrm{rate} = 1.5 \left(= \frac{3}{2}\right)$</span> の場合,
                  <span class="math-inline">$2n = 3m$</span> が成立します.
                </p>
                <p>
                  この関係式と, (自己) 相関関数を利用すると, 逐次処理のために必要な <span class="math-inline">$\mathrm{offset}$</span> の値は,
                  以下のように定義できます (<span class="math-inline">$\mathrm{period}$</span> は, 波形の周期です).
                </p>
                <div class="math-block">
                  $\mathrm{offset} = \mathrm{round}\left(\frac{\mathrm{period}}{\mathrm{rate} - 1}\right) \quad \left(\mathrm{rate} \gt 1\right)$
                </div>
                <p><code>playbackRate</code> プロパティによるリサンプリングと, 上記のタイムストレッチのアルゴリズムを実装すると以下のようになります.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

// const rate  = 3 / 2;
// const pitch = 1 / rate;

const rate  = 1.5;
const pitch = 0.66;

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const audioBuffer = await context.decodeAudioData(arrayBuffer);

    const numberOfChannels = audioBuffer.numberOfChannels;

    const resampleRate = audioBuffer.sampleRate / pitch;

    const length = Math.ceil(audioBuffer.length / rate);

    const pitchShiftAudioBuffer = context.createBuffer(numberOfChannels, length, audioBuffer.sampleRate);

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const inputBuffer  = audioBuffer.getChannelData(channelNumber);
      const outputBuffer = new Float32Array(length);

      // Time Stretch
      const templateSize = Math.trunc(resampleRate * 0.01);

      // Set the range that correlation function detects peaks is between 5 msec (0.005 sec) and 20 msec (0.02 sec)
      const minPeriod = Math.trunc(resampleRate * 0.005);
      const maxPeriod = Math.trunc(resampleRate * 0.02);

      const x = new Float32Array(templateSize);
      const y = new Float32Array(templateSize);
      const r = new Float32Array(maxPeriod + 1);

      let offset0 = 0;
      let offset1 = 0;

      while ((offset0 + (2 * maxPeriod)) &lt; inputBuffer.length) {
        for (let n = 0; n &lt; templateSize; n++) {
          x[n] = inputBuffer[offset0 + n];
        }

        let maxCorrelation = 0.0;

        let period = minPeriod;

        for (let m = minPeriod; m &lt;= maxPeriod; m++) {
          for (let n = 0; n &lt; templateSize; n++) {
            y[n] = inputBuffer[offset0 + m + n];
          }

          r[m] = 0.0;

          // Correlation function
          for (let n = 0; n &lt; templateSize; n++) {
            r[m] += x[n] * y[n];
          }

          if (r[m] &gt; maxCorrelation) {
            maxCorrelation = r[m];  // The peak of correlation function
            period = m;
          }
        }

        // Overlap-Add
        for (let n = 0; n &lt; period; n++) {
          outputBuffer[offset1 + n]  = inputBuffer[offset0 + n] * ((period - n) / period);  // Monotonically decreasing
          outputBuffer[offset1 + n] += inputBuffer[offset0 + period + n] * (n / period);    // Monotonically increasing
        }

        const offset = Math.round(period / (rate - 1.0));

        for (let n = period; n &lt; offset; n++) {
          if ((offset0 + period + n) &gt;= inputBuffer.length) {
            break;
          }

          outputBuffer[offset1 + n] = inputBuffer[offset0 + period + n];
        }

        offset0 += period + offset;
        offset1 += offset;
      }

      pitchShiftAudioBuffer.copyToChannel(outputBuffer, channelNumber);
    }

    const source = new AudioBufferSourceNode(context, { buffer: pitchShiftAudioBuffer });

    // Resampling
    source.playbackRate.value = pitch;

    source.connect(context.destination);

    source.start(0);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              </section>
              <section id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch-slow">
                <h6>再生速度を遅くする場合</h6>
                <p>
                  同様に, 再生速度を遅くする場合を解説します. 再生速度を遅くする場合とは, 再生速度を
                  <span class="math-inline">$\mathrm{rate}$</span> で表した場合, <span class="math-inline">$\mathrm{rate} \lt 1$</span> となる場合です. そして,
                  <b>再生速度を速くする場合はオーディオデータを縮小</b>するのに対して, <b>再生速度を遅くする場合はオーディオデータを伸長</b>します (つまり,
                  オーディオデータを伸縮するというアルゴリズムの本質は同じです). 具体例として,
                  <span class="math-inline">$\mathrm{rate} = \frac{2}{3}$</span> (<span class="math-inline">$0.66 \cdots$</span> 倍速)
                  でタイムストレッチする場合を考えます. この場合, 以下のイラストで表すように, 2 周期分の波形を 3
                  周期に伸長する処理をオーディオデータ全体に適用します (すなわち, タイムストレッチ適用後のオーディオデータのサイズは,
                  <span class="">$\frac{3}{2} = 1.5$</span> 倍となります). ただし, 単純に波形を重ねて伸長するだけでは,
                  <span class="math-inline">$n$</span> 番目の周期と <span class="math-inline">$n + 1$</span> 番目の周期が同時に再生されるだけなので,
                  <span class="math-inline">$n$</span> 番目の周期には, <b>単調増加関数を乗算した波形</b>と,
                  <span class="math-inline">$n + 1$</span> 番目の周期には, <b>単調減少関数を乗算した波形</b>を<b>オーバーラップアド</b>することによって,
                  伸長部分における不連続点を除去, あるいは, 緩和しつつ, 再生速度を遅くすることができます.
                </p>
                <p>
                  この周期の伸長処理を, イラスト中の <span class="math-inline">$\mathrm{offset}$</span> を更新して, 逐次的に実行することで,
                  オーディオデータ全体の再生速度を遅くすることができます.
                </p>
                <figure>
                  <svg id="svg-figure-time-stretch-slow" width="720" height="600" data-parameters="true" data-a="1" />
                  <figcaption>タイムストレッチ (再生速度が <code>1</code> より小さい場合)</figcaption>
                </figure>
                <p>
                  ところで, 説明を簡単にするために, あえてわかりやすい再生速度に設定しましたが, <code>1</code> より小さい
                  <span class="math-inline">$\mathrm{rate}$</span> において, <span class="math-inline">$m$</span> 周期分の波形を,
                  <span class="math-inline">$n$</span> 周期分の波形に伸長するというのは, 再生速度を速くする場合と同様の, 以下の関係式が定義できます.
                </p>
                <div class="math-block">$n = \mathrm{rate} \cdot m \quad \left(0 \lt \mathrm{rate} \lt 1\right)$</div>
                <p>
                  <span class="math-inline">$\mathrm{rate} = 0.66 \cdots \left(= \frac{2}{3}\right)$</span> の場合,
                  <span class="math-inline">$3n = 2m$</span> が成立します.
                </p>
                <p>
                  この関係式と, (自己) 相関関数を利用すると, 逐次処理のために必要な <span class="math-inline">$\mathrm{offset}$</span> の値は,
                  以下のように定義できます (<span class="math-inline">$\mathrm{period}$</span> は, 波形の周期です).
                </p>
                <div class="math-block">
                  $\mathrm{offset} = \mathrm{round}\left(\frac{\mathrm{period} \cdot \mathrm{rate}}{1.0 - \mathrm{rate}}\right) \quad \left(0 \lt \mathrm{rate}
                  \lt 1\right)$
                </div>
                <p><code>playbackRate</code> プロパティによるリサンプリングと, 上記のタイムストレッチのアルゴリズムを実装すると以下のようになります.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

// const rate  = 2 / 3;
// const pitch = 1 / rate;

const rate  = 0.66;
const pitch = 1.5;

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const audioBuffer = await context.decodeAudioData(arrayBuffer);

    const numberOfChannels = audioBuffer.numberOfChannels;

    const resampleRate = audioBuffer.sampleRate / pitch;

    const length = Math.ceil(audioBuffer.length / rate);

    const pitchShiftAudioBuffer = context.createBuffer(numberOfChannels, length, audioBuffer.sampleRate);

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const inputBuffer  = audioBuffer.getChannelData(channelNumber);
      const outputBuffer = new Float32Array(length);

      // Time Stretch
      const templateSize = Math.trunc(resampleRate * 0.01);

      // Set the range that correlation function detects peaks is between 5 msec (0.005 sec) and 20 msec (0.02 sec)
      const minPeriod = Math.trunc(resampleRate * 0.005);
      const maxPeriod = Math.trunc(resampleRate * 0.02);

      const x = new Float32Array(templateSize);
      const y = new Float32Array(templateSize);
      const r = new Float32Array(maxPeriod + 1);

      let offset0 = 0;
      let offset1 = 0;

      while ((offset0 + (2 * maxPeriod)) &lt; inputBuffer.length) {
        for (let n = 0; n &lt; templateSize; n++) {
          x[n] = inputBuffer[offset0 + n];
        }

        let maxCorrelation = 0.0;
        let period = minPeriod;

        for (let m = minPeriod; m &lt;= maxPeriod; m++) {
          for (let n = 0; n &lt; templateSize; n++) {
            y[n] = inputBuffer[offset0 + m + n];
          }

          r[m] = 0.0;

          // Correlation function
          for (let n = 0; n &lt; templateSize; n++) {
            r[m] += x[n] * y[n];
          }

          if (r[m] &gt; maxCorrelation) {
            maxCorrelation = r[m];  // The peak of correlation function
            period = m;
          }
        }

        for (let n = 0; n &lt; period; n++) {
          outputBuffer[offset1 + n] = inputBuffer[offset0 + n];
        }

        for (let n = 0; n &lt; period; n++) {
          outputBuffer[offset1 + period + n]  = inputBuffer[offset0 + period + n] * ((period - n) / period);  // Monotonically decreasing
          outputBuffer[offset1 + period + n] += inputBuffer[offset0 + n] * (n / period);                      // Monotonically increasing
        }

        const offset = Math.round((period * rate) / (1.0 - rate));

        for (let n = period; n &lt; offset; n++) {
          if ((offset0 + n) &gt;= inputBuffer.length) {
            break;
          }

          outputBuffer[offset1 + period + n] = inputBuffer[offset0 + n];
        }

        offset0 += offset;
        offset1 += period + offset;
      }

      pitchShiftAudioBuffer.copyToChannel(outputBuffer, channelNumber);
    }

    const source = new AudioBufferSourceNode(context, { buffer: pitchShiftAudioBuffer });

    // Resampling
    source.playbackRate.value = pitch;

    source.connect(context.destination);

    source.start(0);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              </section>
              <article id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch-correlation-functions">
                <h6>相関関数</h6>
                <p>
                  相関関数は以下のように定義されます (ちなみに, <b>同じ信号系列</b>の過去の信号を積和しているので, より厳密には, <b>自己相関関数</b>の定義です).
                  <span class="math-inline">$y\left(m\right)$</span> は (自己) 相関関数,
                  <span class="math-inline">$x\left(n\right)$</span> は相関対象の信号系列, <span class="math-inline">$N$</span> は (自己)
                  相関関数のサイズを表しています.
                </p>
                <div class="math-block">$y\left(m\right) = \sum_{n=0}^{N-1}x\left(n\right)x\left(n + m\right) \quad \left(0 \le m \le N - 1 \right)$</div>
                <p>
                  (自己) 相関関数は, 元の信号系列と, その <span class="math-inline">$m$</span> サンプル過去の信号を
                  <span class="math-inline">$N$</span> 個の区間に限定して, 積和演算で算出します (演算の意味は異なりますが, 形式的に数式で考えると,
                  コンボリューション積分 (畳み込み積分) と同じです).
                </p>
                <p>
                  オーディオ信号処理においては, <b>相関関数は, 基本周期とその整数倍で正のピークを表すという特徴があります</b>. つまり,
                  <b>波形データを走査してピークを検出することで, 近似的な周期を求めることができます</b>.
                </p>
              </article>
              <article id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch-resampling">
                <h6>一般的なオーディオ信号処理におけるリサンプリング</h6>
                <p>
                  一般的なオーディオ信号処理におけるリサンプリングのアルゴリズムには, <b>線形補間</b>によるアルゴリズムと,
                  <b>シンク関数</b>を利用したアルゴリズムがあります.
                </p>
                <p>以下は, 線形補間によるリサンプリングの実装例です</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

// const pitch = 2 / 3;
const pitch = 0.66;

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const audioBuffer = await context.decodeAudioData(arrayBuffer);

    const numberOfChannels = audioBuffer.numberOfChannels;

    const length = Math.ceil(audioBuffer.length / pitch);

    const resampledAudioBuffer = context.createBuffer(numberOfChannels, length, audioBuffer.sampleRate);

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const inputBuffer  = audioBuffer.getChannelData(channelNumber);
      const outputBuffer = new Float32Array(length);

      // Resampling
      for (let n = 0; n &lt; length; n++) {
        const t = pitch * n;
        const m = Math.trunc(t);

        const delta = t - m;

        if ((m &gt;= 0) &amp;&amp; ((m + 1) &lt; length)) {
          outputBuffer[n] = (delta * inputBuffer[m + 1]) + ((1 - delta) * inputBuffer[m]);
        }
      }

      resampledAudioBuffer.copyToChannel(outputBuffer, channelNumber);
    }

    const source = new AudioBufferSourceNode(context, { buffer: resampledAudioBuffer });

    source.connect(context.destination);

    source.start(0);
  })
  .catch((error) =&gt; {
    // error handling
  }); </code></pre>
                <p>
                  もっとも, 線形補間は, リサンプル点を直線で補間するので, 実際の波形とは異なったオーディオデータとなってしまいます (ユースケースによっては,
                  線形補間で十分な場合もあると思いますが). サンプリング定理にしたがったアルゴリズムは, シンク関数によるリサンプリングの一択です. 以下は,
                  シンク関数のサイズを <code>24</code> とした場合の, リサンプリングの実装例です.
                </p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const SIZE_OF_SINC = 24;

function sinc(n) {
  if (n === 0) {
    return 1;
  }

  return Math.sin(n) / n;
}

const context = new AudioContext();

// const pitch = 2 / 3;
const pitch = 0.66;

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const audioBuffer = await context.decodeAudioData(arrayBuffer);

    const numberOfChannels = audioBuffer.numberOfChannels;

    const length = Math.ceil(audioBuffer.length / pitch);

    const resampledAudioBuffer = context.createBuffer(numberOfChannels, length, audioBuffer.sampleRate);

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const inputBuffer  = audioBuffer.getChannelData(channelNumber);
      const outputBuffer = new Float32Array(length);

      // Resampling
      for (let n = 0; n &lt; length; n++) {
        const t = pitch * n;
        const offset = Math.trunc(t);

        const halfOfSincSize = SIZE_OF_SINC / 2;

        for (let m = (offset - halfOfSincSize); m &lt;= (offset + halfOfSincSize); m++) {
          if ((m &gt;= 0) &amp;&amp; (m &lt; inputBuffer.length)) {
            outputBuffer[n] += inputBuffer[m] * sinc(Math.PI * (t - m));
          }
        }
      }

      resampledAudioBuffer.copyToChannel(outputBuffer, channelNumber);
    }

    const source = new AudioBufferSourceNode(context, { buffer: resampledAudioBuffer });

    source.connect(context.destination);

    source.start(0);
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              </article>
              <article id="section-effectors-by-audio-worklet-pitch-shifter-by-resampling-and-time-stretch-web-audio-api-time-stretch">
                <h6>Web Audio API におけるタイムストレッチ</h6>
                <p>
                  タイムストレッチのアルゴリズムは複雑になりますが, <code>MediaElementAudioSourceNode</code> を利用する場合, その音源となる,
                  <code>HTMLAudioElement</code> や <code>HTMLVideoElement</code> の <code>playbackRate</code> プロパティは,
                  タイムストレッチを効かせながら再生速度を変更することができます (<code>AudioBufferSourceNode</code> の <code>playbackRate</code> プロパティ
                  (<code>AudioParam</code>) とは異なるので注意してください). したがって, タイムストレッチのみが目的であれば,
                  <code>MediaElementAudioSourceNode</code> を利用して, <code>HTMLAudioElement</code> や <code>HTMLVideoElement</code> の
                  <code>playbackRate</code> プロパティを制御すれば簡単に実装できます.
                </p>
                <p>
                  ちなみに, <code>HTMLAudioElement</code> や <code>HTMLVideoElement</code> には <b><code>preservesPitch</code></b> プロパティが定義されており,
                  デフォルト値が <code>true</code> なので, タイムストレッチが効きます. したがって, このプロパティを <code>false</code> にすると,
                  タイムストレッチが無効になり, <code>playbackRate</code> プロパティ変更とともに, ピッチも変化します.
                </p>
              </article>
            </section>
            <section id="section-effectors-by-audio-worklet-pitch-shifter-by-phase-vocoder">
              <h5>フェーズボコーダ</h5>
              <p>
                このセクションでは,
                <a href="https://github.com/olvb/phaze" target="_blank" rel="noopener noreferrer">Phaze: a real-time web audio pitch-shifter</a>
                で利用されている, フェーズボコーダを利用したピッチシフター (それと同時に可能な, タイムストレッチ) の実装を解説します.
              </p>
              <p>
                フェーズボコーダのアルゴリズムは 1 つではなく, 例えば, 最初期のフェーズボコーダは, パンドパスフィルタ (フィルタバンク) を並列接続して,
                それぞれの周波数帯域で <b>STFT</b> (<b>Short Time Fourier Transform</b>: <b>短時間フーリエ変換</b>. 実装上は単純に, サンプル数が少ない FFT です)
                を適用して, ピッチ変換やスペクトルの伸縮を実行します. 変換対象のピッチの検出には, <b>瞬時周波数</b> (位相の時間微分) を利用します.
              </p>
              <p>
                現代的なフェーズボコーダでは, バンドパスフィルタを利用するのではなく, 振幅スペクトルを走査して, ピークを探索して,
                それぞれのピークをシフトするのが一般的です.
                <a href="https://github.com/olvb/phaze" target="_blank" rel="noopener noreferrer">Phaze: a real-time web audio pitch-shifter</a>
                のフェーズボコーダも, おおよそこのような実装となっています (さらにモダンで高度なフェーズボコーダだと,
                振幅スペクトルのピークの帯域ごとに窓関数のサイズを可変にするといったより複雑な実装が適用されています.
              </p>
              <p>実装のステップは大きく 3 つです.</p>
              <ol>
                <li>STFT (FFT) を適用して, 振幅スペクトル (厳密には, その 2 乗なので, パワースペクトル) の配列を取得</li>
                <li>パワースペクトルを走査して, スペクトルのピークのインデックスと, ピークの数を検出</li>
                <li>
                  ピークをピッチ (タイムストレッチもする場合は, 再生速度の逆数を乗算) に応じて, 実部と虚部でそれぞれシフトして, ISTFT (IFFT) で,
                  ピッチシフトした時間領域の波形を取得
                </li>
              </ol>
              <p>
                <code>OverlapAddProcessor</code> クラスを拡張した <code>PitchShifterProcessor</code> サブクラスの実装です.
                (周波数分解能をデフォルトより高くするために, <code>OverlapAddProcessor</code> クラスを拡張します). また,
                <code>processOverlapAdd</code> メソッドで呼び出している, <code>FFT</code> / <code>IFFT</code> 関数は,
                <a href="#section-fast-fourier-transform-code">高速フーリエ変換の実装</a>セクションで記載している実装を
                <code>AudioWorkletGlobalScope</code> 内で定義していると仮定してください.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/pitch-shifter.js&apos;

class PitchShifterProcessor extends OverlapAddProcessor {
  static createHanningWindow(size) {
    const w = new Float32Array(size);

    for (let n = 0; n &lt; size; n++) {
      w[n] = 0.5 - 0.5 * Math.cos((2 * Math.PI * n) / (size - 1));
    }

    return w;
  }

  constructor(options) {
    super(options);

    this.pitch = 1;
    this.speed = 1;

    this.timeCursor = 0;

    this.hanningWindow = PitchShifterProcessor.createHanningWindow(this.frameSize);

    this.port.onmessage = (event) =&gt; {
      if (event.data.pitch &gt; 0) {
        this.pitch = event.data.pitch;
      }

      if (event.data.speed &gt; 0) {
        this.speed = event.data.speed;
      }
    };
  }

  /** @overdrive */
  processOverlapAdd(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    const numberOfChannels = input.length;

    const fftSize = this.frameSize;

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      const reals = new Float32Array(fftSize);
      const imags = new Float32Array(fftSize);

      for (let n = 0; n &lt; fftSize; n++) {
        reals[n] = this.hanningWindow[n] * input[channelNumber][n];
      }

      FFT(reals, imags, fftSize);

      const halfOfFFTSizze = fftSize / 2;
      const bufferSize     = halfOfFFTSizze + 1;

      const magnitudes  = new Float32Array(bufferSize);
      const peakIndexes = new Uint16Array(bufferSize);

      for (let k = 0; k &lt; bufferSize; k++) {
        magnitudes[k] = (reals[k] ** 2) + (imags[k] ** 2);
      }

      let numberOfPeaks = 0;

      // Find peaks
      let index = 2;

      const end = halfOfFFTSizze + 1 - 2;

      while (index &lt; end) {
        const magnitude = magnitudes[index];

        if ((magnitudes[index - 1] &gt;= magnitude) || (magnitudes[index - 2] &gt;= magnitude)) {
          ++index;
          continue;
        }

        if ((magnitudes[index + 1] &gt;= magnitude) || (magnitudes[index + 2] &gt;= magnitude)) {
          ++index;
          continue;
        }

        peakIndexes[numberOfPeaks++] = index;

        index += 2;
      }

      // Shift peaks
      const shiftedReals = new Float32Array(fftSize);
      const shiftedImags = new Float32Array(fftSize);

      for (let k = 0; k &lt; numberOfPeaks; k++) {
        const peakIndex = peakIndexes[k];

        const shiftedPeakIndex = Math.round(peakIndex * this.pitch * (1 / this.speed));

        if (shiftedPeakIndex &gt; bufferSize) {
          break;
        }

        let startIndex = 0;
        let endIndex   = fftSize;;

        if (k &gt; 0) {
          const peakIndexBefore = peakIndexes[k - 1];

          startIndex = peakIndex - Math.floor((peakIndex - peakIndexBefore) / 2);
        }

        if (k &lt; (numberOfPeaks - 1)) {
          const peakIndexAfter = peakIndexes[k + 1];

          endIndex = peakIndex + Math.ceil((peakIndexAfter - peakIndex) / 2);
        }

        const startOffset = startIndex - peakIndex;
        const endOffset   = endIndex   - peakIndex;

        for (let m = startOffset; m &lt; endOffset; m++) {
          const binCountIndex = peakIndex + m;

          const shiftedBinCountIndex = shiftedPeakIndex + m;

          if (shiftedBinCountIndex &gt;= bufferSize) {
            break;
          }

          const omega = (2 * Math.PI * (shiftedBinCountIndex - binCountIndex)) / fftSize;

          const shiftedReal = Math.cos(omega * this.timeCursor);
          const shiftedImag = Math.sin(omega * this.timeCursor);

          shiftedReals[shiftedBinCountIndex] += (reals[binCountIndex] * shiftedReal) - (imags[binCountIndex] * shiftedImag);
          shiftedImags[shiftedBinCountIndex] += (reals[binCountIndex] * shiftedImag) + (imags[binCountIndex] * shiftedReal);
        }
      }

      for (let k = 1; k &lt; halfOfFFTSizze; k++) {
        shiftedReals[fftSize - k] = 0.0 + shiftedReals[k];
        shiftedImags[fftSize - k] = 0.0 - shiftedImags[k];
      }

      IFFT(shiftedReals, shiftedImags, fftSize);

      for (let n = 0; n &lt; fftSize; n++) {
        output[channelNumber][n] = this.hanningWindow[n] * shiftedReals[n];
      }
    }

    this.timeCursor += this.hopSize;
  }
}

registerProcessor(&apos;PitchShifterProcessor&apos;, PitchShifterProcessor);</code></pre>
              <p>メインスレッドの実装例です.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-html line-numbers">&lt;button type=&quot;button&quot; id=&quot;button-pitch-shifter&quot; disabled&gt;start&lt;/button&gt;
&lt;label for=&quot;range-pitch-shifter&quot;&gt;pitch&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-pitch-shifter&quot; disabled value=&quot;1&quot; min=&quot;0.5&quot; max=&quot;4&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-pitch-shifter-value&quot;&gt;1.00&lt;/span&gt;
&lt;label for=&quot;range-time-stretch&quot;&gt;speed&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-time-stretch&quot; disabled value=&quot;1&quot; min=&quot;0.5&quot; max=&quot;4&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-time-stretch-value&quot;&gt;1.00&lt;/span&gt;</code></pre>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const buttonElement = document.getElementById(&apos;button-pitch-shifter&apos;);

const rangePitchElement = document.getElementById(&apos;range-pitch-shifter&apos;);
const rangeSpeedElement = document.getElementById(&apos;range-time-stretch&apos;);
const spanPitchElement  = document.getElementById(&apos;print-pitch-shifter-value&apos;);
const spanSpeedElement  = document.getElementById(&apos;print-time-stretch-value&apos;);

let source    = null;
let processor = null;

let pitch = 1;
let speed = 1;

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
     // &apos;./audio-worklets/pitch-shifter.js&apos; is URL that has subclass that extends `OverlapAddProcessor`
     await context.audioWorklet.addModule(&apos;./audio-worklets/pitch-shifter.js&apos;)

     const buffer = await context.decodeAudioData(arrayBuffer);

     buttonElement.removeAttribute(&apos;disabled&apos;);
     rangePitchElement.removeAttribute(&apos;disabled&apos;);
     rangeSpeedElement.removeAttribute(&apos;disabled&apos;);

     buttonElement.addEventListener(&apos;click&apos;, () =&gt; {
       if ((source === null) &amp;&amp; (processor === null)) {
         source    = new AudioBufferSourceNode(context, { buffer, playbackRate: speed });
         processor = new AudioWorkletNode(context, &apos;PitchShifterProcessor&apos;);

         processor.port.postMessage({ pitch, speed });

         // AudioBufferSourceNode (Input) -&gt; AudioWorkletNode (Pitch Shifter) -&gt; AudioDestinationNode (Output)
         source.connect(processor);
         processor.connect(context.destination);

         source.start(0);

         source.onended = () =&gt; {
           source = null;

           if (processor) {
             processor.disconnect();
             processor = null;
           }

           buttonElement.textContent = &apos;start&apos;
         };

         buttonElement.textContent = &apos;stop&apos;
       } else {
         source.stop(0);

         source = null;

         if (processor) {
           processor.disconnect();
           processor = null;
         }

         buttonElement.textContent = &apos;start&apos;
       }
     });

     rangePitchElement.addEventListener(&apos;input&apos;, (event) =&gt; {
       pitch = event.currentTarget.valueAsNumber;

       if (processor) {
         processor.port.postMessage({ pitch });
       }

       spanPitchElement.textContent = pitch.toFixed(2);
     });

     rangeSpeedElement.addEventListener(&apos;input&apos;, (event) =&gt; {
       speed = event.currentTarget.valueAsNumber;

       if (source &amp;&amp; processor) {
         source.playbackRate.value = speed;
         processor.port.postMessage({ speed });
       }

       spanSpeedElement.textContent = speed.toFixed(2);
     });
  })
  .catch((error) =&gt; {
    // error handling
  });</code></pre>
              <div class="app-container app-pitch-shifter">
                <div class="app-headline">
                  <button type="button" id="button-pitch-shifter" disabled>start</button>
                </div>
                <div>
                  <dl>
                    <div>
                      <dt><label for="range-pitch-shifter">pitch</label></dt>
                      <dd>
                        <input type="range" id="range-pitch-shifter" disabled value="1" min="0.5" max="4" step="0.05" /><span id="print-pitch-shifter-value"
                          >1.00</span>
                      </dd>
                    </div>
                    <div>
                      <dt><label for="range-time-stretch">speed</label></dt>
                      <dd>
                        <input type="range" id="range-time-stretch" disabled value="1" min="0.5" max="4" step="0.05" /><span id="print-time-stretch-value"
                          >1.00</span>
                      </dd>
                    </div>
                  </dl>
                </div>
              </div>
              <p>
                このセクションでは, 原音をピッチシフトしましたが, 原音とピッチシフトしたエフェクト音をミックスすることで,
                オクターバーやハーモナイザーを実装できます. また, <code>pitch</code> を <code>AudioParamDescriptor</code> として実装して,
                メインスレッド側でパラメータをオートメーションすれば, ピッチベンドも実装できるでしょう.
              </p>
            </section>
          </section>
          <section id="section-effectors-by-audio-worklet-vocal-canceler-on-spectrum">
            <h4>スペクトル領域でのボーカルキャンセラ</h4>
            <p>
              AudioWorklet によるオーディオ信号処理の実装例として,
              <a href="#section-audio-worklet-examples-vocal-canceler">時間領域でのボーカルキャンセラ</a>の実装を解説しました. しかしながら,
              時間領域でのボーカルキャンセラは実装はシンプルですが, ドラムなど中央に位置する楽器音も取り除かれてしまう, また,
              ボーカルキャンセラ後の左右のチャンネルのデータは同じになるので, モノラル再生になってしまうというデメリットがありました.
            </p>
            <p>
              スペクトル領域でのボーカルキャンセラは, 実装は複雑になりますが, ボーカル以外の中央に位置する楽器音は残しつつ, かつ,
              左右のチャンネルのデータの特徴は保ちながらステレオ再生が可能になります. 具体的に説明すれば, スペクトル領域での演算にすることで,
              ボーカルの周波数帯域に範囲を限定して, 左右のチャンネルを減算することで,
              時間領域のボーカルキャンセラよりも精度のよいボーカルキャンセラを実装することが可能となります.
            </p>
            <p>
              時間領域ではボーカル音を対象に除去しているのではなく, 音源の位置関係から結果として除去しているのに対して,
              <b>スペクトル領域でのボーカルキャンセラは, ボーカル音を対象にして除去するアルゴリズムです</b>.
            </p>
            <p>
              ただし, スペクトル領域でのボーカルキャンセラでも, 左右のチャンネルデータが必要なので,
              対象のオーディオデータはステレオである必要があることには変わりません.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/vocal-canceler-on-spectrum.js&apos;

class SpectrumVocalCancelerProcessor extends OverlapAddProcessor {
  // Safe positive minimum on `float` (6 digits)
  static MINIMUM_AMPLITUDE = 0.000001;

  static createHanningWindow(size) {
    const w = new Float32Array(size);

    for (let n = 0; n &lt; size; n++) {
      w[n] = 0.5 - 0.5 * Math.cos((2 * Math.PI * n) / (size - 1));
    }

    return w;
  }

  constructor(options) {
    super(options);

    this.depth        = 0;
    this.minFrequency = 200;
    this.maxFrequency = 8000;
    this.threshold    = 0.05;

    this.hanningWindow = SpectrumVocalCancelerProcessor.createHanningWindow(this.frameSize);

    this.port.onmessage = (event) =&gt; {
      if ((event.data.depth &gt;= 0) &amp;&amp; (event.data.depth &lt;= 1)) {
        this.depth = event.data.depth;
      }

      if ((event.data.minFrequency &gt;= 0) &amp;&amp; (event.data.minFrequency &lt; this.maxFrequency)) {
        this.minFrequency = event.data.minFrequency;
      }

      if (event.data.maxFrequency &gt;= this.minFrequency) {
        this.maxFrequency = event.data.maxFrequency;
      }

      if ((event.data.threshold &gt;= 0) &amp;&amp; (event.data.threshold &lt;= 1)) {
        this.threshold = event.data.threshold;
      }
    };
  }

  /** @override */
  processOverlapAdd(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    if ((input.length === 0) || (output.length === 0)) {
      return true;
    }

    if ((input.length !== 2) || (output.length !== 2)) {
      for (let channelNumber = 0, numberOfChannels = input.length; channelNumber &lt; numberOfChannels; channelNumber++) {
        output[channelNumber].set(input[channelNumber]);
      }

      return true;
    }

    if (this.depth === 0) {
      for (let channelNumber = 0, numberOfChannels = input.length; channelNumber &lt; numberOfChannels; channelNumber++) {
        output[channelNumber].set(input[channelNumber]);
      }

      return true;
    }

    const inputLs = input[0];
    const inputRs = input[1];

    const outputLs = output[0];
    const outputRs = output[1];

    const fftSize = this.frameSize;

    const realLs = new Float32Array(fftSize);
    const realRs = new Float32Array(fftSize);
    const imagLs = new Float32Array(fftSize);
    const imagRs = new Float32Array(fftSize);

    for (let n = 0; n &lt; fftSize; n++) {
      realLs[n] = this.hanningWindow[n] * inputLs[n];
      realRs[n] = this.hanningWindow[n] * inputRs[n];
    }

    FFT(realLs, imagLs, fftSize);
    FFT(realRs, imagRs, fftSize);

    const absLs = new Float32Array(fftSize);
    const absRs = new Float32Array(fftSize);
    const argLs = new Float32Array(fftSize);
    const argRs = new Float32Array(fftSize);

    for (let k = 0; k &lt; fftSize; k++) {
      absLs[k] = Math.sqrt((realLs[k] ** 2) + (imagLs[k] ** 2));
      absRs[k] = Math.sqrt((realRs[k] ** 2) + (imagRs[k] ** 2));
      argLs[k] = Math.atan2(imagLs[k], realLs[k]);
      argRs[k] = Math.atan2(imagRs[k], realRs[k]);
    }

    const minIndex = Math.trunc(this.minFrequency * (fftSize / sampleRate));
    const maxIndex = Math.trunc(this.maxFrequency * (fftSize / sampleRate));

    for (let k = minIndex; k &lt; maxIndex; k++) {
      const numerator   = Math.pow((absLs[k] - absRs[k]), 2);
      const denominator = Math.pow((absLs[k] + absRs[k]), 2);

      if (denominator != 0.0) {
        const diff = numerator / denominator;

        if (diff &lt; this.threshold) {
          absLs[k] = SpectrumVocalCancelerProcessor.MINIMUM_AMPLITUDE;
          absRs[k] = SpectrumVocalCancelerProcessor.MINIMUM_AMPLITUDE;

          absLs[fftSize - k] = absLs[k];
          absRs[fftSize - k] = absRs[k];
        }
      }
    }

    for (let k = 0; k &lt; fftSize; k++) {
      realLs[k] = absLs[k] * Math.cos(argLs[k]);
      realRs[k] = absRs[k] * Math.cos(argRs[k]);
      imagLs[k] = absLs[k] * Math.sin(argLs[k]);
      imagRs[k] = absRs[k] * Math.sin(argRs[k]);
    }

    IFFT(realLs, imagLs, fftSize);
    IFFT(realRs, imagRs, fftSize);

    for (let n = 0; n &lt; fftSize; n++) {
      outputLs[n] = ((1 - this.depth) * inputLs[n]) + (this.depth * (this.hanningWindow[n] * realLs[n]));
      outputRs[n] = ((1 - this.depth) * inputRs[n]) + (this.depth * (this.hanningWindow[n] * realRs[n]));
    }

    return true;
  }
}

registerProcessor(&apos;SpectrumVocalCancelerProcessor&apos;, SpectrumVocalCancelerProcessor);</code></pre>
            <p>
              上記の, スペクトル領域でのボーカルキャンセラでは, ボーカルの周波数帯域を <code>200 Hz</code> ~ <code>8000 Hz</code> と仮定して,
              その間の振幅スペクトルから, 以下の式で定義される閾値を算出します (<span class="math-inline">$X_{L}\left(k\right)$</span>
              は左チャンネルの振幅スペクトル (の配列), <span class="math-inline">$X_{R}\left(k\right)$</span> は右チャンネルの振幅スペクトル (の配列)
              を表しています).
            </p>
            <div class="math-block">
              $D\left(k\right) = \frac{\left|X_{L}\left(k\right) - X_{R}\left(k\right)\right|^{2}}{\left|X_{L}\left(k\right) + X_{R}\left(k\right)\right|^{2}}$
            </div>
            <p>
              この算出した閾値 (<span class="math-inline">$D\left(k\right)$</span>) が, あらかじめ指定した閾値 (変数 <code>threshold</code>) を下回っていれば,
              ボーカル音とみなして, 32 bit 浮動小数点の保証される正の最小値 (およそ, 小数点以下 6 桁) の振幅に設定することで除去しています (また,
              振幅スペクトルは, ナイキスト周波数を軸に線対称となるので, 線対称となる後半のインデックスにも同様の値を設定します).
            </p>
            <p>
              あとは, メインスレッドの実装例です. <code>threshold</code> はハードコーディングでもよいですが, アプリケーション側から設定可能にすると,
              除去レベルを調整しやすいので, 以下のメインスレッドの実装例では, ユーザーインタラクティブに制御可能にしています.
            </p>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-html line-numbers">&lt;input type=&quot;file&quot; id=&quot;file-vocal-canceler&quot; /&gt;
&lt;audio id=&quot;audio-vocal-canceler&quot; controls&gt;&lt;/audio&gt;
&lt;label for=&quot;range-vocal-canceler-depth&quot;&gt;depth&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-vocal-canceler-depth&quot; disabled value=&quot;0&quot; min=&quot;0.5&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-vocal-canceler-depth-value&quot;&gt;0.00&lt;/span&gt;
&lt;label for=&quot;range-vocal-canceler-min-frequency&quot;&gt;minFrequency&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-vocal-canceler-min-frequency&quot; disabled value=&quot;200&quot; min=&quot;100&quot; max=&quot;16000&quot; step=&quot;100&quot; /&gt;
&lt;span id=&quot;print-vocal-canceler-min-frequency-value&quot;&gt;2000 Hz&lt;/span&gt;
&lt;label for=&quot;range-vocal-canceler-range&quot;&gt;range (maxFrequency)&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-vocal-canceler-range&quot; disabled value=&quot;7800&quot; min=&quot;100&quot; max=&quot;16000&quot; step=&quot;100&quot; /&gt;
&lt;span id=&quot;print-vocal-canceler-range-value&quot;&gt;7800 Hz (8000 Hz)&lt;/span&gt;
&lt;label for=&quot;range-vocal-canceler-threshold&quot;&gt;threshold&lt;/label&gt;
&lt;input type=&quot;range&quot; id=&quot;range-vocal-canceler-threshold&quot; disabled value=&quot;0.5&quot; min=&quot;0&quot; max=&quot;1&quot; step=&quot;0.05&quot; /&gt;
&lt;span id=&quot;print-vocal-canceler-threshold-value&quot;&gt;0.50&lt;/span&gt;</code></pre>
            <pre
              data-prismjs-copy="クリップボードにコピー"
              data-prismjs-copy-success="コピーしました"
            ><code class="language-js line-numbers">const context = new AudioContext();

const inputElement = document.getElementById(&apos;file-vocal-canceler&apos;);
const audioElement = document.getElementById(&apos;audio-vocal-canceler&apos;);

const rangeDepthElement        = document.getElementById(&apos;range-vocal-canceler-depth&apos;);
const rangeMinFrequencyElement = document.getElementById(&apos;range-vocal-canceler-min-frequency&apos;);
const rangeRangeElement        = document.getElementById(&apos;range-vocal-canceler-range&apos;);
const rangeThresholdElement    = document.getElementById(&apos;range-vocal-canceler-threshold&apos;);
const spanDepthElement         = document.getElementById(&apos;print-vocal-canceler-depth-value&apos;);
const spanMinFrequencyElement  = document.getElementById(&apos;print-vocal-canceler-min-frequency-value&apos;);
const spanRangeElement         = document.getElementById(&apos;print-vocal-canceler-range-value&apos;);
const spanThresholdElement     = document.getElementById(&apos;print-vocal-canceler-threshold-value&apos;);

let source    = null;
let processor = null;

let depth        = 0;
let minFrequency = 200;
let maxFrequency = 8000;
let threshold    = 0.5;

inputElement.addEventListener(&apos;click&apos;, async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  rangeDepthElement.removeAttribute(&apos;disabled&apos;);
  rangeMinFrequencyElement.removeAttribute(&apos;disabled&apos;);
  rangeRangeElement.removeAttribute(&apos;disabled&apos;);
  rangeThresholdElement.removeAttribute(&apos;disabled&apos;);

  context.audioWorklet.addModule(&apos;./audio-worklets/vocal-canceler-on-spectrum.js&apos;)
    .then(() =&gt; {
      processor = new AudioWorkletNode(context, &apos;SpectrumVocalCancelerProcessor&apos;);

      processor.port.postMessage({ depth });
      processor.port.postMessage({ minFrequency });
      processor.port.postMessage({ maxFrequency });
      processor.port.postMessage({ threshold });
    })
    .catch((error) =&gt; {
      // error handling
    })
}, { once: true });

inputElement.addEventListener(&apos;change&apos;, (event) =&gt; {
  const file = event.currentTarget.files[0];

  audioElement.src = window.URL.createObjectURL(file);
});

audioElement.addEventListener(&apos;loadstart&apos;, () =&gt; {
  if (processor === null) {
    return;
  }

  if (source === null) {
    source = new MediaElementAudioSourceNode(context, { mediaElement: audioElement });

    // MediaElementAudioSourceNode (Input) -&gt; AudioWorkletNode (Vocal Canceler) -&gt; AudioDestinationNode (Output)
    source.connect(processor);
    processor.connect(context.destination);
  }
});

rangeDepthElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  depth = event.currentTarget.valueAsNumber;

  if (processor) {
    processor.port.postMessage({ depth });
  }

  spanDepthElement.textContent = depth.toFixed(2);
});

rangeMinFrequencyElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  minFrequency = event.currentTarget.valueAsNumber;

  if (processor) {
    processor.port.postMessage({ minFrequency });
  }

  spanMinFrequencyElement.textContent = `${minFrequency} Hz`;
});

rangeRangeElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  const range = event.currentTarget.valueAsNumber;

  maxFrequency = minFrequency + range;

  if (processor) {
    processor.port.postMessage({ maxFrequency });
  }

  spanRangeElement.textContent = `${range} Hz (${maxFrequency} Hz)`;
});

rangeThresholdElement.addEventListener(&apos;input&apos;, (event) =&gt; {
  threshold = event.currentTarget.valueAsNumber;

  if (processor) {
    processor.port.postMessage({ threshold });
  }

  spanThresholdElement.textContent = threshold.toFixed(2);
});</code></pre>
            <div class="app-container app-vocal-canceler">
              <div class="app-headline">
                <label for="file-vocal-canceler">Upload Audio File</label>
                <input type="file" id="file-vocal-canceler" accept="audio/*" />
                <audio id="audio-vocal-canceler" controls></audio>
              </div>
              <div>
                <dl>
                  <div>
                    <dt><label for="range-vocal-canceler-depth">depth</label></dt>
                    <dd>
                      <input type="range" id="range-vocal-canceler-depth" disabled value="0" min="0.5" max="1" step="0.05" /><span
                        id="print-vocal-canceler-depth-value"
                        >0.00</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-vocal-canceler-min-frequency">minFrequency</label></dt>
                    <dd>
                      <input type="range" id="range-vocal-canceler-min-frequency" disabled value="200" min="100" max="16000" step="100" /><span
                        id="print-vocal-canceler-min-frequency-value"
                        >200 Hz</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-vocal-canceler-range">range (maxFrequency)</label></dt>
                    <dd>
                      <input type="range" id="range-vocal-canceler-range" disabled value="7800" min="100" max="16000" step="100" /><span
                        id="print-vocal-canceler-range-value"
                        >7800 Hz (8000 Hz)</span>
                    </dd>
                  </div>
                  <div>
                    <dt><label for="range-vocal-canceler-threshold">threshold</label></dt>
                    <dd>
                      <input type="range" id="range-vocal-canceler-threshold" disabled value="0.5" min="0" max="1" step="0.05" /><span
                        id="print-vocal-canceler-threshold-value"
                        >0.50</span>
                    </dd>
                  </div>
                </dl>
              </div>
            </div>
            <p>
              スペクトル領域のボーカルキャンセラでも, 音源によってはあまり除去できない場合もあるかと思います. また,
              ボーカルの周波数帯域と重なるような楽器音などがある場合は, その楽器音も除去されてしまいます. 実は, ボーカルキャンセラを含めた, <b>音源分離</b>は,
              まだ完璧なアプローチがない, 研究対象のテーマであり, 今後は, 人工知能の発展とともに, 信号処理のアプローチだけでなく,
              機械学習や深層学習のアプローチも取り入れられて発展していくと思われます.
            </p>
          </section>
        </section>
        <section id="section-effectors-connection-order">
          <h3>エフェクターの接続順</h3>
          <p>
            ここまでのセクションで様々なエフェクター (空間系, モジュレーション系, フィルタ系, 歪み系など) を解説してきました. 最後に, 解説しておくこととして, 1
            つのエフェクーだけを利用する場合は考慮する必要のない問題ですが, 多くの場合, 複数のエフェクターを同時に利用することになるでしょう.
            <a href="#section-effectors-amp-simulator-and-effectors-distortion">アンプシミュレーターと歪み系エフェクターによるディストーションサウンド</a>のセクションでも記載しましたが, <b>エフェクターの接続順は出力されるサウンドに大きく影響を与えます</b>.
          </p>
          <p>
            もちろん, 仕様的に決まりがあるわけではないので, 利用するエフェクターの個数の階乗 (<span class="math-inline">$n!$</span>)
            も接続順が存在することになります. しかしながら, <b>エフェクターの接続順にはおおよその定石があります</b>.
          </p>
          <p>
            このセクションでは, ローランド社のマルチエフェクターの ME シリーズである,
            <a href="https://www.boss.info/jp/products/me-70/" target="_blank" rel="noopener noreferrer">ME-70</a> と
            <a href="https://www.boss.info/jp/products/me-80/" target="_blank" rel="noopener noreferrer">ME-80</a> にもとづいて,
            エフェクターの接続順の定石を解説します (いずれの機種も, 生産・販売終了していますが, コンパクトエフェクター並べたような UI で直感的で使えるのと,
            デジタルすぎないサウンドメイクができることから, まだまだ現役で利用されているギタリストは多くいらっしゃいます).
          </p>
          <ol>
            <li>入力 (ギター, ベースなど)</li>
            <li>ワウ, Whammy など飛び道具系</li>
            <li>コンプレッサー</li>
            <li>歪み系エフェクター (ブースター, オーバードライブ, ファズなど</li>
            <li>アンプシミュレーター (もしくは, 実機のアンプにリターン接続) とイコライザー</li>
            <li>ノイズリダクション (ノイズサプレッサー)</li>
            <li>モジュレーション系 (コーラス・フランジャー, トレモロ, フェイザーなど)</li>
            <li>ディレイ</li>
            <li>リバーブ</li>
            <li>出力 (スピーカー, レコーダー端子など)</li>
          </ol>
          <p>実機のアンプを利用する場合などは, センド・リターン端子を利用するので接続自体は複雑になりますが, エフェクターの接続順の定石は変わりません.</p>
          <p>
            また, ME-70 と ME-80 にもとづいたエフェクターの接続順の定石を記載しましたが, ローランド社が公開している,
            <a href="https://articles.boss.info/ja/the-ultimate-guide-to-guitar-effects-pedal-order-and-signal-chain/" target="_blank" rel="noopener noreferrer"
              >ギター用エフェクトの接続順ガイド</a>も非常に参考になると思います.
          </p>
          <table>
            <caption>
              実機のマルチエフェクターの接続順例
            </caption>
            <thead>
              <tr>
                <th scope="col">Effector</th>
                <th scope="col">Description</th>
                <th scope="col" style="width: 440px">Image</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="white-space: nowrap">BOSS ME-70</td>
                <td>
                  筐体右上に, エフェクターの接続順がプリントされています. BOSS ME シリーズは, コンパクトエフェクターを並べたような UI と, ノブによる操作が主で,
                  非常に直感的なサウンドメイクが可能です. コンパクトエフェクターのパイオニアである, BOSS 製品らしいマルチエフェクターと言えます.
                </td>
                <td><img src="images/boss-me-70.png" alt="BOSS ME-70" width="3588" height="566" loading="lazy" /></td>
              </tr>
              <tr>
                <td style="white-space: nowrap"><a href="https://www.boss.info/jp/products/gt-1/" target="_blank" rel="noopener noreferrer">BOSS GT-1</a></td>
                <td>
                  ディスプレイ下のボタンの並びが一般的なエフェクターの接続順となっていますが, 右側のノブで選択したエフェクターの接続順を変更可能で,
                  ディスプレイ上の接続で確認できます
                </td>
                <td><img src="images/boss-gt-1.png" alt="BOSS GT-1" width="3900" height="2385" loading="lazy" /></td>
              </tr>
              <tr>
                <td style="white-space: nowrap"><a href="https://line6.jp/helix/helix-lt.html" target="_blank" rel="noopener noreferrer">LINE6 HELIX LT</a></td>
                <td>
                  ディスプレイにエフェクターの接続順が表示されており, その接続順を変えることが可能です. また, アイコンでエフェクターのタイプ (歪み系,
                  モジュレーション系など) が認識しやすいようになっています. エフェクターのパラメータは, ディスプレイと対応する下のノブで調整します.
                </td>
                <td><img src="images/line6-helix-lt.png" alt="LINE6 HELIX LT" width="3639" height="2939" loading="lazy" /></td>
              </tr>
              <tr>
                <td style="white-space: nowrap">
                  <a href="https://www.vintagedigital.com.au/digitech-2112/" target="_blank" rel="noopener noreferrer">DigiTech GSP 2112</a>
                </td>
                <td>
                  X JAPAN の
                  <a href="https://www.youtube.com/watch?v=E_hAH_8I0co" target="_blank" rel="noopener noreferrer">DAHLIA</a>
                  のギターパートやボーカルエフェクトにも利用されたことでも有名な
                  <a href="https://www.tube-tester.com/sites/gsp2101/sites/gsp2101main.htm" target="_blank" rel="noopener noreferrer">DigiTech GSP 2101</a>
                  の後継機種で, 1997 年に発売された, ラック式の真空管プリアンプ &amp; マルチエフェクターです.
                  ディスプレイにエフェクターの接続順や設定値が表示されていますが, GSP 2101 と比較すると,
                  ディスプレイの表示と設定するパラメータのコントロールが劇的にわかりやすくなっています. また, プリセットを MIDI
                  フットコントローラーでも切り替え可能です. Web MIDI API によってこのような MIDI 機器からの MIDI
                  メッセージをブラウザでも受信することが可能になります. (かなり, 個人的な見解ですが, X JAPAN の
                  <a href="https://www.youtube.com/watch?v=E_hAH_8I0co" target="_blank" rel="noopener noreferrer">DAHLIA</a> は, エフェクターを限界まで駆使した
                  (スラッシュメタルな歪みと GSP 2101 のクリーンサウンド, 空間系, ワウ, Whammy, スライサー, サスティナーなど多彩なエフェクターと, 20
                  トラック以上のギターパートやオーケストラのミキシング・マスタリング技術), まさに, Music と Technology が融合した Art と言える名曲です. この
                  Technology 側を継承・創造してくことが, ドキュメント制作のモチベーションでもあります). ちなみに, GSP 2112 は, hide
                  さんが最後に購入した機種とも言われています.
                </td>
                <td><img src="images/digitech-gsp-2112.png" alt="DigiTech GSP 2112" width="3705" height="1332" loading="lazy" /></td>
              </tr>
            </tbody>
          </table>
          <p>
            もっとも, あくまで定石であって, 必ずしも上記の接続順である必要はありません. エフェクターの接続順を学び,
            創造していくのは「守破離」の概念と似ています. 「守」はまず基本や定石を学ぶこと, 知ること. 「破」は「守」を基に, 自分の好みに合う接続にしたり,
            エフェクター個々の特性に合わせて, エフェクターの接続順をカスタマイズしたりしていくこと. 「離」は, 「守」「破」を経験したうえで,
            まだこの世の中にはないようなサウンドエフェクトを創造したり, アンチパターンと思われていたことをあえて利用できるジャンルを創造したりしていくことです.
          </p>
        </section>
        <section id="section-effectors-conclusion">
          <h3>Web Audio API におけるエフェクターの実装のまとめ</h3>
          <p>
            エフェクターのセクションはかなり長く, 理解が必要なオーディオ信号処理も多かったと思いますが (このサイトのオーナーも執筆に約 1 年間を要しました ...),
            これまでのセクションで記載したエフェクターは, エフェクターのタイプ (空間系, モジュレーション系, フィルタ系, 歪み系, 飛び道具系など)
            のなかで基本的なエフェクターです.
          </p>
          <p>
            アプリケーションのユースケースによっては, これだけでも十分な場合もあるかもしれませんが, 現実世界のエフェクターは, 例えば,
            ディレイだけでもいくつかの種類があります (アナログディレイやテープディレイのシミュレート, ピンポンディレイやリバースディレイ,
            コーラス風のディレイなど).
          </p>
          <p>
            現実世界に存在してるエフェクターや, まだ存在していない未知のエフェクターを創造していく場合でも,
            まずはこのドキュメントで記載した基本を理解しておくことは重要です. 特に, 実装が複雑に思えるエフェクターでも, <code>AudioNode</code> の接続と,
            <code>AudioParam</code> のオートメーションなどで実装できないかを検討してみることが Web Audio API で実装するうえでは重要です. また, AudioWorklet
            が必要なエフェクターもいくつか記載しましたが, 直接オーディオデータにアクセスできる限り, 実装できないエフェクーはかなり少ないでしょう.
          </p>
          <p>
            このセクションで記載した基本を理解して, ぜひ, 未知のエフェクターを創造してください. あるいは, アナログ回路に深い理解がある場合,
            好きな真空管アンプの回路を研究して, アンプシミュレーターを開発してみるのも楽しいと思います. 「楽しみ」と「創造」は無限です.
            少しでもそのお役に立てれば幸いです.
          </p>
        </section>
      </section>
      <section id="section-spatialization-audo">
        <h2>3D オーディオ (立体音響)</h2>
        <p>
          Web Audio API では, 3D オーディオ (立体音響, 仕様では, 「Spatialization 」という単語が適用されています) でも, 実装を抽象化するクラスがあります.
          <b><code>PannerNode</code></b> クラスと <b><code>AudioListener</code></b> クラスです.
        </p>
        <p>
          <code>PannerNode</code> クラスは <code>AudioNode</code> のサブクラスで, <b>音源の空間的なパラメータ</b>が定義されています.
          <code>AudioListener</code> クラスは, <code>AudioContext</code> インスタンスの <b>listener</b> プロパティに,
          シングルトンとしてインスタンスが設定されています. こちらは, <b>音を聴く側の空間的なパラメータ</b>が定義されています (<code>AudioListener</code>
          インスタンスは生成できません. <code>AudioContext</code> インスタンスにあるシングルトンの <code>AudioListener</code> インスタンスを参照して利用します).
          命名にも表れていますが, <code>AudioListener</code> クラスは <code>AudioNode</code> のサブクラスではないことに注意してください. さらに,
          <code>AudioListener</code> の設定だけを変更することはできず (変更しても, 値が有効になりません),
          <b>1 つ以上の <code>PannerNode</code> インスタンスを生成して, 他の <code>AudioNode</code> に接続することで有効になります</b>.
        </p>
        <p>
          Web Audio API の 3D オーディオ, つまり, <code>PannerNode</code> と <code>AudioListener</code> の仕様の多くは,
          <a href="https://openal.org/" target="_blank" rel="noopener noreferrer">OpenAL</a> にもとづいています. したがって, OpenAL に理解がある場合, 比較的,
          Web Audio API の 3D オーディオも理解しやすいと思います.
        </p>
        <section id="section-spatialization-x-y-x">
          <h3>3D オーディオの座標系</h3>
          <p>
            <code>PannerNode</code> と <code>AudioListener</code> の各プロパティを理解するためには, Web Audio API における 3D
            空間の座標系の定義を理解しておく必要があります.
          </p>
          <dl>
            <dt>x 軸</dt>
            <dd>x 軸は横方向 (左右) の座標を表します. x 軸の値が大きいほど右側に音源やリスナーが位置することになります</dd>
            <dt>y 軸</dt>
            <dd>
              y 軸は高さ方向の座標を表します. y 軸の値が大きいほど高い位置に音源やリスナーが位置することになります. 数学や物理においては, z
              軸で高さ方向の座標を表すことが多いですが, Web Audio API の 3D オーディオの座標系では, y 軸が高さ方向なので注意してください
            </dd>
            <dt>z 軸</dt>
            <dd>
              z 軸は奥行方向の座標を表します. z 軸の値が大きいほど前方に音源やリスナーが位置することになります. x 軸と z 軸の座標によって, (理論上は)
              すべての平面上の音源やリスナーの位置を決定することができます (もちろん, 数値型には最大・最小値があるので, 厳密にはすべてではありませんが)
            </dd>
          </dl>
          <figure>
            <svg id="svg-figure-3-dimensional-coordinate" width="600" height="600" />
            <figcaption>Web Audio API の 3D オーディオの座標系</figcaption>
          </figure>
        </section>
        <section id="section-spatialization-vector">
          <h3>ベクトル</h3>
          <p>
            <code>PannerNode</code> や <code>AudioListener</code> のプロパティには, <b>ベクトル</b> (3 つのプロパティで, 3 次元のベクトルを表現)
            を意味するプロパティがあります. ベクトルに関しては, 数学や物理と同じなので, 高校数学や大学などでベクトル, あるいは,
            線形代数を学んで理解してればスキップして問題ありません.
          </p>
          <p>
            <b>ベクトル</b>とは, <b>大きさ</b>と<b>向き</b>をもつ物理量, あるいは, それを抽象化した概念のことです. ベクトルの代表例としては, 速度,
            重力や電磁気力などの力があります. それに対して, スカラーとは大きさのみで向きの概念をもたない物理量ことです. スカラーの代表例としては,
            速さや質量などがあります. また, 音の特性に関連する物理量 (振幅・周波数・周期) や, Web Audio API の <code>AudioParam</code> (ゲインやディレイタイム,
            フィルタのカットオフ周波数) も向きの概念をもたず, 大きさのみをもつスカラーです. そして, のちほど解説しますが, 例えば, <code>PannerNode</code> の 3
            つのプロパティ, <code>orientationX</code>, <code>orientationY</code>, <code>orientationZ</code> は, それぞれはスカラーですが, 3
            つのプロパティを合わせて, 大きさと向きを表現するベクトルとなります.
          </p>
          <p>
            このセクションでは理解の容易さを優先して, 2 次元のベクトルのみを解説しますが, 本質を理解できれば, 3 次元にもそのまま応用できます (ちなみに,
            理論物理学の 1 つである, ひも理論では, 空間の次元は本来 10 次元存在するとされています. まだ, 実証されているわけではないので,
            真実かどうかはわかりませんが). ベクトルでは当たり前のように解説される内積や外積などは省略して, あくまで, Web Audio API における 3D オーディオの理解
            (<code>PannerNode</code> と <code>AudioListener</code>) を目的に最小限の解説をします (興味があれば,
            ベクトルに関する詳細なドキュメントや書籍があるので, それらを参照してください).
          </p>
          <p>
            ベクトルは大きさと向きをもつので, 2 次元以上のベクトルでは, スカラーのように単一の値で表現することができません. したがって,
            <b>必ず次元の数だけ要素が存在します</b>. プログラミング言語で実装する場合, ベクトルは, 配列のようなコレクション, あるいは, もっと型に制約をつけた,
            <b>タプル</b>となるでしょう (<code>new Float32Array(2)</code>, <code>[number, number]</code> 型など).
          </p>
          <p>したがって, 2 次元ベクトルの場合, 要素数は 2 つです. ここで, 2 次元ベクトルを表現するモデル (クラス) を実装すると以下のようになるでしょう.</p>
          <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">class Vector2 {
  constructor(x, y) {
    this.vectors = [x, y];
  }

  get x() {
    return this.vectors[0];
  }

  get y() {
    return this.vectors[1];
  }
}</code></pre>
          <p>
            先に解説したように, Web Audio API の 3D オーディオにおいては, x - z 軸が 2 次元平面を表しますが, ベクトルの解説においては, 数学や物理,
            一般にしたがって, x - y 軸を 2 次元平面とします.
          </p>
          <p>
            x 座標の値と, y 座標の値をそれぞれ 1 つずつ決定すれば, 平面上の任意の点の位置が決まります. そして, 原点
            <code>(0, 0)</code> とこの点を直線で結ぶことができます. この直線の, <b>原点から 2 次元上の任意の点へ向かう方向</b>
            <span class="math-inline">$\left(\overrightarrow{OA}\right)$</span> がイメージとしての向きです. これが, 逆だと向きは異なり, 逆ベクトルと呼ばれます
            <span class="math-inline">$\left(\overrightarrow{AO}\right)$</span>.
          </p>
          <p>
            もちろん, ベクトルの演算においては, ラジアンなどで定量的に向きを表します. 2 次元上の任意の点と原点との距離がベクトルの大きさになります. また, x
            軸とベクトルのなす角がベクトルの向きとなります (説明の便宜上, ベクトルの端点の 1 つを原点に設定していますが, 実際には, 2
            次元上の任意の点を設定できます). 大きさを求めるには, 任意の点の距離を<b>三平方の定理</b> (<b>ピタゴラスの定理</b>) を使うことで算出できます. 向きは,
            ほとんどのプログラミング言語の, 数学関数に定義されている <b><code>atan2(y, x)</code></b> 関数を使うことで, ラジアン単位で取得することができます
            (ただし, この引数 <code>y</code>, <code>x</code> は原点からの座標である必要があります).
          </p>
          <p>2 次元ベクトルの大きさとなす角 (向き) を算出するメソッドを追加実装した <code>Vector2</code> クラスの実装は以下のようになります.</p>
          <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">class Vector2 {
  constructor(x, y) {
    this.vectors = [x, y];
  }

  get x() {
    return this.vectors[0];
  }

  get y() {
    return this.vectors[1];
  }

  get scalar() {
    const [x, y] = this.vectors;

    return Math.sqrt((x ** 2) + (y ** 2));
  }

  get radian() {
    const [x, y] = this.vectors;

    return Math.atan2(y, x);
  }
}</code></pre>
          <p>以下は, 比較的よく知られている直角三角形を構成するベクトルの x, y 成分をコンストラクタの引数にした場合の, 大きさとなす角 (向き) の例です.</p>
          <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">// 1 : 1 : √2
const v = new Vector2(1, 1);

console.log(v.scalar);  // 1.4142135623730951 ... ≒ √2
console.log(v.radian);  // 0.7853981633974483 ... ≒ π/4</code></pre>
          <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">// 1 : √3 : 2
const v = new Vector2(1, Math.sqrt(3));

console.log(v.scalar);  // 1.9999999999999998 ... ≒ 2
console.log(v.radian);  // 1.0471975511965976 ... ≒ π/3</code></pre>
          <pre data-prismjs-copy="クリップボードにコピー" data-prismjs-copy-success="コピーしました"><code class="language-js line-numbers">// 3 : 4 : 5
const v = new Vector2(3, 4);

console.log(v.scalar);  // 5
console.log(v.radian);  // 0.9272952180016122 ... ≒ acos(3/5) ≒ 53.13°</code></pre>
          <p>
            以下は, 2 次元平面を表す SVG ですが, この 2 次元平面上をカーソル移動すると, 原点からカーソルの座標に向かうベクトルを表示します.
            そのベクトルを構成する, x 成分のベクトルと y 成分のベクトルも表示します (これらは, 基底ベクトルと呼ばれます). そして, ベクトルの大きさと向き
            (なす角) を数値で表示するようにしています.
          </p>
          <p>
            このぐらいが理解できていれば, Web Audio API における 3D オーディオ, つまり, <code>PannerNode</code> と <code>AudioListener</code> を理解するための,
            ベクトルの理解は十分だと思います.
          </p>
          <figure>
            <div class="flexbox">
              <svg id="svg-animation-vectors" width="600" height="600" style="cursor: move" />
              <ul>
                <li>x = <span id="print-vector-x-value">0</span></li>
                <li>y = <span id="print-vector-y-value">0</span></li>
                <li>Scalar = <span id="print-vector-scalar-value">0</span></li>
                <li>Radian = <span id="print-vector-radian-value">0</span></li>
              </ul>
            </div>
            <figcaption>2 次元ベクトル</figcaption>
          </figure>
          <article id="section-spatialization-vector-atan2">
            <h4>atan2 を使う理由</h4>
            <p>
              ここまでのセクションで, 複素平面上の実部と虚部 (<span class="math-inline">$z = x + jy$</span>) から偏角を求める場合や, ベクトルのなす角 (向き)
              を求める場合に, <code>atan2</code> 関数を利用してきました. 少し疑問にもたれたかもしれませんが, 座標に相当する値から, その角度を求めるのであれば,
              <code>atan</code> 関数, つまり, 逆正接関数 (<span class="math-inline">$\tan^{-1}\left(\frac{y}{x}\right)$</span>)
              でもよいのではないかという疑問です. 数学的には, <code>atan</code> 関数を利用することも正しいのですが, <code>atan</code> の返すラジアンは,
              <span class="math-inline">$-\frac{\pi}{2} &le; \tan^{-1}\left(\frac{y}{x}\right) &le; \frac{\pi}{2}$</span> となるので,
              <span class="math-inline">$2\pi$</span> の範囲の角度を算出する必要がある場合, x, y ごとに場合分けをする必要があり, さらに, x は
              <code>0</code> による除算も防ぐように考慮する必要があります. もちろん, 地道にこの場合分けをして, 角度を求めてもよいのですが,
              ほとんどのプログラミング言語には, この場合分けを必要とせず, 座標から <span class="math-inline">$2\pi$</span> の範囲の角度 (<span
                class="math-inline"
                >$-\pi &le; \mathrm{atan2}\left(y, x\right) &le; \pi$</span>) をラジアン単位で取得できる <code>atan2</code> 関数が実装されているので, そちらを利用したほうが,
              実装コストやレビューコストが圧倒的に少なくて済むというメリットが大きいからです (逆に, デメリットを見つけるほうが難しいかもしれません.
              強いて言えば, <code>atan2</code> 関数が利用できないプログラミング言語を使う場合でしょうか ...).
            </p>
          </article>
        </section>
        <section id="section-spatialization-audio-panner-node">
          <h3>PannerNode</h3>
          <p>
            <code>PannerNode</code> インスタンスを生成するには, コンストラクタ呼び出しか <b><code>createPanner</code></b> メソッドを呼び出します.
            コンストラクタ形式であれば, 第 2 引数に, <b><code>PannerOptions</code></b> 型で初期化時に,
            <code>PannerNode</code> のプロパティを設定することができます.
          </p>
          <p>
            概要でも解説しましたが, <code>PannerNode</code> インスタンスを他の <code>AudioNode</code> に接続することによって,
            <code>AudioListener</code> インスタンスの値が 3D オーディオに影響するようになります.
          </p>
          <figure>
            <svg id="svg-figure-node-connections-for-panner-node" width="400" height="520" />
            <figcaption><code>PannerNode</code> のノード接続図</figcaption>
          </figure>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const panner     = new PannerNode(context, { positionX: 5 });

// If use `createPanner`
// const panner = context.createPanner();
//
// panner.positionX.value = 5;

// OscillatorNode (Input) -&gt; PannerNode (Panner) -&gt; AudioDestinationNode (Output)
// Then, the instance of `AudioListener` is available
oscillator.connect(panner);
panner.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);</code></pre>
          <img src="images/panner-node.png" alt="PannerNode" width="1232" height="770" loading="lazy" />
          <p><code>PannerNode</code> のプロパティは多いので, まずは, すべてをリストアップして概要を記載します.</p>
          <table class="auto-table">
            <caption>
              PannerNode のプロパティ
            </caption>
            <thead>
              <tr>
                <th scope="col">Property</th>
                <th scope="col">Description</th>
                <th scope="col">Default</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>panningModel</td>
                <td>空間音響のアルゴリズム</td>
                <td><code>&apos;equalpower&apos;</code></td>
              </tr>
              <tr>
                <td>distanceModel</td>
                <td>音がリスナーに伝達する際の音量減衰のアルゴリズム</td>
                <td><code>&apos;inverse&apos;</code></td>
              </tr>
              <tr>
                <td>positionX</td>
                <td>音源の x 座標</td>
                <td><code>0</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>positionY</td>
                <td>音源の y 座標</td>
                <td><code>0</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>positionZ</td>
                <td>音源の z 座標</td>
                <td><code>0</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>orientationX</td>
                <td>音源の向き (ベクトルの x 成分)</td>
                <td><code>1</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>orientationY</td>
                <td>音源の向き (ベクトルの y 成分)</td>
                <td><code>0</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>orientationZ</td>
                <td>音源の向き (ベクトルの z 成分)</td>
                <td><code>0</code> (<code>AudioParam</code> の <code>value</code>)</td>
              </tr>
              <tr>
                <td>refDistance</td>
                <td>音量減衰の算出に影響する</td>
                <td><code>1</code></td>
              </tr>
              <tr>
                <td>maxDistance</td>
                <td>音量減衰の算出に影響する</td>
                <td><code>10000</code></td>
              </tr>
              <tr>
                <td>rolloffFactor</td>
                <td>音量減衰の算出に影響する</td>
                <td><code>1</code></td>
              </tr>
              <tr>
                <td>coneInnerAngle</td>
                <td>音の指向性に影響する</td>
                <td><code>360</code></td>
              </tr>
              <tr>
                <td>coneOuterAngle</td>
                <td>音の指向性に影響する</td>
                <td><code>360</code></td>
              </tr>
              <tr>
                <td>coneOuterGain</td>
                <td>音の指向性に影響する</td>
                <td><code>0</code></td>
              </tr>
            </tbody>
          </table>
          <section id="section-spatialization-audio-panner-node-panning-model">
            <h4>panningModel プロパティ</h4>
            <p>
              <b><code>panningModel</code></b> プロパティは, 空間音響アルゴリズムを決定するプロパティです. もう少し平易なテキストで説明すると, どれぐらい忠実に
              3D オーディオ空間, つまり, 実際の音響空間をシミュレートするかを決定するプロパティです. 指定可能なアルゴリズム (<b
                ><code>PanningModelType</code></b>
              列挙型) は 2 種類です.
            </p>
            <table>
              <caption>
                PanningModelType
              </caption>
              <thead>
                <tr>
                  <th scope="col">Value</th>
                  <th scope="col">Description</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>&apos;equalpower&apos;</code></td>
                  <td>各チャンネルに対して等しく音を伝達する (デフォルト値)</td>
                </tr>
                <tr>
                  <td><code>&apos;HRTF&apos;</code></td>
                  <td>人の頭部 (頭や耳) が音の伝達に影響している状態, つまり, <b>HRTF</b> (<b>頭部伝達関数</b>) を利用する</td>
                </tr>
              </tbody>
            </table>
            <p>
              <code>&apos;equalpower&apos;</code> よりも <code>&apos;HRTF&apos;</code> のほうがより高度な音響空間のアルゴリズムなので, (理論上は)
              <code>&apos;HRTF&apos;</code>のほうがより忠実に実際の音響空間をシミュレートします.
            </p>
          </section>
          <section id="section-spatialization-audio-panner-node-distance-model">
            <h4>distanceModel プロパティ</h4>
            <p>
              <b><code>distanceModel</code></b> プロパティは, 音がリスナーに伝達するまでの音量減衰のアルゴリズムを決定するプロパティです. 一般的な音響空間では,
              近くの音は大きく, 遠くの音は小さく知覚されるはずです. すなわち, その音量減衰現象を Web Audio API でシミュレートするためのアルゴリズムを決定します.
              指定可能なアルゴリズム (<b><code>DistanceModelType</code></b> 列挙型) は 3 種類です. また, それぞれのアルゴリズにおいて,
              減衰の算出式が定義されています.
            </p>
            <p>
              <span class="math-inline">$f$</span> は, <code>rolloffFactor</code> プロパティ, <span class="math-inline">$d$</span> は, 音源からの距離 (<code
                >positionX</code>
              などから算出), <span class="math-inline">$d_{\mathrm{max}}$</span> は, <code>maxDistance</code> プロパティ,
              <span class="math-inline">$d_{\mathrm{ref}}$</span> は, <code>refDistance</code> プロパティです.
            </p>
            <table class="nowrap-table">
              <caption>
                DistanceModelType
              </caption>
              <thead>
                <tr>
                  <th scope="col">Value</th>
                  <th scope="col">Description</th>
                  <th scope="col">Formula</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>&apos;linear&apos;</code></td>
                  <td>線形減衰</td>
                  <td>
                    <span class="math-inline"
                      >$1 - f\frac{\mathrm{max}\left[\mathrm{min}\left(d, d_{\mathrm{max}}'\right), d_{\mathrm{ref}}'\right] -
                      d_{\mathrm{ref}}'}{d_{\mathrm{max}}' - d_{\mathrm{ref}}'}$</span>
                  </td>
                </tr>
                <tr>
                  <td><code>&apos;inverse&apos;</code></td>
                  <td>逆数減衰 (デフォルト値)</td>
                  <td>
                    <span class="math-inline"
                      >$\frac{d_{\mathrm{ref}}}{d_{\mathrm{ref}} + f\left[\mathrm{max}\left(d, d_{\mathrm{ref}}\right) - d_{\mathrm{ref}}\right]}$</span>
                  </td>
                </tr>
                <tr>
                  <td><code>&apos;exponential&apos;</code></td>
                  <td>指数減衰</td>
                  <td><span class="math-inline">$\left[\frac{\mathrm{max}\left(d, d_{\mathrm{ref}}\right)}{d_{\mathrm{ref}}}\right]^{-f}$</span></td>
                </tr>
              </tbody>
            </table>
          </section>
          <section id="section-spatialization-audio-panner-node-position-xyz">
            <h4>positionX / positionY / positionZ プロパティ</h4>
            <p>
              音源 (パン) の位置を設定するプロパティで, <code>AudioParam</code> 型です. デフォルト値は, いずれも <code>0</code> なので, 原点 (<code
                >(0, 0, 0)</code>) に音源が位置することになります.
            </p>
            <p>
              初期の仕様では, これらのプロパティは <code>AudioParam</code> 型ではなかったので,
              <b><code>setPosition(x, y, z)</code></b> メソッドで音源の位置を設定する必要がありました. 最新の仕様でも,
              <code>setPosition</code> メソッドは残っていますが, <b>非推奨</b>となっているので利用しないほうがよいでしょう.
            </p>
            <figure>
              <svg id="svg-figure-panner-node-position" width="800" height="400" />
              <img src="images/pan.svg" alt="音源の位置 (デフォルト値)" width="32" height="32" loading="lazy" style="transform: translate(-590px, -188px)" />
              <img src="images/pan.svg" alt="音源の位置 (-1, 1, 1)" width="32" height="32" loading="lazy" style="transform: translate(-320px, -230px)" />
              <figcaption>positionX / positionY / positionZ プロパティ (左は, デフォルト値である原点に位置しています)</figcaption>
            </figure>
            <article id="section-spatialization-audio-panner-node-position-xyz-vs-stereo-panner-node">
              <h5>PannerNode の positionX と StereoPannerNode</h5>
              <p>
                すでに気づかれたかもしれませんが, <code>PannerNode</code> の <code>positionX</code> プロパティは, <code>AudioParam</code> 型なので, ここに LFO
                を接続することでもオートパンを実装することは可能です. しかしながら, 左右に (x 軸方向に) 音源を振るだけであれば,
                <code>StereoPannerNode</code> を使うべきです. <code>PannerNode</code> は, あくまで 3D オーディオをユースケースとしているので,
                <code>positionX</code> を変更するだけでも, <code>StereoPannerNode</code> よりも CPU の負荷が大きく, パフォーマンスの点で不利となるからです.
              </p>
            </article>
          </section>
          <section id="section-spatialization-audio-panner-node-orientation-xyz">
            <h4>orientationX / orientationY / orientationZ プロパティ</h4>
            <p>
              音源 (パン) の向きを設定するプロパティで, <code>AudioParam</code> 型です.
              <b>デフォルト値は, <code>(1, 0, 0)</code> なので, x 軸の正の方向 (リスナーの初期位置から見て, 右側方向) を向いています</b>.
              <b>ベクトルのなす角のみを決定するので, x, y, z それぞれの値の比率が重要となります</b>. 例えば, 初期値をいくら乗算しても, x
              軸の方向のどちらかを向くだけです.
            </p>
            <p>
              音源の向き (なす角) を y 軸や, z 軸方向にも向ける例として, <code>(1, -√2, 1)</code> とすると, 平面手前向きに
              <span class="math-inline">$\frac{\pi}{4}$</span>, 高さ下方向にも <span class="math-inline">$\frac{\pi}{4}$</span> の方向を向いた音源に設定できます
              (ベクトルの大きさは, <code>positionX</code> / <code>positionY</code> / <code>positionZ</code> によって決定されます).
            </p>
            <figure>
              <svg id="svg-figure-panner-node-orientation" width="800" height="400" />
              <img src="images/pan.svg" alt="音源の向き (デフォルト値)" width="32" height="32" loading="lazy" style="transform: translate(-590px, -188px)" />
              <img
                src="images/pan.svg"
                alt="音源の向き (1, -√2, 1)"
                width="32"
                height="32"
                loading="lazy"
                style="transform: translate(-260px, -188px) rotateZ(45deg)"
              />
              <figcaption>orientationX / orientationY / orientationZ プロパティ (左は, デフォルト値である x 軸の正の方向を向いています)</figcaption>
            </figure>
            <p>
              初期の仕様では, これらのプロパティは <code>AudioParam</code> 型ではなかったので,
              <b><code>setOrientation(x, y, z)</code></b> メソッドで音源の向きを設定する必要がありました. 最新の仕様でも,
              <code>setOrientation</code> メソッドは残っていますが, <b>非推奨</b>となっているので利用しないほうがよいでしょう.
            </p>
          </section>
          <section id="section-spatialization-audio-panner-node-reduce-volume">
            <h4>refDistance / maxDistance / rolloffFactor プロパティ</h4>
            <p>音量減衰を算出するためのプロパティです (すでに解説しましたが, アルゴリズムは <code>DistanceModelType</code> ごとに異なります).</p>
            <p>
              最も音量減衰への影響が大きいのは, <b><code>refDistance</code></b> プロパティです. <code>refDistance</code> プロパティのデフォルト値は
              <code>1</code> です.
            </p>
            <p>
              <b><code>maxDistance</code></b> プロパティは, 音量減衰の限界となる距離を決定するプロパティで, この値を超える距離まで音源とリスナーが離れると,
              それ以上の音量減衰は発生しません. <code>maxDistance</code> プロパティのデフォルト値は <code>10000</code> です.
            </p>
            <p>
              <b><code>rolloffFactor</code></b> プロパティは, 音量減衰の速さを決定するプロパティで, この値が小さいほど緩やかに音量減衰して,
              大きいほど急激に音量減衰します. <code>rolloffFactor</code> プロパティのデフォルト値は <code>1</code> です.
            </p>
          </section>
          <section id="section-spatialization-audio-panner-node-sound-cone">
            <h4>coneInnerAngle / coneOuterAngle / coneOuterGain プロパティ</h4>
            <p>
              これらのプロパティは, <b>サウンドコーン</b>と呼ばれる, 指向性のラウドネスを記述するモデルに関連するので, まずは, サウンドコーンについて解説します.
            </p>
            <section id="section-spatialization-audio-panner-node-sound-cone-about-sound-cone">
              <h5>サウンドコーン</h5>
              <p>
                向きをもたないパン (<b>無指向性の音源</b>) は, どの向きにおいても距離に応じて音量が決まります. 向きをもつパン (<b>指向性をもつ音源</b>) は,
                向きに応じて音量が変化します. この, 指向性をもつ音源の音量を表すモデルがサウンドコーンです. サウンドコーンは,
                <b>内部コーン</b>と<b>外部コーン</b>によって構成されます.
              </p>
              <figure>
                <svg id="svg-figure-sound-cone" width="800" height="600" />
                <figcaption>サウンドコーン (左は無指向性音源の場合, 右は指向性音源の場合)</figcaption>
              </figure>
              <p>
                より詳細な解説は, Windows DirectX XAudio2 API の
                <a href="https://learn.microsoft.com/en-us/windows/win32/xaudio2/sound-cones" target="_blank" rel="noopener noreferrer">Sound Cones</a>
                が参考になるかもしれません.
              </p>
            </section>
            <p>
              <b><code>coneInnerAngle</code></b> プロパティは, 内部コーンの範囲を決定するプロパティです. 内部コーンの内側では音量減衰が発生しません.
              <code>coneInnerAngle</code> プロパティのデフォルト値は <code>360</code> (ラジアンではなく, 度数) です.
            </p>
            <p>
              <b><code>coneOuterAngle</code></b> プロパティは, 外部コーンの範囲を決定するプロパティです. 内部コーンの外側で, かつ, 外部コーンの内側では
              <code>coneOuterGain</code> プロパティで設定された音量減衰の減衰率に徐々に近づくように音量減衰が発生します. そして, 外部コーンの外側では
              <code>coneOuterGain</code> プロパティで設定された値に応じて, 常に一定量の音量減衰が発生します.
              <code>coneOuterAngle</code> プロパティのデフォルト値は <code>360</code> (ラジアンではなく, 度数) です.
            </p>
            <p>
              <b><code>coneOuterGain</code></b> プロパティは, 外部コーンの外側における音量減衰の減衰率を決定するプロパティです.
              <code>coneOuterGain</code> プロパティのデフォルト値は <code>0</code> です.
            </p>
            <p>
              サウンドコーンを決定するこれら 3 つのプロパティがデフォルト値の場合, パンは向きをもちません (つまり, 無指向性の音源となります). したがって,
              <code>PannerNode</code> の <code>orientationX</code> / <code>orientationY</code> / <code>orientationZ</code> で指定するパンの向きは 3D
              オーディオに影響しなくなります.
            </p>
          </section>
        </section>
        <section id="section-spatialization-audio-audio-listener">
          <h3>AudioListener</h3>
          <p>
            <code>AudioListener</code> インスタンスを利用するには, <code>Panner</code> インスタンスを生成した, 他の
            <code>AudioNode</code> に接続することによって, <code>AudioContext</code> インスタンスの <code>listener</code> プロパティでアクセス可能になります.
          </p>
          <p>
            概要でも解説しましたが, <code>PannerNode</code> インスタンスを他の <code>AudioNode</code> に接続することによって,
            <code>AudioListener</code> インスタンスの値が 3D オーディオに影響するようになります.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const panner     = new PannerNode(context);

// OscillatorNode (Input) -&gt; PannerNode (Panner) -&gt; AudioDestinationNode (Output)
// Then, the instance of `AudioListener` is available
oscillator.connect(panner);
panner.connect(context.destination);

// Set listener position
context.listener.setPosition(-5, 0, 0);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);</code></pre>
          <img src="images/audio-listener.png" alt="AudioListener" width="1232" height="770" loading="lazy" />
          <p><code>PannerNode</code> に比べると, プロパティは少ないです. リスナーの位置と向きを設定する <code>AudioParam</code> 型のプロパティのみです.</p>
          <section id="section-spatialization-audio-listener-position-xyz">
            <h4>positionX / positionY / positionZ プロパティ</h4>
            <p>
              リスナーの位置を設定するプロパティで, <code>AudioParam</code> 型です. 音源 (パン) と同様に, デフォルト値は, いずれも <code>0</code> なので, 原点
              (<code>(0, 0, 0)</code>) にリスナーが位置しています.
            </p>
            <figure>
              <svg id="svg-figure-audio-listener-position" width="800" height="400" />
              <img
                src="images/listener-default.svg"
                alt="リスナーの位置 (デフォルト値)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-590px, -188px)"
              />
              <img
                src="images/listener-default.svg"
                alt="リスナーの位置 (-5, 0, 0)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-320px, -188px)"
              />
              <figcaption>positionX / positionY / positionZ プロパティ (左は, デフォルト値である原点に位置しています)</figcaption>
            </figure>
          </section>
          <section id="section-spatialization-audio-listener-forward-xyz">
            <h4>forwardX / forwardY / forwardZ プロパティ</h4>
            <p>
              リスナーの向きを設定するプロパティで, <code>AudioParam</code> 型です.
              <b>デフォルト値は, <code>(0, 0, -1)</code> なので, z 軸の負の方向を向いています</b>. したがって, 上記のサンプルコードでは, x 軸の
              <code>-5</code> にリスナーの位置を移動させたことで, 右側から音が聴こえます. リスナーの位置は同じでも, 逆向き (<code>(0, 0, 1)</code>)
              に変更すると, 左側から音が聴こえます.
            </p>
            <p>
              また, <code>PannerNode</code> の <code>orientationX</code> / <code>orientationY</code> / <code>orientationZ</code> プロパティと同様に,
              <b>ベクトルのなす角のみを決定するので, x, y, z それぞれの値の比率が重要となります</b>.
            </p>
            <figure>
              <svg id="svg-figure-audio-listener-forward" width="800" height="400" />
              <img
                src="images/listener-default.svg"
                alt="リスナーの向き (デフォルト値)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-590px, -188px)"
              />
              <img
                src="images/listener-00-1.svg"
                alt="リスナーの向き (0, 0, 1)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-260px, -188px)"
              />
              <figcaption>forwardX / forwardY / forwardZ プロパティ (左は, デフォルト値である z 軸の負の方向を向いています)</figcaption>
            </figure>
          </section>
          <section id="section-spatialization-audio-listener-up-xyz">
            <h4>upX / upY / upZ プロパティ</h4>
            <p>
              リスナーの向きを設定するプロパティですが, リスナーの頭部の向きを設定します. ざっくりと言ってしまえば,
              上の方を見ている状態や下の方を見ている状態を設定できるということです. これらのプロパティも, <code>AudioParam</code> 型です.
              <b>デフォルト値は, <code>(0, 1, 0)</code> なので, 正面を向いている状態ということになります</b>.
            </p>
            <p>
              したがって, 例えば, <code>(1, 1, 0)</code> にすると, x 軸方向に <span class="math-inline">$\frac{\pi}{4}$</span> 傾いた y
              軸の正の方向に頭部が向くことになります.
            </p>
            <figure>
              <svg id="svg-figure-audio-listener-up" width="800" height="400" />
              <img
                src="images/listener-default.svg"
                alt="リスナーの頭部の向き (デフォルト値)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-590px, -188px)"
              />
              <img
                src="images/listener-default.svg"
                alt="リスナーの頭部の向き (1, 1, 0)"
                width="32"
                height="32"
                loading="lazy"
                style="opacity: 0.3; transform: translate(-250px, -188px) rotate(45deg)"
              />
              <figcaption>upX / upY / upZ プロパティ (左は, デフォルト値である y 軸の正の方向を向いています)</figcaption>
            </figure>
          </section>
          <article id="section-spatialization-audio-listener-set-position">
            <h4>Firefox での positionX / positionY / positionZ の設定</h4>
            <p>
              Firefox 142 の時点では, <code>positionX</code> / <code>positionY</code> / <code>positionZ</code> プロパティが実装されていないので, Firefox
              も対応ブラウザに含む場合は, 非推奨ではありますが <code>AudioListener</code> の
              <b><code>setPosition(x, y, z)</code></b> メソッドを利用する必要があります.
            </p>
          </article>
          <article id="section-spatialization-audio-listener-set-orientation">
            <h4>Firefox での forwardX / forwardY / forwardZ / upX / upY / upZ の設定</h4>
            <p>
              同じく, Firefox 142 の時点では, <code>forwardX</code> / <code>forwardY</code> / <code>forwardZ</code> プロパティ, および, <code>upX</code> /
              <code>upY</code> / <code>upZ</code> プロパティが実装されていないので, Firefox も対応ブラウザに含む場合は, 非推奨ではありますが
              <code>AudioListener</code> の <b><code>setOrientation(forwardX, forwardY, forwardZ, upX, upY, upZ)</code></b> メソッドを利用する必要があります.
            </p>
          </article>
        </section>
      </section>
      <section id="section-sound-visualization">
        <h2>音の視覚化</h2>
        <p>
          これまでの解説やデモでもすでに音の視覚化は実装していましたが, このセクションでは, オーディオ信号処理の再習をしつつ, 音の視覚化のためのクラスである
          <b><code>AnalyserNode</code></b> クラスのプロパティやメソッドを主に解説します (これらを理解するために, オーディオ信号処理の理解が必要になります).
        </p>
        <p>
          <code>AnalyserNode</code> による波形描画はあくまで, <b>リアルタイムの波形描画</b>をユースケースとしてします. 一方で, 波形エディタなどでは,
          リアルタイムの波形描画は必要なく事前の (オフラインの) 波形描画が必要になることもあります. このセクションでは,
          リアルタイム・オフラインどちらも解説します.
        </p>
        <p>
          ところで, 音を視覚化するためには, 何らかのグラフィックス API を利用する必要があります. Web Audio API の仕様においては, グラフィックス API
          までは含まれていないので, アプリケーションの要件に応じた適切なグラフィックス API を選択する必要があります.
        </p>
        <section id="section-sound-visualization-graphic-apis">
          <h3>グラフィックス API</h3>
          <section id="section-sound-visualization-graphic-apis-html-and-css">
            <h4>HTML / CSS</h4>
            <p>
              グラフィックス API ではありませんが, HTML と CSS を駆使することで, もっと言ってしまえば, HTML のボックスモデルの矩形を CSS
              で装飾することで波形描画することができます. しかしながら, 本来の利用目的とは言いがたく, また, 描画のスタイルによってはパス (線)
              で描画したいケースもあるので, その場合は対応できません.
            </p>
          </section>
          <section id="section-sound-visualization-graphic-apis-svg">
            <h4>SVG</h4>
            <p>
              HTML と同様の API, つまり, DOM で描画できるグラフィックス API が SVG です (実体は, XML なので DOM で操作できます). SVG の
              <abbr title="Multipurpose Internet Mail Extensions">MIME</abbr> タイプは <code>image/svg+xml</code> で画像データとして扱うことができます. また,
              ベクター形式なので, 拡大しても鮮明に描画できます (SVG: Scalable Vector Graphics). 描画可能なオブジェクトも, 矩形やパスだけでなく, 多角形,
              円や楕円, べジュ曲線などさまざまです (もっとも, 音の視覚化だけであれば, 最低限, 矩形とパス, テキストの描画ができれば十分でしょう).
              デメリットとしては, Canvas と比較すると DOM 操作による描画 (要素の追加や属性値の設定) なので, パフォーマンスが劣ることです.
            </p>
          </section>
          <section id="section-sound-visualization-graphic-apis-canvas">
            <h4>Canvas</h4>
            <p>
              ビットマップで <code>1px</code> 単位で RGBA の値を設定して描画することが可能で, Canvas そのものは <code>HTMLElement</code> の 1 つで,
              画像コーデックではありませんが (実体 (描画データ) は, <code>ImageData</code> クラスの <code>data</code> プロパティである
              <code>Uint8ClampedArray</code> の数値データです), JPEG や PNG など様々な画像コーデックにエクスポートすることができます. ローレイヤーな描画 (<code
                >ImageData</code>
              クラスを利用した描画) では <code>1px</code> 単位での描画が可能ですが, 抽象度の高い, 矩形やパス, 楕円, べジュ曲線, テキストなど,
              オブジェクトの描画やスタイルの設定の API も定義されています. デメリットとしては, ビットマップ形式での描画なので, 鮮明に描画するためには,
              デバイスピクセルレシオ (<code>devicePixelRatio</code> プロパティ) を考慮して座標やサイズを計算する必要があります.
            </p>
          </section>
          <section id="section-sound-visualization-graphic-apis-web-gl">
            <h4>WebGL</h4>
            <p>
              3D のグラフィックス API です. おそらく, 3D オーディオをメインに利用する Web アプリケーションでは必須になるでしょう. しかし,
              <a href="https://wgld.org/sitemap.html" target="_blank" rel="noopener noreferrer"
                >WebGL は仕様が膨大で, さらに, その基礎として CG の学習コストも大きいので</a>, このドキュメンントでは扱いません.
            </p>
            <p>音の視覚化の本質的なロジックを理解するためには, Canvas や SVG などの 2D グラフィックス API で十分だからです.</p>
            <p>
              ちなみに, WebGL の次世代の仕様として, <a href="https://www.w3.org/TR/webgpu/" target="_blank" rel="noopener noreferrer">WebGPU</a> が 2025 年 8 月
              27 日時点で Candidate Recommendation (勧告候補) の段階にあります.
            </p>
          </section>
          <table>
            <caption>
              グラフィックス API
            </caption>
            <thead>
              <tr>
                <th scope="col" style="white-space: nowrap">Graphics API</th>
                <th scope="col">Dimension</th>
                <th scope="col">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>HTML / CSS</td>
                <td>2D</td>
                <td>ベクター形式で学習コストも少ない. パスの描画が難しい</td>
              </tr>
              <tr>
                <td><a href="https://svgwg.org/svg2-draft/" target="_blank" rel="noopener noreferrer">SVG</a></td>
                <td>2D</td>
                <td>
                  ベクター形式で API も DOM なので学習コストが低い. <code>image/svg+xml</code> として画像データとして保存できる. Canvas と比較すると, DOM
                  操作による描画なのでパフォーマンスが劣る
                </td>
              </tr>
              <tr>
                <td><a href="https://html.spec.whatwg.org/#the-canvas-element" target="_blank" rel="noopener noreferrer">Canvas</a></td>
                <td>2D</td>
                <td>
                  ビットマップ形式で, オブジェクトを描画する API から, ローレイヤー (<code>ImageData</code> クラス) で <code>1px</code> ごとの RGBA を描画可能.
                  また, 様々な画像コーデックとして保存できる. デバイスピクセルレシオを考慮する必要があるのと, SVG と比べると学習コストが高い
                </td>
              </tr>
              <tr>
                <td><a href="https://registry.khronos.org/webgl/specs/latest/" target="_blank" rel="noopener noreferrer">WebGL</a></td>
                <td>2D or 3D</td>
                <td>アプリケーションの要件によっては必要になる. もっとも学習コストが高い</td>
              </tr>
            </tbody>
          </table>
          <p>このドキュメントでは, グラフィックス API として SVG と Canvas を利用します. 理由は以下の 2 つです.</p>
          <ul>
            <li>
              物理的な音の視覚化の実装に注力することで, 音響学・オーディオ信号処理の理解を深める (3D グラフィックス API は, その本質から外れる解説が多くなる)
            </li>
            <li>
              また, そのために, ユーザーに視覚的効果を与えることが主目的の, いわゆる VJ アプリは想定せず, あくまでも,
              波形・スペクトルアナライザーとしての実装を解説する
            </li>
          </ul>
          <p>描画オブジェクトも以下の 3 つができれば十分です.</p>
          <dl>
            <dt>パス</dt>
            <dd>波形の描画</dd>
            <dt>矩形</dt>
            <dd>座標 (x 軸と y 軸) の描画</dd>
            <dt>テキスト</dt>
            <dd>時間や周波数, 振幅など値の描画</dd>
          </dl>
          <p>
            もっとも, 3D グラフィックス API による描画や VJ アプリでも, 基礎となるのは 2D グラフィックスでのアナライザーとしての波形描画なので,
            これを理解しておけば, その応用で実装することができるはずです.
          </p>
        </section>
        <section id="section-sound-visualization-analyser-node">
          <h3>AnalyserNode</h3>
          <p>
            <code>AnalyserNode</code> で波形データを取得するためのは, 対象のオーディオグラフ (<code>AudioNode</code> の接続) に
            <code>AnalyserNode</code> インスタンスを接続している必要があります. <code>AnalyserNode</code> インスタンスので生成は,
            <code>AnalyserNode</code> コンストラクタ呼び出しか, <code>AudioContext</code> インスタンスの
            <b><code>createAnalyser</code></b> メソッドを呼び出す必要があります. コンストラクタ呼び出しであれば, 第 2 引数 (<b><code>AnalyserOptions</code></b>
            型) で初期値を設定することが可能です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const gain       = new GainNode(context, { gain: 0.5 });
const analyser   = new AnalyserNode(context);

// If use `createAnalyser`
// const analyser = context.createAnalyser();

// OscillatorNode (Input) -&gt; GainNode (Gain) -&gt; AnalyserNode (Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(gain);
gain.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);</code></pre>
          <p>
            1 つ注意すべきなのでは, <code>AnalyserNode</code> インスタンスの接続順です. 上記のコード例であれば,
            <code>GainNode</code> の影響を受けた波形を取得することになります. ここで, 原音の波形を取得したければ,
            <code>OscillatorNode</code> インスタンスの直後に接続する必要があります.
          </p>
          <p>
            ちなみに, <code>AnalyserNode</code> インスタンスは <code>AudioDestinationNode</code> に接続する必要はありません (接続しても問題ありません.
            このあたりはコーディング規約などで制約してください).
          </p>
          <img src="images/analyser-node.png" alt="AnalyserNode" width="1232" height="770" loading="lazy" />
          <table class="auto-table">
            <caption>
              AnalyserNode のプロパティ
            </caption>
            <thead>
              <tr>
                <th scope="col" style="white-space: nowrap">Property</th>
                <th scope="col">Description</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><code>fftSize</code></td>
                <td>
                  <code>32</code> から <code>32768</code> までの 2 の冪乗. 時間領域では 1 回のメソッド実行で取得するデータサイズで, 周波数領域では FFT サイズ.
                  デフォルト値は <code>2048</code>.
                </td>
              </tr>
              <tr>
                <td><code>frequencyBinCount</code></td>
                <td>
                  <code>fftSize</code> プロパティの <span class="math-inline">$\frac{1}{2}$</span> の値. 読み取り専用. <code>frequencyBinCount</code> は,
                  <b>ナイキスト周波数に相当するインデックス</b>.
                </td>
              </tr>
              <tr>
                <td><code>maxDecibels</code></td>
                <td>
                  振幅スペクトルのデシベル値から, <code>getByteFrequencyData</code> メソッドの値の最大値の変換に利用される. デフォルト値は
                  <code>-30</code> (dB). <code>getByteFrequencyData</code> メソッド以外には影響しないプロパティ.
                </td>
              </tr>
              <tr>
                <td><code>minDecibels</code></td>
                <td>
                  振幅スペクトルのデシベル値から, <code>getByteFrequencyData</code> メソッドの値の最小値の変換に利用される. デフォルト値は
                  <code>-100</code> (dB). <code>getByteFrequencyData</code> メソッド以外には影響しないプロパティ.
                </td>
              </tr>
              <tr>
                <td><code>smoothingTimeConstant</code></td>
                <td>スペクトルの更新フレームの頻度を設定するプロパティ. デフォルト値は <code>0.8</code>. 時間領域の波形描画には影響しない.</td>
              </tr>
            </tbody>
          </table>
          <p>
            まとめると, <code>AnalyserNode</code> のプロパティで, <code>fftSize</code> プロパティのみがすべての波形描画のメソッドにおいて関係しています. また,
          </p>
          <ul>
            <li>時間領域の波形描画は, <code>fftSize</code> プロパティ以外は関係しません</li>
            <li><code>getByteFrequencyData</code> メソッドのみ, すべてのプロパティが関係します</li>
          </ul>
          <p>したがって, まずは時間領域の波形描画から解説します. また, ここで, SVG と Canvas の API を理解しておきましょう.</p>
          <p>また, 以降のセクションでは, 以下のような SVG または, Canvas の HTML タグが記述されている前提でコードを解説します.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;svg id=&quot;svg&quot; width=&quot;720&quot; height=&quot;180&quot;&gt;&lt;/svg&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;canvas id=&quot;canvas&quot; width=&quot;720&quot; height=&quot;180&quot;&gt;&lt;/canvas&gt;</code></pre>
          <section id="section-sound-visualization-analyser-node-render-time-domain">
            <h4>時間領域の波形描画</h4>
            <p>
              時間領域の波形描画で関係のある, <code>AnalyserNode</code> のプロパティは <code>fftSize</code> プロパティのみです. この値は,
              <a href="#section-effectors-by-audio-worklet-time-and-frequency-resolution">時間分解能と周波数分解能</a>に関わってくる値です.
              時間領域の波形描画だけを実行するのであれば, この値はできるだけ小さいほうが, 時間分解能としては高くなります.
            </p>
            <section id="section-sound-visualization-analyser-node-render-time-domain-get-float-time-domain-data">
              <h5>getFloatTimeDomainData メソッド</h5>
              <p>
                <b><code>getFloatTimeDomainData</code></b> メソッドは, <code>Float32Array</code> を引数にとり,
                <code>Float32Array</code> のサイズだけ時間領域の振幅を格納します. その値の範囲は <b><code>-1</code> ~ <code>1</code></b> です (つまり,
                <code>0</code> が無音となります). ただし, 引数に指定する <code>Float32Array</code> のサイズが <code>fftSize</code> プロパティより大きい場合,
                超過した要素は <code>0</code> のままです. また, <code>fftSize</code> プロパティより小さい場合は, 不足した分の値は無視されます. したがって,
                イディオム的には, <code>fftSize</code> プロパティのサイズの <code>Float32Array</code> を引数に指定します.
              </p>
              <p>
                そして, <code>getFloatTimeDomainData</code> メソッドをアニメーションで繰り返し実行することで, 実行時における,
                <code>fftSize</code> プロパティサイズのサンプル数だけ時間領域の波形を描画できることになります. 以下は,
                <code>requestAnimationFrame</code> メソッドでアニメーションを実行する場合のコード例です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const analyser   = new AnalyserNode(context);

// OscillatorNode (Input) -&gt; AnalyserNode (Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    // TOOD: Draw time domain wave to SVG or Canvas
  }

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

render();</code></pre>
              <p>
                したがって, あとは, SVG や Canvas にアニメーションごとに取得して <code>Float32Array</code> のインデックスと値, そして, サンプリング周期から x
                座標と y 座標の値を算出していくことで, 時間領域の波形や, 時間のテキストを描画していくことができます.
              </p>
              <p>
                まずは, 波形描画だけを実装してみます. y 軸の値は, <code>getFloatTimeDomainData</code> メソッドで取得した <code>Float32Array</code> の値と, SVG
                や Canvas の <code>height</code> の値から算出できます. 振幅と y 座標のマッピングを考えると,
              </p>
              <ul>
                <li>振幅が <code>1</code> の場合, y 座標は <code>0</code></li>
                <li>振幅が <code>-1</code> の場合, y 座標は <code>height</code></li>
                <li>振幅が <code>0</code> の場合, y 座標は <code>height</code> の半分</li>
              </ul>
              <figure>
                <svg id="svg-figure-mapping-amplitude-and-height-in-float32" width="720" height="360" data-parameters="true" data-a="1.0" />
                <figcaption>振幅 (<code>Float32Array</code>) と <code>height</code> のマッピング</figcaption>
              </figure>
              <p>この対応関係から, y 座標の値は <code>(1 - data[n]) * (height / 2)</code> で算出することができます.</p>
              <p>
                x 軸は, SVG や Canvas の <code>width</code> を <code>fftSize</code> プロパティのサイズだけ等分に描画するので,
                <code>n * (width / analyser.fftSize)</code> で算出することができます.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const analyser   = new AnalyserNode(context);

// OscillatorNode (Input) -&gt; AnalyserNode (Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);

// const svg = document.getElementById(&apos;svg&apos;);
//
// const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
// const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

// const canvas = document.getElementById(&apos;canvas&apos;);
//
// const width  = canvas.width;
// const height = canvas.height;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (width / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);
  }

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

render();</code></pre>
              <p>x 座標と y 座標が算出できたので, あとは波形を描画していくだけですが, ここはグラフィックス API によって異なりまります.</p>
              <p>
                SVG の場合, パスの描画は <code>document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;)</code> で
                <code>SVGPathElement</code> を生成して, その <code>d</code> 属性に, 座標を設定します. また, パスのスタイルを設定するには,
                <code>stroke</code> 属性 (パスの色) や <code>stroke-width</code> 属性 (パスの幅) などいくつかあるので, このあたりは
                <a href="https://svgwg.org/svg2-draft/" target="_blank" rel="noopener noreferrer">SVG の仕様</a>などを参考にしてくださいなどを参考にしてください.
              </p>
              <p>
                また, <code>SVGPathElement</code> の <code>d</code> 属性は, パスの開始座標のプレフィックスに <code>M</code> をつけて,
                それ以降の座標のプレフィックスには <code>L</code> をつけます.
              </p>
              <p>アニメーションごとの描画の最初に, 前回の描画されたパスを削除するために <code>SVGPathElement</code> の <code>d</code> 属性を削除しています.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const analyser   = new AnalyserNode(context);

// OscillatorNode (Input) -&gt; AnalyserNode (Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);

const svg = document.getElementById(&apos;svg&apos;);

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

svg.appendChild(path);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (width / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

render();</code></pre>
              <p>次に, Canvas を利用する場合です.</p>
              <p>
                まず, Canvas (<code>HTMLCanvasElement</code>) そのものは, 描画の API が定義されていないので, <b><code>getContext</code></b> メソッドの引数に
                <code>&apos;2d&apos;</code> を指定して, <b><code>CanvasRenderingContext2D</code></b> インスタンスを取得します.
                <code>CanvasRenderingContext2D</code> に, 描画や描画スタイルに関するメソッドやプロパティが定義されています.
              </p>
              <p>
                SVG の場合と同様に, アニメーションごとの描画の最初に, <b><code>clearRect</code></b> メソッドで前回の描画をクリアしています. ただし, SVG
                と異なるのは, Canvas の場合, 基本的にはすべての描画をクリアすることになります (引数を調整すれば, 部分的にクリアすることも可能ですが, SVG
                のように描画オブジェクト (DOM) を指定して削除するような API はなく, 基本的にはできません (もちろん, <code>1px</code> サイズの矩形を
                <code>clearRect</code> していくことで擬似的には可能ですが, 実装が容易ではありません)).
              </p>
              <p>
                次に, パスの描画を開始するために <b><code>beginPath</code></b> メソッドを実行します.
              </p>
              <p>
                あとは, SVG のパスの描画の考え方と同じで, パスの最初は <b><code>moveTo</code></b> メソッドで開始座標を移動して, それ以降は,
                <b><code>lineTo</code></b> メソッドでパスの座標を指定していきます (ちなみに, パスの描画の終わりに,
                <b><code>closePath</code></b> メソッドを呼び出すことでパスの終点と始点を同一にすることも可能です)
              </p>
              <p>
                パスのスタイルは, <b><code>storkeStyle</code></b> プロパティでパスの色を,
                <b><code>lineWidth</code></b> プロパティでパスの幅を指定することが可能です.
              </p>
              <p>
                Canvas においては, パスの描画というのは, 描画する座標を定義しているにすぎません. 最終的にそのパスを Canvas 上に描画するには,
                <b><code>stroke</code></b> メソッドか <b><code>fill</code></b> メソッド (塗りつぶしの場合) を呼び出す必要があります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context);
const analyser   = new AnalyserNode(context);

// OscillatorNode (Input) -&gt; AnalyserNode (Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);

const canvas = document.getElementById(&apos;canvas&apos;);

const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (width / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (n === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  // renderingContext.closePath();

  renderingContext.storke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

render();</code></pre>
              <p>
                以下のコードは, SVG でユーザーインタラクティブに時間領域の波形のパスのみを描画するコードです. また, 時間分解能を優先して,
                <code>fftSize</code> プロパティを <code>128</code> サンプルに設定しています (サンプリング周波数 <code>48000 Hz</code> で, 約
                <code>2.5 msec</code> のサンプル数を 1 フレームで描画することになります).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const svg = document.getElementById(&apos;svg-animation-time-domain-wave-path&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (width / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-time-domain-wave-path&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <svg id="svg-animation-time-domain-wave-path" width="720" height="180"></svg>
                <figcaption>
                  <span>SVG による時間領域の波形描画</span>
                  <button type="button" id="button-svg-time-domain-wave-path">start</button>
                </figcaption>
              </figure>
              <p>同様に, 以下のコードは, Canvas でユーザーインタラクティブに時間領域の波形のパスのみを描画するコードです.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const canvas           = document.getElementById(&apos;canvas-animation-time-domain-wave-path&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (width / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (n === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-time-domain-wave-path&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <canvas id="canvas-animation-time-domain-wave-path" width="720" height="180"></canvas>
                <figcaption>
                  <span>Canvas による時間領域の波形描画</span>
                  <button type="button" id="button-canvas-time-domain-wave-path">start</button>
                </figcaption>
              </figure>
              <section id="section-sound-visualization-analyser-node-render-time-domain-get-float-time-domain-data-with-coordinate-and-texts">
                <h6>座標とテキストの描画</h6>
                <p>x 座標に関しては, <code>height / 2</code> の y 座標で, <code>width</code> いっぱいに矩形を描画すればよいでしょう.</p>
                <p>
                  y 座標に関しては, このあとに振幅の値のテキストを描画することを考慮して, 右方向に <code>24px</code> ずらして, 高さは
                  <code>height</code> で矩形を描画しています (テキスト描画位置によって, サイズや方向は異なるので, 1 つの例として捉えてください).
                </p>
                <p>以下のコードは, SVG でユーザーインタラクティブに, 時間領域の波形のパスに追加して, x 座標と y 座標を描画するコードです.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const svg = document.getElementById(&apos;svg-animation-time-domain-wave-path-with-coordinate&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth = width - 24;

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, &apos;0&apos;);
xRect.setAttribute(&apos;y&apos;, ((height / 2) - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, width.toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, &apos;24&apos;);
yRect.setAttribute(&apos;y&apos;, &apos;0&apos;);
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, height.toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-time-domain-wave-path-with-coordinate&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <svg id="svg-animation-time-domain-wave-path-with-coordinate" width="720" height="180"></svg>
                  <figcaption>
                    <span>SVG による時間領域の波形描画 (x 座標, y 座標表示)</span>
                    <button type="button" id="button-svg-time-domain-wave-path-with-coordinate">start</button>
                  </figcaption>
                </figure>
                <p>同様に, 以下のコードは, Canvas でユーザーインタラクティブに, 時間領域の波形のパスに追加して, x 座標と y 座標を描画するコードです.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const canvas           = document.getElementById(&apos;canvas-animation-time-domain-wave-path-with-coordinate&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth = width - 24;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(0, ((height / 2) - 1), width, 2);
  renderingContext.fillRect(24, 0, 2, height);

  renderingContext.beginPath();

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (n === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-time-domain-wave-path-with-coordinate&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <canvas id="canvas-animation-time-domain-wave-path-with-coordinate" width="720" height="180"></canvas>
                  <figcaption>
                    <span>Canvas による時間領域の波形描画 (x 座標, y 座標表示)</span>
                    <button type="button" id="button-canvas-time-domain-wave-path-with-coordinate">start</button>
                  </figcaption>
                </figure>
                <p>最後に, 振幅と時間のテキストの描画です.</p>
                <p>
                  振幅のテキスト描画は, <code>Float32Array</code> がそのまま振幅値に対応しているので,
                  <a href="#svg-figure-mapping-amplitude-and-height-in-float32">振幅と <code>height</code> のマッピング</a>をもとに, 対応する座標に,
                  振幅のテキスト描画するだけです.
                </p>
                <p>
                  時間のテキスト描画は, デジタルオーディオ信号処理の理解が必要といっても, サンプリングを理解していればそれほど難しくはありません.
                  サンプリングの仕組みから, 取得した時間領域の波形を格納している <code>Float32Array</code> の
                  <span class="math-inline">$n$</span> 番目のインデックスと, その次のインデックスは, サンプリング周波数の逆数, つまり,
                  サンプリング周期だけ時間が進んでいることになります. つまり, <code>Float32Array</code> のインデックスを <span class="math-inline">$n$</span>,
                  サンプリング周期を <span class="math-inline">$T_{s}$</span> とすると, インデックス <span class="math-inline">$n$</span> における時間
                  <span class="math-inline">$t_{n}$</span> は, <span class="math-inline">$t_{n} = n \cdot T_{s}$</span> となるわけです.
                </p>
                <figure>
                  <svg id="svg-figure-sampling-period-to-time-text" width="720" height="360" data-parameters="true" />
                  <figcaption>サンプリング周期 (<span class="math-inline">$T_{s}$</span>) とインデックス</figcaption>
                </figure>
                <p>
                  もっとも, すべてのインデックスで時間のテキストを描画してしまうと, テキストが重なって可読できないので, 実際には,
                  ある程度間引いて描画するのがよいでしょう.
                </p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const samplingPeriod = 1 / context.sampleRate;

const svg = document.getElementById(&apos;svg-animation-time-domain-wave-path-with-coordinate-and-texts&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth = width - 24;

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, &apos;0&apos;);
xRect.setAttribute(&apos;y&apos;, ((height / 2) - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, width.toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, &apos;24&apos;);
yRect.setAttribute(&apos;y&apos;, &apos;0&apos;);
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, height.toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = amplitude.toFixed(1);

  let h = 0;

  switch (amplitude) {
    case 1.0: {
      h = 12;
      break;
    }

    case 0.0: {
      h = -4;
      break;
    }

    case -1.0: {
      h = 0;
      break;
    }
  }

  text.setAttribute(&apos;x&apos;, &apos;24&apos;);
  text.setAttribute(&apos;y&apos;, ((1 - amplitude) * (height / 2) + h).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

for (let n = 0; n &lt; analyser.fftSize; n++) {
  if (n % 16 !== 0) {
    continue;
  }

  const x = n * (innerWidth / analyser.fftSize) + 24 + 4;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${(n * samplingPeriod * 1000).toFixed(2)} msec`;

  text.setAttribute(&apos;x&apos;, x);
  text.setAttribute(&apos;y&apos;, (height / 2 + 12).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
}

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Time&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height / 2 - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Amplitude&apos;;

yLabel.setAttribute(&apos;x&apos;, &apos;28&apos;);
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-time-domain-wave-path-with-coordinate-and-texts&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <svg id="svg-animation-time-domain-wave-path-with-coordinate-and-texts" width="720" height="180"></svg>
                  <figcaption>
                    <span>SVG による時間領域の波形描画 (x 座標, y 座標, および, 振幅・時間テキスト表示)</span>
                    <button type="button" id="button-svg-time-domain-wave-path-with-coordinate-and-texts">start</button>
                  </figcaption>
                </figure>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const samplingPeriod = 1 / context.sampleRate;

const canvas           = document.getElementById(&apos;canvas-animation-time-domain-wave-path-with-coordinate-and-texts&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth = width - 24;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.fftSize);

  analyser.getFloatTimeDomainData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(0, ((height / 2) - 1), width, 2);
  renderingContext.fillRect(24, 0, 2, height);

  renderingContext.font      = &apos;Roboto 12px&apos;;
  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;

  [1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
    let h = 0;

    switch (amplitude) {
      case 1.0: {
        h = 12;
        break;
      }

      case 0.0: {
        h = -4;
        break;
      }

      case -1.0: {
        h = 0;
        break;
      }
    }

    renderingContext.textAlign = &apos;end&apos;;
    renderingContext.fillText(amplitude.toFixed(1), 24, (((1 - amplitude) * (height / 2)) + h));
  });

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    if (n % 16 !== 0) {
      continue;
    }

    const x = (n * (innerWidth / analyser.fftSize)) + 24 + 4;

    renderingContext.textAlign = &apos;start&apos;;
    renderingContext.fillText(`${(n * samplingPeriod * 1000).toFixed(2)} msec`, x, ((height / 2) + 12));
  }

  renderingContext.font = &apos;Roboto 14px&apos;;

  renderingContext.textAlign = &apos;end&apos;;
  renderingContext.fillText(&apos;Time&apos;, width, ((height / 2) - 8));

  renderingContext.textAlign = &apos;start&apos;;
  renderingContext.fillText(&apos;Amplitude&apos;, 28, 12);

  renderingContext.beginPath();

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - data[n]) * (height / 2);

    if (n === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-time-domain-wave-path-with-coordinate-and-texts&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <canvas id="canvas-animation-time-domain-wave-path-with-coordinate-and-texts" width="720" height="180"></canvas>
                  <figcaption>
                    <span>Canvas による時間領域の波形描画 (x 座標, y 座標, および, 振幅・時間テキスト表示)</span>
                    <button type="button" id="button-canvas-time-domain-wave-path-with-coordinate-and-texts">start</button>
                  </figcaption>
                </figure>
                <p>
                  ところで, SVG の場合, 座標軸やテキストなどアニメーションごとに不変な (静的な) 描画は, アニメーション前に描画して,
                  波形のパスのみをアニメーションごとに描画しています. 一方で, Canvas の場合, 基本的にはアニメーションのたびに描画をクリアするので,
                  座標軸やテキストなど本来不変な描画もアニメーションごとに描画しています. したがって, この点においては SVG
                  のほうが描画パフォーマンスが優れていると言えます. 描画自体のパフォーマンスは Canvas が優れていますが,
                  描画するオブジェクトがほとんど不変であれば SVG のほうがパフォーマンスが優れるケースもあるかもしれません.
                </p>
              </section>
            </section>
            <section id="section-sound-visualization-analyser-node-render-time-domain-get-byte-time-domain-data">
              <h5>getByteTimeDomainData メソッド</h5>
              <p>
                <b><code>getByteTimeDomainData</code></b> メソッドは, <code>Uint8Array</code> を引数にとり,
                <code>Uint8Array</code> のサイズだけ時間領域の振幅を格納します. その値の範囲は <b><code>0</code> ~ <code>255</code></b> です. ただし,
                引数に指定する <code>Uint8Array</code> のサイズが <code>fftSize</code> プロパティより大きい場合, 超過した要素は <code>0</code> のままです. また,
                <code>fftSize</code> プロパティより小さい場合は, 不足した分の値は無視されます. したがって, イディオム的には,
                <code>fftSize</code> プロパティのサイズの <code>Uint8Array</code> を引数に指定します. サイズと格納される値の仕様は
                <code>getFloatTimeDomainData</code> メソッドと同じです.
              </p>
              <p><code>getFloatTimeDomainData</code> メソッドと異なるのは, 格納される値の型が異なるので, 振幅と y 座標のマッピングのみです.</p>
              <ul>
                <li><code>255</code> の場合, 振幅が <code>1</code> で, y 座標は <code>0</code></li>
                <li><code>0</code> の場合, 振幅が <code>-1</code> で, y 座標は <code>height</code></li>
                <li><code>128</code> の場合, 振幅が <code>0</code> で, y 座標は <code>height</code> の半分</li>
              </ul>
              <p>
                つまり, <code>getFloatTimeDomainData</code> メソッドで取得できる <code>Float32Array</code> の値を
                <span class="math-inline">$x\left[n\right]$</span>, <code>getByteTimeDomainData</code> メソッで取得できる <code>Uint8Array</code> の値を
                <span class="math-inline">$b\left[n\right]$</span> とすると以下のような値の関係にあります.
              </p>
              <div class="math-block">$b\left[n\right] = \lfloor 128 \cdot \left(1 + x\left[n\right]\right) \rfloor$</div>
              <figure>
                <svg id="svg-figure-mapping-amplitude-and-height-in-uint8" width="720" height="360" data-parameters="true" data-a="1.0" />
                <figcaption>振幅 (<code>Uint8Array</code>) と <code>height</code> のマッピング</figcaption>
              </figure>
              <p>この対応関係から, y 座標の値は <code>(1 - (data[n] / 255)) * height</code> で算出することができます.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const samplingPeriod = 1 / context.sampleRate;

const svg = document.getElementById(&apos;svg-animation-time-domain-wave-path-with-coordinate-and-texts-in-uint8&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth = width - 24;

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, &apos;0&apos;);
xRect.setAttribute(&apos;y&apos;, ((height / 2) - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, width.toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, &apos;24&apos;);
yRect.setAttribute(&apos;y&apos;, &apos;0&apos;);
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, height.toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = amplitude.toFixed(1);

  let h = 0;

  switch (amplitude) {
    case 1.0: {
      h = 12;
      break;
    }

    case 0.0: {
      h = -4;
      break;
    }

    case -1.0: {
      h = 0;
      break;
    }
  }

  text.setAttribute(&apos;x&apos;, &apos;24&apos;);
  text.setAttribute(&apos;y&apos;, ((1 - amplitude) * (height / 2) + h).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

for (let n = 0; n &lt; analyser.fftSize; n++) {
  if (n % 16 !== 0) {
    continue;
  }

  const x = n * (innerWidth / analyser.fftSize) + 24 + 4;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${(n * samplingPeriod * 1000).toFixed(2)} msec`;

  text.setAttribute(&apos;x&apos;, x);
  text.setAttribute(&apos;y&apos;, (height / 2 + 12).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
}

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Time&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height / 2 - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Amplitude&apos;;

yLabel.setAttribute(&apos;x&apos;, &apos;28&apos;);
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

let animationId = null;

const render = () =&gt; {
  const data = new Uint8Array(analyser.fftSize);

  analyser.getByteTimeDomainData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - data[n] / 255) * height;

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-time-domain-wave-path-with-coordinate-and-texts-in-uint8&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <svg id="svg-animation-time-domain-wave-path-with-coordinate-and-texts-in-uint8" width="720" height="180"></svg>
                <figcaption>
                  <span><code>getByteTimeDomainData</code> メソッドによる時間領域の波形描画 (SVG)</span>
                  <button type="button" id="button-svg-time-domain-wave-path-with-coordinate-and-texts-in-uint8">start</button>
                </figcaption>
              </figure>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 128 });

const canvas           = document.getElementById(&apos;canvas-animation-time-domain-wave-path-with-coordinate-and-texts-in-uint8&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth = width - 24;

let animationId = null;

const render = () =&gt; {
  const data = new Uint8Array(analyser.fftSize);

  analyser.getByteTimeDomainData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(0, ((height / 2) - 1), width, 2);
  renderingContext.fillRect(24, 0, 2, height);

  renderingContext.font      = &apos;Roboto 12px&apos;;
  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;

  [1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
    let h = 0;

    switch (amplitude) {
      case 1.0: {
        h = 12;
        break;
      }

      case 0.0: {
        h = -4;
        break;
      }

      case -1.0: {
        h = 0;
        break;
      }
    }

    renderingContext.textAlign = &apos;end&apos;;
    renderingContext.fillText(amplitude.toFixed(1), 24, (((1 - amplitude) * (height / 2)) + h));
  });

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    if (n % 16 !== 0) {
      continue;
    }

    const x = (n * (innerWidth / analyser.fftSize)) + 24 + 4;

    renderingContext.textAlign = &apos;start&apos;;
    renderingContext.fillText(`${(n * samplingPeriod * 1000).toFixed(2)} msec`, x, ((height / 2) + 12));
  }

  renderingContext.font = &apos;Roboto 14px&apos;;

  renderingContext.textAlign = &apos;end&apos;;
  renderingContext.fillText(&apos;Time&apos;, width, ((height / 2) - 8));

  renderingContext.textAlign = &apos;start&apos;;
  renderingContext.fillText(&apos;Amplitude&apos;, 28, 12);

  renderingContext.beginPath();

  for (let n = 0; n &lt; analyser.fftSize; n++) {
    const x = n * (innerWidth / analyser.fftSize);
    const y = (1 - (data[n] / 255)) * height;

    if (n === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-time-domain-wave-path-with-coordinate-and-texts-in-uint8&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <canvas id="canvas-animation-time-domain-wave-path-with-coordinate-and-texts-in-uint8" width="720" height="180"></canvas>
                <figcaption>
                  <span><code>getByteTimeDomainData</code> メソッドによる時間領域の波形描画 (Canvas)</span>
                  <button type="button" id="button-canvas-time-domain-wave-path-with-coordinate-and-texts-in-uint8">start</button>
                </figcaption>
              </figure>
            </section>
          </section>
          <section id="section-sound-visualization-analyser-node-render-frequency-domain">
            <h4>周波数領域領域の波形描画 (振幅スペクトルの描画)</h4>
            <p>
              周波数領域のスペクトル描画で関係のある, <code>AnalyserNode</code> のプロパティは, <code>getFloatFrequencyData</code> メソッドの場合,
              <code>fftSize</code> プロパティ (その <span class="math-inline">$\frac{1}{2}$</span> の値となる <code>frequencyBinCount</code> プロパティ) と
              <code>smoothingTimeConstant</code> プロパティのみです. <code>getByteFrequencyData</code> メソッドの場合, これらのプロパティに追加して,
              <code>maxDecibels</code> プロパティと <code>minDecibels</code> プロパティが関連してきます.
              <a href="#section-effectors-by-audio-worklet-time-and-frequency-resolution">時間分解能と周波数分解能</a>で解説しましたが,
              スペクトル描画のみ場合は, <code>fftSize</code> プロパティはある程度大きくして, 周波数分解能を高くするほうが精度のよい描画となります (もっとも,
              デフォルトの <code>2048</code> サンプルでも十分ですが).
            </p>
            <p>
              また, <code>getFloatFrequencyData</code> メソッド, <code>getByteFrequencyData</code> メソッドで利用される (FFT のプリプロセス処理となる)
              <b>窓関数は<a href="#section-effectors-by-audio-worklet-overlap-add-window-function">ブラックマン窓</a></b>です.
            </p>
            <section id="section-sound-visualization-analyser-node-render-frequency-domain-smoothing-time-constant">
              <h5>smoothingTimeConstant プロパティ</h5>
              <p>
                <b><code>smoothingTimeConstant</code></b> プロパティに関しては, どちらのメソッドにも関係するのと,
                振幅スペクトルの描画ロジックには関わらないので, 先に解説しておきます. <code>smoothingTimeConstant</code> プロパティは
                <code>0 ~ 1</code> までの値を設定可能で, この値が小さいほどスペクトルのアニメーションが速くなり, 大きいほど遅くなります. また,
                <code>1</code> に設定するとスペクトルが更新されなくなります. この設定によって, 時間領域の処理には影響しません (オーディオ API
                一般的なパラメータではなく, Web Audio API 特有のパラメータと思われます).
              </p>
            </section>
            <section id="section-sound-visualization-analyser-node-render-frequency-domain-get-float-frequency-data">
              <h5>getFloatFrequencyData メソッド</h5>
              <p>
                <b><code>getFloatFrequencyData</code></b> メソッドは, <code>Float32Array</code> を引数にとり, FFT
                した結果の振幅スペクトルを<b>デジベル単位で格納します</b>. 可能性としては, 単精度浮動小数点数の値の範囲をとりうることになりますが,
                この範囲をカバーする振幅スペクトルの描画は無理でしょう. しかし, 例えば, 音楽信号であれば, おおよそ <code>-30 dB</code> から
                <code>-60 dB</code> の範囲の振幅になります. つまり, 現実的には, 対象となる音信号がどのような特性かを想定することで, ある程度範囲を限定して,
                振幅スペクトルを描画することができます.
              </p>
              <p>
                <code>getFloatFrequencyData</code> メソッドの引数に渡す <code>Float32Array</code> のサイズは, サンプリング定理より,
                <b><code>frequencyBinCount</code></b> 以下のサイズである必要があります. もっとも, それより大きいサイズを指定してもエラーにはなりませんが,
                サンプリング定理から, <b>振幅スペクトルはナイキスト周波数を軸に線対称</b>となるので (すなわち,
                ナイキスト周波数までの振幅スペクトルが取得できればよいので), <code>frequencyBinCount</code> より大きい要素は
                <code>0</code> のままになる仕様となっています. イディオム的には, <code>frequencyBinCount</code> プロパティのサイズの
                <code>Float32Array</code> を引数に指定します (もしくは, 例えば, 音楽信号を対象にする場合,
                ナイキスト周波数付近の振幅スペクトルまで不要であるケースも多いので, <code>frequencyBinCount</code> のさらに
                <span class="math-inline">$\frac{1}{2}$</span> 程度のサイズでもよいかもしれません. この場合, サンプリング周波数を
                <code>48000 Hz</code> とすると, <code>12000 Hz</code> ぐらいまでの振幅スペクトルを描画できることになります).
              </p>
              <p>
                そして, <code>getFloatFrequencyData</code> メソッドをアニメーションで繰り返し実行することで, アニメーションごとに,
                <code>fftSize</code> プロパティで指定されたサイズで FFT を実行して, 結果となる複素数の配列から, 絶対値を取得して,
                振幅スペクトルをデシベル単位で格納します. そして, サンプリング定理から,
                <code>frequencyBinCount</code> のインデックスがナイキスト周波数に対応するインデックスとなるので, 引数の <code>Float32Array</code> のサイズを
                <code>frequencyBinCount</code> にしていた場合, ナイキスト周波数のインデックスの 1 つ前のインデックス (<code>frequencyBinCount - 1</code>)
                に対応するまでの周波数成分の振幅スペクトルがデシベル単位で格納されることになります. 以下は,
                <code>requestAnimationFrame</code> メソッドでアニメーションを実行する場合のコード例です.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });
const analyser   = new AnalyserNode(context);

// OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
oscillator.connect(analyser);
analyser.connect(context.destination);

// Start oscillator immediately
oscillator.start(0);

// Stop oscillator after 5 sec
oscillator.stop(context.currentTime + 5);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    // TOOD: Draw amplitude spectrum to SVG or Canvas
  }

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

render();</code></pre>
              <p>
                <code>getFloatFrequencyData</code> メソッドで振幅スペクトルを描画するためには, 振幅値の範囲をデシベルで決める必要があります. 今回は,
                <code>0 dB</code> から <code>-60 dB</code> までの振幅を対象に描画することにします. この範囲と, SVG または Canvas の高さとのマッピングを考えます.
              </p>
              <ul>
                <li>振幅スペクトルが <code>0</code> (dB) の場合, y 座標は <code>0</code></li>
                <li>振幅スペクトルが <code>-60</code> (dB) の場合, y 座標は <code>height</code></li>
              </ul>
              <figure>
                <svg id="svg-figure-mapping-amplitude-spectrum-and-height-in-float32" width="720" height="360" />
                <figcaption>振幅スペクトル (dB) と <code>height</code> のマッピング</figcaption>
              </figure>
              <p>この対応関係から, y 座標の値は,</p>
              <p><code>(0 - data[k]) * (height / (max - min)) = (0 - data[k]) * (height / (0 - (-60))) = -data[k] * (height / 60)</code></p>
              <p>で算出することができます.</p>
              <p>
                x 軸は, SVG や Canvas の <code>width</code> を <code>frequencyBinCount</code> プロパティのサイズだけ等分に描画するので,
                <code>k * (width / analyser.frequencyBinCount)</code> で算出することができます.
              </p>
              <p>
                以下のコードは, SVG でユーザーインタラクティブに振幅スペクトルのパスのみを描画するコードです. 周波数分解能を優先して,
                <code>fftSize</code> プロパティを <code>16384</code> サンプルに設定しています (サンプリング周波数 <code>48000 Hz</code> で,
                <code>2 Hz</code> の精度で振幅スペクトルを描画することができます).
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const svg = document.getElementById(&apos;svg-animation-amplitude-spectrum-path&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const maxDecibels = 0;
const minDecibels = -60;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    // for Chrome
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (width / analyser.frequencyBinCount);
    const y = (0 - data[k]) * (height / (maxDecibels - minDecibels));

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-animation-spectrum-path&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <p>
                ところで, <code>getFloatFrequencyData</code> メソッドで取得したすべての振幅スペクトルの数値を
                <code>Number.isFinite</code> で有限値かどうかを判定していますが, これは本来は不要なのですが, Chrome の実装のバグで,
                <code>Infinity</code> が格納されてしまう場合があるので, その対策としてこの判定をしています (Firefox や Safari では
                <code>Infinity</code> になることはないので, この判定は不要です). 将来のバージョンにおいて, 修正されて不要になる可能性はありますが,
                現状はこの判定が必要です).
              </p>
              <figure>
                <svg id="svg-animation-amplitude-spectrum-path" width="720" height="180"></svg>
                <figcaption>
                  <span>SVG による周波数領域 (振幅スペクトル) の波形描画</span>
                  <button type="button" id="button-svg-animation-spectrum-path">start</button>
                </figcaption>
              </figure>
              <p>同様に, 以下のコードは, Canvas でユーザーインタラクティブに振幅スペクトルのパスのみを描画するコードです.</p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const canvas           = document.getElementById(&apos;canvas-animation-amplitude-spectrum-path&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const maxDecibels = 0;
const minDecibels = -60;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (width / analyser.frequencyBinCount);
    const y = (0 - data[k]) * (height / (maxDecibels - minDecibels));

    if (k === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-animation-spectrum-path&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <canvas id="canvas-animation-amplitude-spectrum-path" width="720" height="180"></canvas>
                <figcaption>
                  <span>Canvas による周波数領域 (振幅スペクトル) の波形描画</span>
                  <button type="button" id="button-canvas-animation-spectrum-path">start</button>
                </figcaption>
              </figure>
              <section id="section-sound-visualization-analyser-node-render-frequency-domain-get-float-frequency-data-with-coordinate-and-texts">
                <h6>座標とテキストの描画</h6>
                <p>
                  x, y 座標にともに, このあとに振幅の値のテキストを描画することを考慮して, x 座標, y 座標をそれぞれテキストサイズを考慮して, 右方向に
                  <code>48px</code> ずらして, 上方向に <code>24px</code> ずらして描画しておきます (テキスト描画位置によって, サイズや方向は異なるので, 1
                  つの例として捉えてください).
                </p>
                <p>以下のコードは, SVG でユーザーインタラクティブに, 振幅スペクトルのパスに追加して, x 座標と y 座標を描画するコードです.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const svg = document.getElementById(&apos;svg-animation-amplitude-spectrum-path-with-coordinate&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const maxDecibels = 0;
const minDecibels = -60;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
xRect.setAttribute(&apos;y&apos;, (height - translateY - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, (width - translateX).toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, (height - translateX).toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    // for Chrome
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = Math.min(((0 - data[k]) * (innerHeight / (maxDecibels - minDecibels)) - translateY), (height - translateY));

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-amplitude-spectrum-path-with-coordinate&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <svg id="svg-animation-amplitude-spectrum-path-with-coordinate" width="720" height="180"></svg>
                  <figcaption>
                    <span>SVG による周波数領域 (振幅スペクトル) の波形描画 (x 座標, y 座標表示)</span>
                    <button type="button" id="button-svg-amplitude-spectrum-path-with-coordinate">start</button>
                  </figcaption>
                </figure>
                <p>同様に, 以下のコードは, Canvas でユーザーインタラクティブに, 振幅スペクトルのパスに追加して, x 座標と y 座標を描画するコードです.</p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const canvas           = document.getElementById(&apos;canvas-animation-amplitude-spectrum-path-with-coordinate&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const maxDecibels = 0;
const minDecibels = -60;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = Math.min(((0 - data[k]) * (innerHeight / (maxDecibels - minDecibels)) - translateY), (height - translateY));

    if (k === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(translateX, (height - translateY - 1), innerWidth, 2);
  renderingContext.fillRect(translateX, translateY, 2, innerHeight);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-amplitude-spectrum-path-with-coordinate&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <canvas id="canvas-animation-amplitude-spectrum-path-with-coordinate" width="720" height="180"></canvas>
                  <figcaption>
                    <span>Canvas による周波数領域 (振幅スペクトル) の波形描画 (x 座標, y 座標表示)</span>
                    <button type="button" id="button-canvas-amplitude-spectrum-path-with-coordinate">start</button>
                  </figcaption>
                </figure>
                <p>
                  振幅スペクトの描画の y 座標の算出で, y 座標の加減と比較して, 小さい方の値を描画する y 座標としていますが, これは,
                  <code>-60 dB</code> はアプリケーション側で決定した描画対象の振幅スペクトルのデシベルの下限であり,
                  <code>getFloatFrequencyData</code> メソッドで取得できる値はそれを下回ることも十分にありえるからです.
                </p>
                <p>最後に, 振幅と時間のテキストと描画です.</p>
                <p>
                  振幅のテキスト描画は,
                  <a href="#svg-figure-mapping-amplitude-spectrum-and-height-in-float32">描画対象のデシベル範囲と <code>height</code> のマッピング</a>から,
                  <code>1 dB</code> 間は <code>height / (max - min)</code> となりますが, すべてを描画してもテキストが重なって可読できないので,
                  ここで記載するコード例では, <code>10 dB</code> ごとに描画することにします. つまり, <code>height</code> を 6 等分した間隔で, <code>0 dB</code>,
                  <code>-10 dB</code>, <code>-20 dB</code>, ... <code>-50 dB</code>, <code>-60 dB</code> と描画することになります.
                </p>
                <p>
                  周波数のテキスト描画は, <a href="#section-effectors-by-audio-worklet-time-and-frequency-resolution">周波数分解能</a>と
                  <code>Float32Array</code> のインデックスから算出することができます. 取得した振幅スペクトルを格納している <code>Float32Array</code> の
                  <span class="math-inline">$k$</span> 番目のインデックスに対応する周波数 <span class="math-inline">$f_{k}$</span> は, サンプリング周波数を
                  <span class="math-inline">$f_{s}$</span>, FFT サイズ (<code>fftSize</code> プロパティの値) を <span class="math-inline">$N$</span>とすると,
                  <span class="math-inline">$f_{k} = k \cdot \left(\frac{f_{s}}{N}\right)$</span> となります. 周波数軸も,
                  すべてのインデックスでテキストを描画してしまうと, テキストが重なって視認できないので, 実際には, ある程度間引いて描画したり,
                  <code>frequencyBinCount</code> の <code>1/2</code> や, <code>1/4</code> に対応するインデックスまで描画したりすればよいでしょう.
                </p>
                <figure>
                  <svg id="svg-figure-frequency-resolution-to-frequency-text" width="720" height="360" />
                  <figcaption>
                    周波数分解能 (<span class="math-inline">$f_{\mathrm{resolution}} = \left(\frac{f_{s}}{N}\right)$</span>) とインデックス
                  </figcaption>
                </figure>
                <p>
                  厳密には, <code>frequencyBinCount</code> プロパティのサイズで取得した振幅スペクトルは, ナイキスト周波数に相当するインデックスの 1
                  つ前までの周波数成分の取得になります. ただし, <code>fftSize</code> プロパティのサイズが 2 の冪乗であるので,
                  <code>frequencyBinCount</code> プロパティのサイズは常に<b>偶数</b>で, かつ,
                  振幅スペクトルはナイキスト周波数の成分を軸に<b>線対称</b>となります. したがって, 実質的に, ナイキスト周波数の周波数成分まで取得できます
                  (上記のイラストを参照してください).
                </p>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const frequencyResolution = context.sampleRate / analyser.fftSize;

const svg = document.getElementById(&apos;svg-animation-amplitude-spectrum-path-with-coordinate-and-texts&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const maxDecibels = 0;
const minDecibels = -60;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
xRect.setAttribute(&apos;y&apos;, (height - translateY - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, (width - translateX).toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, (height - translateX).toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[0, -10, -20, -30, -40, -50, -60].forEach((dB, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${dB} dB`;

  text.setAttribute(&apos;x&apos;, &apos;44&apos;);
  text.setAttribute(&apos;y&apos;, (index * (innerHeight / 6) + translateY).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
  if (k % 1024 !== 0) {
    continue;
  }

  const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${Math.trunc(k * frequencyResolution)} Hz`;

  text.setAttribute(&apos;x&apos;, x);
  text.setAttribute(&apos;y&apos;, (height - 8).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
}

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Frequency (Hz)&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height - translateY - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Amplitude (dB)&apos;;

yLabel.setAttribute(&apos;x&apos;, translateY.toString(10));
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    // for Chrome
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = Math.min(((0 - data[k]) * (innerHeight / (maxDecibels - minDecibels)) - translateY), (height - translateY));

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-amplitude-spectrum-path-with-coordinate-and-texts&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <svg id="svg-animation-amplitude-spectrum-path-with-coordinate-and-texts" width="720" height="180"></svg>
                  <figcaption>
                    <span>SVG による周波数領域 (振幅スペクトル) の波形描画 (x 座標, y 座標, および, 振幅・周波数テキスト表示)</span>
                    <button type="button" id="button-svg-amplitude-spectrum-path-with-coordinate-and-texts">start</button>
                  </figcaption>
                </figure>
                <pre
                  data-prismjs-copy="クリップボードにコピー"
                  data-prismjs-copy-success="コピーしました"
                ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const frequencyResolution = context.sampleRate / analyser.fftSize;

const canvas           = document.getElementById(&apos;canvas-animation-amplitude-spectrum-path-with-coordinate-and-texts&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const maxDecibels = 0;
const minDecibels = -60;

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = Math.min(((0 - data[k]) * (innerHeight / (maxDecibels - minDecibels)) - translateY), (height - translateY));

    if (k === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth   = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(translateX, (height - translateY - 1), innerWidth, 2);
  renderingContext.fillRect(translateX, translateY, 2, innerHeight);

  renderingContext.font = &apos;Roboto 12px&apos;;
  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;

  [0, -10, -20, -30, -40, -50, -60].forEach((dB, index) =&gt; {
    renderingContext.textAlign = &apos;end&apos;;
    renderingContext.fillText(`${dB} dB`, 44, (index * (innerHeight / 6) + translateY));
  });

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (k % 1024 !== 0) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;

    renderingContext.textAlign = &apos;start&apos;;
    renderingContext.fillText(`${k * frequencyResolution} Hz`, x, (height - 8));
  }

  renderingContext.font = &apos;Roboto 16px&apos;;

  renderingContext.textAlign = &apos;end&apos;;
  renderingContext.fillText(&apos;Frequency (Hz)&apos;, width, (height - translateY - 8));

  renderingContext.textAlign = &apos;start&apos;;
  renderingContext.fillText(&apos;Amplitude (dB)&apos;, translateY, 12);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-amplitude-spectrum-path-with-coordinate-and-texts&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
                <figure>
                  <canvas id="canvas-animation-amplitude-spectrum-path-with-coordinate-and-texts" width="720" height="180"></canvas>
                  <figcaption>
                    <span>Canvas による周波数領域 (振幅スペクトル) の波形描画 (x 座標, y 座標, および, 振幅・周波数テキスト表示)</span>
                    <button type="button" id="button-canvas-amplitude-spectrum-path-with-coordinate-and-texts">start</button>
                  </figcaption>
                </figure>
              </section>
            </section>
            <section id="section-sound-visualization-analyser-node-render-frequency-domain-get-byte-frequency-data">
              <h5>getByteFrequencyData メソッド</h5>
              <p>
                <b><code>getByteFrequencyData</code></b> メソッドは, <code>Uint8Array</code> を引数にとり,
                <code>Uint8Array</code> のサイズだけ振幅スペクトル格納します. 振幅スペクトルの範囲は <b><code>0</code> ~ <code>255</code></b> です. ただし,
                引数に指定する <code>Uint8Array</code> のサイズが <code>frequencyBinCount</code> プロパティより大きい場合, 超過した要素は
                <code>0</code> のままです. <code>frequencyBinCount</code> プロパティより小さい場合は, 不足した分の値は無視されます. したがって,
                イディオム的には, <code>frequencyBinCount</code> プロパティのサイズの <code>Uint8Array</code> を引数に指定します. これら仕様は
                <code>getFloatFrequencyData</code> メソッドと同じです.
              </p>
              <p><code>getFloatFrequencyData</code> メソッドと異なるのは, 格納される値の型が異なるので, 振幅と y 座標のマッピングのみです.</p>
              <ul>
                <li>
                  <code>255</code> の場合, 振幅が <b><code>maxDecibels</code> プロパティで指定されているデシベル</b> (デフォルト値は, <code>-30 dB</code>) で, y
                  座標は <code>0</code>
                </li>
                <li>
                  <code>0</code> の場合, 振幅が <b><code>minDecibels</code> プロパティで指定されているデシベル</b> (デフォルト値は, <code>-100 dB</code>) で, y
                  座標は <code>height</code>
                </li>
              </ul>
              <p>
                つまり, <code>getFloatTimeDomainData</code> メソッドで取得できる <code>Float32Array</code> の値を
                <span class="math-inline">$X\left[k\right]$</span>, <code>getByteTimeDomainData</code> メソッで取得できる <code>Uint8Array</code> の値を
                <span class="math-inline">$B\left[k\right]$</span>, <code>maxDecibels</code> プロパティの値を
                <span class="math-inline">$\mathrm{dB_{max}}$</span>, <code>minDecibels</code> プロパティの値を
                <span class="math-inline">$\mathrm{dB_{min}}$</span> とすると以下のような値の関係にあります.
              </p>
              <div class="math-block">
                $B\left[k\right] = \lfloor \left(\frac{255}{\mathrm{dB_{max}} - \mathrm{dB_{min}}}\right) \cdot \left(X\left[k\right] - \mathrm{dB_{min}}\right)
                \rfloor$
              </div>
              <p>
                少し理解しにくい関係式ですが, 端的には, <code>getFloatFrequencyData</code> で取得できる振幅スペクトルのデシベル値を, 符号なし 8 bit の範囲
                (<code>0</code> ~ <code>255</code>) にマッピングしているということです. そして, そのマッピングの対象となるデシベルの範囲を,
                <code>maxDecibels</code> プロパティと <code>minDecibels</code> プロパティで決定しています.
              </p>
              <figure>
                <svg id="svg-figure-relation-get-float-frequency-data-and-get-byte-frequency-data" width="800" height="450" />
                <figcaption><code>getFloatFrequencyData</code> メソッドと <code>getByteFrequencyData</code> メソッドの関係</figcaption>
              </figure>
              <p>
                振幅スペクトル, および, そのテキストは, 符号なし 8 bit の最大値の <code>255</code> で除算して <code>0</code> ~
                <code>1</code> の範囲で正規化して描画するのがよいでしょう (振幅スペクトルは絶対値なので, 負数はありません). また,
                <code>maxDecibels</code> プロパティと <code>minDecibels</code> プロパティも多くのケースにおいてはデフォルト値のままで問題ないですが,
                以下のコードとデモでは, あえて変更して, <code>getFloatFrequencyData</code> メソッドで描画したコード例と合わせるように,
                <code>maxDecibels</code> プロパティを <code>0</code> (<code>dB</code>), <code>minDecibels</code> プロパティを <code>-60</code> (<code>dB</code>)
                に設定して実装します.
              </p>
              <p>振幅スペクトルを正規化すると, y 座標の値は <code>(1 - (data[n] / 255)) * height</code> で算出することができます.</p>
              <figure>
                <svg id="svg-figure-mapping-amplitude-spectrum-and-height-in-uint8" width="720" height="360" />
                <figcaption>正規化した振幅スペクトルと <code>height</code> のマッピング</figcaption>
              </figure>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384, maxDecibels: 0, minDecibels: -60 });

const frequencyResolution = context.sampleRate / analyser.fftSize;

const svg = document.getElementById(&apos;svg-animation-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, lineWidth.toString(10));
path.setAttribute(&apos;stroke-linecap&apos;, lineCap);
path.setAttribute(&apos;stroke-linejoin&apos;, lineJoin);

svg.appendChild(path);

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
xRect.setAttribute(&apos;y&apos;, (height - translateY - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, innerWidth.toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, innerHeight.toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[1.0, 0.5, 0.0].forEach((amplitude, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = amplitude.toFixed(1);

  text.setAttribute(&apos;x&apos;, &apos;44&apos;);
  text.setAttribute(&apos;y&apos;, (index * (innerHeight / 2) + translateY).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
  if (k % 1024 !== 0) {
    continue;
  }

  const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${Math.trunc(k * frequencyResolution)} Hz`;

  text.setAttribute(&apos;x&apos;, x);
  text.setAttribute(&apos;y&apos;, (height - 8).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
}

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Frequency (Hz)&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height - translateY - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Amplitude&apos;;

yLabel.setAttribute(&apos;x&apos;, &apos;28&apos;);
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

let animationId = null;

const render = () =&gt; {
  const data = new Uint8Array(analyser.frequencyBinCount);

  analyser.getByteFrequencyData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = (1 - data[k] / 255) * innerHeight + translateY;

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <svg id="svg-animation-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8" width="720" height="180"></svg>
                <figcaption>
                  <span><code>getByteFrequencyData</code> メソッドによる周波数領域 (振幅スペクトル) の波形描画 (SVG)</span>
                  <button type="button" id="button-svg-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8">start</button>
                </figcaption>
              </figure>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384, maxDecibels: 0, minDecibels: -60 });

const frequencyResolution = context.sampleRate / analyser.fftSize;

const canvas           = document.getElementById(&apos;canvas-animation-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8&apos;);
const renderingContext = canvas.getContext(&apos;2d&apos;);

const width  = canvas.width;
const height = canvas.height;

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

let animationId = null;

const render = () =&gt; {
  const data = new Uint8Array(analyser.frequencyBinCount);

  analyser.getByteFrequencyData(data);

  renderingContext.clearRect(0, 0, width, height);

  renderingContext.beginPath();

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;
    const y = (1 - data[k] / 255) * innerHeight + translateY;

    if (k === 0) {
      renderingContext.moveTo(x, y);
    } else {
      renderingContext.lineTo(x, y);
    }
  }

  renderingContext.lineWidth = 1.5;
  renderingContext.strokeStyle = &apos;rgb(0 0 255)&apos;;

  renderingContext.stroke();

  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;
  renderingContext.fillRect(translateX, height - translateY - 1, innerWidth, 2);
  renderingContext.fillRect(translateX, translateY, 2, innerHeight);

  renderingContext.font = &apos;Roboto 12px&apos;;
  renderingContext.fillStyle = &apos;rgb(153 153 153)&apos;;

  [1.0, 0.5, 0.0].forEach((amplitude, index) =&gt; {
    renderingContext.textAlign = &apos;end&apos;;
    renderingContext.fillText(amplitude.toFixed(1), 44, index * (innerHeight / 2) + translateY);
  });

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (k % 1024 !== 0) {
      continue;
    }

    const x = k * (innerWidth / analyser.frequencyBinCount) + translateX;

    renderingContext.textAlign = &apos;start&apos;;
    renderingContext.fillText(`${k * frequencyResolution} Hz`, x, height - 8);
  }

  renderingContext.font = &apos;Roboto 16px&apos;;

  renderingContext.textAlign = &apos;end&apos;;
  renderingContext.fillText(&apos;Frequency (Hz)&apos;, width, height - translateY - 8);

  renderingContext.textAlign = &apos;start&apos;;
  renderingContext.fillText(&apos;Amplitude&apos;, 28, 12);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-canvas-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <canvas id="canvas-animation-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8" width="720" height="180"></canvas>
                <figcaption>
                  <span><code>getByteFrequencyData</code> メソッドによる周波数領域 (振幅スペクトル) の波形描画 (Canvas)</span>
                  <button type="button" id="button-canvas-amplitude-spectrum-path-with-coordinate-and-texts-in-uint8">start</button>
                </figcaption>
              </figure>
            </section>
            <article id="section-sound-visualization-analyser-node-render-frequency-domain-phase">
              <h5>位相スペクトルの描画</h5>
              <p>
                <code>AnalyserNode</code> クラスには, <code>getFloatFrequencyData</code> メソッドのような,
                位相スペクトルを取得するためのメソッドは仕様にありません (すでに解説していますが, 人間の聴覚は位相スペクトルの違いに鈍感という特性があり, 実際,
                エフェクターの実装などにおいても, 位相スペクトルで演算をするということが非常に稀なケースであるからです. しかしながら, API
                として定義されていなくても, 離散フーリエ変換 (高速フーリエ変換) を直接適用することができるので, FFT を適用した結果の複素数の配列から,
                偏角の配列を取得して, 周波数を横軸に位相をプロットしていくことで位相スペクトルを視覚化することができます.
              </p>
              <p>
                また, 位相スペクトルが必要なケースでも, <code>BiquadFilterNode</code> によるフィルタの位相特性を取得する場合は,
                <code>BiquadFilterNode</code> インスタンスの <b><code>getFrequencyResponse</code></b> メソッドを利用することで可能です (<a
                  href="#section-effectors-filter-biquad-filter-node-allpass"
                  >All-Pass Filter の周波数特性 (位相スペクトル)</a>
                は, FFT を実装することなく, <code>getFrequencyResponse</code> メソッドで位相スペクトルを取得することで視覚化しています).
              </p>
              <p>
                位相スペクトルの視覚化の解説として,
                <a href="#section-effectors-by-audio-worklet-overlap-add-audio-worklet-processor-and-window-function-overlap-add-processor"
                  ><code>OverlapAddProcessor</code> クラス</a>を拡張子したクラスを利用して実装します (<code>AudioWorkletProcessor</code> クラスで実装すると, FFT サイズが
                <code>renderQuantumSize</code> (<code>128</code> サンプル) で固定されてしまい, 十分な周波数分解能が確保できないからです).
                <code>processOverlapAdd</code> メソッドで呼び出している, <code>FFT</code> 関数は,
                <a href="#section-fast-fourier-transform-code">高速フーリエ変換の実装</a>セクションで記載している実装を
                <code>AudioWorkletGlobalScope</code> 内で定義していると仮定してください.
              </p>
              <p>
                窓関数は, 位相スペクトルを描画に利用するだけなので, <code>AnalyserNode</code> と同じく, ブラックマン窓を FFT の実行前に適用すればよいでしょう.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">// Filename is &apos;./audio-worklets/phase-spectrum.js&apos;

/**
 * This class extends `OverlapAddProcessor`.
 */
class PhaseSpectrumOverlapAddProcessor extends OverlapAddProcessor {
  constructor(options) {
    super(options);

    this.blackmanWindow = this.createBlackmanWindow(this.frameSize);

    this.frequencyBinCount = this.frameSize / 2;
  }

  /** @overdrive */
  processOverlapAdd(inputs, outputs) {
    const input  = inputs[0];
    const output = outputs[0];

    const numberOfChannels = input.length;

    for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
      // Bypass
      output[channelNumber].set(input[channelNumber]);

      const phases = new Float32Array(this.frequencyBinCount);

      const reals = new Float32Array(this.frameSize);
      const imags = new Float32Array(this.frameSize);

      for (let n = 0; n &lt; this.frameSize; n++) {
        reals[n] = this.blackmanWindow[n] * input[channelNumber][n];
      }

      FFT(reals, imags, this.frameSize);

      const plot = Math.trunc(440 * (this.frameSize / sampleRate));

      for (let k = 0; k &lt; this.frequencyBinCount; k++) {
        if (k !== plot) {
          continue;
        }

        if ((reals[k] !== 0) &amp;&amp; (imags[k] !== 0)) {
          phases[k] = Math.atan2(imags[k], reals[k]);
        }
      }

      // Post to main thread
      this.port.postMessage(phases);
    }
  }

  createBlackmanWindow(size) {
    const w = new Float32Array(size);

    const alpha = 0.16;

    const a0 = (1 - alpha) / 2;
    const a1 = 1 / 2;
    const a2 = alpha / 2;

    for (let n = 0; n &lt; size; n++) {
      w[n] = a0 - a1 * Math.cos((2 * Math.PI * n) / size) + a2 * Math.cos((4 * Math.PI * n) / size);
    }

    return w;
  }
}

registerProcessor('PhaseSpectrumOverlapAddProcessor', PhaseSpectrumOverlapAddProcessor);</code></pre>
              <p>
                <b>位相スペクトルはナイキスト周波数を軸に点対称</b>となります. 線対称か点対称かの違いはありますが, 振幅スペクトと同じく,
                ナイキスト周波数までの位相スペクトルが取得できればよいので, FFT サイズの値を <code>1/2</code> にして
                <code>frequencyBinCount</code> プロパティとして算出しています. したがって, メインスレッドに <code>postMessage</code> される
                <code>Float32Array</code> のサイズは, <code>frequencyBinCount</code> サイズとなります. また, 偏角はすでに解説していますが,
                <code>Math.atan2</code> メソッドに, 対応するインデックスの虚軸の値と実軸の値を引数に指定することで, ラジアンとして算出できます.
              </p>
              <p>
                メインスレッドでは, <code>MessagePort</code> の <code>onmessage</code> イベントハンドラで取得した位相スペクトルを描画します. 位相のプロット範囲
                (プロットの下限と上限) の設定はいくつか考えられるので (<span class="math-inline">$0 \leq \theta \leq 2 \pi$</span>, <code>0°</code> ~
                <code>360°</code> など), ここでは, <span class="math-inline">$- \pi \leq \theta \leq \pi$</span> の範囲で位相スペクトルを描画することにします.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const fftSize = 2048;

const numberOfPlots = fftSize / 64;

const frequencyResolution = context.sampleRate / fftSize;

const svg = document.getElementById(&apos;svg-animation-phase-spectrum-path-with-coordinate-and-texts&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
xRect.setAttribute(&apos;y&apos;, (height - translateY - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, (width - translateX).toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, (height - translateX).toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[&apos;π&apos;, &apos;π/2&apos;, &apos;0&apos;, &apos;-π/2&apos;, &apos;-π&apos;].forEach((radian, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${radian} rad`;

  text.setAttribute(&apos;x&apos;, &apos;44&apos;);
  text.setAttribute(&apos;y&apos;, (index * (innerHeight / 4) + translateY).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

for (let k = 0; k &lt; numberOfPlots; k++) {
  if ((k % 2) !== 0) {
    continue;
  }

  const x = k * (innerWidth / numberOfPlots) + translateX;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${Math.trunc(k * frequencyResolution)} Hz`;

  text.setAttribute(&apos;x&apos;, x);
  text.setAttribute(&apos;y&apos;, (height - 8).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
}

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Frequency (Hz)&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height - translateY - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Phase (radian)&apos;;

yLabel.setAttribute(&apos;x&apos;, translateY.toString(10));
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

const render = (data) =&gt; {
  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; numberOfPlots; k++) {
    const x = k * (innerWidth / numberOfPlots) + translateX;
    const y = ((Math.PI - data[k]) / Math.PI) * (innerHeight / 2) + translateY;

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);
};

const buttonElement = document.getElementById(&apos;button-svg-phase-spectrum-path-with-coordinate-and-texts&apos;);

let processor  = null;
let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (processor === null) {
    await context.audioWorklet.addModule(&apos;./audio-worklets/phase-spectrum.js&apos;);

    processor = new AudioWorkletNode(context, &apos;PhaseSpectrumOverlapAddProcessor&apos;, {
      processorOptions: {
        frameSize: fftSize
      }
    });

    processor.port.onmessage = (event) =&gt; {
      render(event.data);
    };
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioWorkletNode (Phase Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(processor);
  processor.connect(context.destination);

  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <figure>
                <svg id="svg-animation-phase-spectrum-path-with-coordinate-and-texts" width="720" height="180"></svg>
                <figcaption>
                  <span>周波数領域 (位相スペクトル) の波形描画 (<code>440 Hz</code> の正弦波. 段差成分あり)</span>
                  <button type="button" id="button-svg-phase-spectrum-path-with-coordinate-and-texts">start</button>
                </figcaption>
              </figure>
              <p>
                しかしながら, リアルタイム処理では, 窓関数を適用しても完全に段差成分を除去することはできないので,
                本来は存在しない周波数成分の位相もシフトしてしまいます. 特に, 振幅スペクトルと異なり, 段差成分も
                <span class="math-inline">$- \pi \leq \theta \leq \pi$</span> の範囲でシフトするので, 位相スペクトルの段差成分は,
                より目立って視覚化されてしまいます (それでも, <code>440 Hz</code> の成分が
                <span class="math-inline">$- \pi \leq \theta \leq \pi$</span> の範囲でシフトしていることも確認はできると思います).
              </p>
              <p><code>440 Hz</code> 正弦波の (本来の) 位相スペクトルは以下のようになります.</p>
              <figure>
                <svg id="svg-animation-440-hz-sin-phase-spectrum-path-with-coordinate-and-texts" width="720" height="180"></svg>
                <figcaption>
                  <span>周波数領域 (位相スペクトル) の波形描画 (<code>440 Hz</code> の正弦波. 段差成分除去)</span>
                  <button type="button" id="button-svg-440-hz-sin-phase-spectrum-path-with-coordinate-and-texts">start</button>
                </figcaption>
              </figure>
            </article>
            <article id="section-sound-visualization-analyser-node-render-frequency-logarithmic-scale">
              <h5>周波数軸の対数スケール表示</h5>
              <p>
                これまでの周波数軸でのスペクトル描画の, 周波数軸はすべて線形スケールで表示していました. しかしながら, スペクトルの場合,
                低音域から高音域まで視覚化したいユースケースも多くあります. 線形スケールでは, 現実的なグラフィックス API の領域の幅
                (デスクトップサイズで横スクロールが発生しない幅として, 13 インチで <code>1400 px</code> ぐらい) では, 低音域を詳細に視覚化すると,
                高音域が視覚化できなくなり (視認できなくなり), 小音域まで視覚化すると, 低音域の詳細が視覚化できなく (視認できなく) なります.
                この問題を解決するために, 周波数軸を対数スケールで表示するケースがよくあります (すでに,
                フィルタの周波数特性などでは対数スケールで表示していますが). このセクションでは, 対数スケールで, 振幅スペクトルを描画する実装を解説します.
              </p>
              <p>
                対数スケールの尺度は様々ですが, このセクションでは, <code>10</code> を低とする<b>常用対数</b> (<code>Math.log10</code>) を利用して, 10
                帯域のグラフィックイコライザーの帯域を視覚化できるように, <code>32 Hz</code> から, 約 2 倍ずつステップした, <code>32 Hz</code>,
                <code>62.5 Hz</code>, <code>125 Hz</code>, <code>250 Hz</code>, <code>500 Hz</code>, <code>1000 Hz</code>, <code>2000 Hz</code>,
                <code>4000 Hz</code>, <code>8000 Hz</code>, <code>16000 Hz</code> の 10 つの周波数 (帯域) を対数スケールで描画します.
              </p>
              <p>
                周波数のテキストの座標は線形スケールと考え方は同じです. 10 帯域を描画対象の幅で, 等間隔に描画するので <code>width / 10</code> で算出できます
                (<code>10</code> は帯域数です).
              </p>
              <p>
                線形スケールと異なるのは, 振幅スペクトルのパスの x 座標の算出です. 描画範囲の幅を <code>1</code> として, 対数スケールでプロットする最小周波数を
                <span class="math-inline">$f_{\mathrm{min}}$</span>, 最大周波数を <span class="math-inline">$f_{\mathrm{max}}$</span> とすると, インデックス
                <span class="math-inline">$k$</span> 番目に対応する対数スケールの周波数を <span class="math-inline">$f_{k}$</span> の, プロットする x 座標
                <span class="math-inline">$x_{k}$</span> の算出は,
              </p>
              <div class="math-block">
                $x_{k} = \frac{\log_{10}\left(\frac{f_{k}}{f_{\mathrm{min}}}\right)}{\log_{10}\left(\frac{f_{\mathrm{max}}}{f_{\mathrm{min}}}\right)}$
              </div>
              <p>対数の性質より, 真数の除算は, 減算に展開可能なので, 以下の式と等価です (算出式の意味としては, こちらのほうが理解しやすいかもしれません).</p>
              <div class="math-block">
                $x_{k} = \frac{\log_{10}\left(f_{k}\right) - \log_{10}\left(f_{\mathrm{min}}\right)}{\log_{10}\left(f_{\mathrm{max}}\right) -
                \log_{10}\left(f_{\mathrm{min}}\right)}$
              </div>
              <p>
                プロット対象の周波数の描画対象の最小周波数からの x 軸のオフセット (分子) が, 描画範囲の周波数 (分母) に対して,
                どの程度の割合かを常用対数で算出しているのが上記の式の意味です (この算出の本質的な考え方は,
                <a href="#section-sound-visualization-analyser-node-render-frequency-domain-get-byte-frequency-data"
                  ><code>getFloatFrequencyData</code> メソッドと <code>getByteFrequencyData</code> メソッドの関係式</a>でも同じです).
              </p>
              <p>
                算出したプロット対象の x 座標の値 (オフセット比) は, 描画範囲の幅を <code>1</code> としているので,
                <span class="math-inline">$x_{k}$</span> に実際の描画範囲の幅を乗算すればよいでしょう.
              </p>
              <p>
                <span class="math-inline">$f_{\mathrm{min}}$</span> と <span class="math-inline">$f_{\mathrm{max}}$</span> は, このセクションの実装では,
                それぞれ <code>32</code> (<code>Hz</code>) と <code>16000</code> (<code>Hz</code>) となります.
              </p>
              <pre
                data-prismjs-copy="クリップボードにコピー"
                data-prismjs-copy-success="コピーしました"
              ><code class="language-js line-numbers">const context = new AudioContext();

const analyser = new AnalyserNode(context, { fftSize: 16384 });

const frequencies = [32, 62.5, 125, 250, 500, 1000, 2000, 4000, 8000, 16000];

const minFrequency = frequencies[0];
const maxFrequency = frequencies[frequencies.length - 1];

const ratio      = maxFrequency / minFrequency;
const log10Ratio = Math.log10(ratio);

const frequencyResolution = context.sampleRate / analyser.fftSize;

const svg = document.getElementById(&apos;svg-animation-logarithmic-scale-amplitude-spectrum&apos;);

const width  = Number(svg.getAttribute(&apos;width&apos;) ?? &apos;0&apos;);
const height = Number(svg.getAttribute(&apos;height&apos;) ?? &apos;0&apos;);

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 48;
const translateY  = 24;

const maxDecibels = 0;
const minDecibels = -60;

const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

path.setAttribute(&apos;stroke&apos;, &apos;rgb(0 0 255)&apos;);
path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

svg.appendChild(path);

const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
xRect.setAttribute(&apos;y&apos;, (height - translateY - 1).toString(10));
xRect.setAttribute(&apos;width&apos;, (width - translateX).toString(10));
xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(xRect);

const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
yRect.setAttribute(&apos;height&apos;, (height - translateX).toString(10));
yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yRect.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);

svg.appendChild(yRect);

const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

[0, -10, -20, -30, -40, -50, -60].forEach((dB, index) =&gt; {
  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  text.textContent = `${dB} dB`;

  text.setAttribute(&apos;x&apos;, &apos;44&apos;);
  text.setAttribute(&apos;y&apos;, (index * (innerHeight / 6) + translateY).toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

frequencies.forEach((frequency, index) =&gt; {
  const x = (index * (innerWidth / frequencies.length)) + translateX;
  const y = height - 8;

  const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  if (frequency &gt;= 1000) {
    text.textContent = `${Math.trunc(frequency / 1000)} kHz`;
  } else {
    text.textContent = `${frequency} Hz`;
  }

  text.setAttribute(&apos;x&apos;, x.toString(10));
  text.setAttribute(&apos;y&apos;, y.toString(10));
  text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  text.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
  text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

  g.appendChild(text);
});

svg.appendChild(g);

const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

xLabel.textContent = &apos;Frequency (Hz)&apos;;

xLabel.setAttribute(&apos;x&apos;, width.toString(10));
xLabel.setAttribute(&apos;y&apos;, (height - translateY - 8).toString(10));
xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

yLabel.textContent = &apos;Amplitude (dB)&apos;;

yLabel.setAttribute(&apos;x&apos;, translateY.toString(10));
yLabel.setAttribute(&apos;y&apos;, &apos;12&apos;);
yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(153 153 153)&apos;);
yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

svg.appendChild(xLabel);
svg.appendChild(yLabel);

let animationId = null;

const render = () =&gt; {
  const data = new Float32Array(analyser.frequencyBinCount);

  analyser.getFloatFrequencyData(data);

  path.removeAttribute(&apos;d&apos;);

  let d = &apos;&apos;;

  for (let k = 0; k &lt; analyser.frequencyBinCount; k++) {
    if (k === 0) {
      continue;
    }

    // for Chrome
    if (!Number.isFinite(data[k])) {
      continue;
    }

    const frequency = k * frequencyResolution;

    const x = ((Math.log10(frequency / minFrequency) / log10Ratio) * innerWidth) + translateX;
    const y = Math.min(((0 - data[k]) * (innerHeight / (maxDecibels - minDecibels)) - translateY), (height - translateY));

    if (x &lt; translateX) {
      continue;
    }

    if (d === &apos;&apos;) {
      d += `M${translateX} ${translateY + innerHeight}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  animationId = window.requestAnimationFrame(() =&gt; {
    render();
  });
};

const buttonElement = document.getElementById(&apos;button-svg-logarithmic-scale-amplitude-spectrum&apos;);

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context, { type: &apos;sawtooth&apos; });

  // OscillatorNode (Input) -&gt; AnalyserNode (Spectrum Analyser) -&gt; AudioDestinationNode (Output)
  oscillator.connect(analyser);
  analyser.connect(context.destination);

  oscillator.start(0);

  render();

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);

  oscillator = null;

  if (animationId) {
    window.cancelAnimationFrame(animationId);
    animationId = null;
  }

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
              <p>
                <code>getFloatFrequencyData</code> メソッド取得した, 振幅スペクトルの <code>Float32Array</code> の <code>0</code> 番目のインデックスは
                <code>0 Hz</code> に対応しますが, 対数の定義より, <b>対数の真数は <code>0</code> より大きくなければならない</b>ので,
                振幅スペクトルの座標をプロットするループで, <code>0</code> 番目のインデックスは <code>continue</code> していることに着目してください (これは,
                <code>getByteFrequencyData</code> メソッドを利用した場合でも同じです).
              </p>
              <figure>
                <svg id="svg-animation-logarithmic-scale-amplitude-spectrum" width="720" height="180"></svg>
                <figcaption>
                  <span>対数スケールによる振幅スペクトル</span>
                  <button type="button" id="button-svg-logarithmic-scale-amplitude-spectrum">start</button>
                </figcaption>
              </figure>
              <p>ちなみに, 振幅スペクトルもデシベル単位でプロットしているので, 周波数軸も振幅軸も対数スケールで視覚化していることになります.</p>
              <p>また, 周波数軸を対数スケールにするので, 位相スペクトルも同様の実装で対応できます.</p>
            </article>
          </section>
        </section>
        <section id="section-sound-visualization-audio-buffer">
          <h3>オーディオデータ全体の視覚化</h3>
          <p>
            <code>AnalyserNode</code> クラスを利用した波形描画は, リアルタイムの描画でした. 一方で, 波形エディターなど, リアルタイム性は必要なく,
            オーディオデータ全体の波形描画が必要なケースも多くあります. このセクションでは, その実装を解説します.
          </p>
          <p>
            前提条件として, <b>描画前にオーディオデータ全体を取得できる必要がある</b>ので, <code>AudioBuffer</code> インスタンスが必要となります. つまり,
            オーディオソースとしては. <code>AudioBufferSourceNode</code> を利用します.
          </p>
          <p>
            <code>MediaElementAudioSourceNode</code> は, オーディオデータ全体を取得する API がありません. 取得するには, Audio Worklet が必要で,
            バッファに再生データを格納し続ける必要があります. さらに, 一度再生完了しないと, オーディオデータ全体を取得することができません.
          </p>
          <p>
            楽曲データーのような場合, <code>MediaElementAudioSourceNode</code> を利用するのがユースケースとしては正しいのですが,
            オーディオデータ全体の描画と再生を考慮すると, <code>AudioBufferSourceNode</code> と <code>AudioBuffer</code> を利用するのが合理的と言えます.
          </p>
          <p>
            <code>AudioBuffer</code> インスタンスの <code>getChannelData</code> メソッドで取得できるオーディオデータの型は, <code>Float32Array</code> なので,
            波形描画の実装は, <code>getFloatTimeDomainData</code> メソッドを利用しする場合と同じです. 描画対象の <code>Float32Array</code> サイズが
            <code>AudioBuffer</code> インスタンスの <code>length</code> プロパティの値になっていることが異なるぐらいです.
          </p>
          <p>
            波形のプロットするサンプルの座標の算出は <code>getFloatTimeDomainData</code> と同じですが, 楽曲データの場合は
            (オーディオデータのサイズにもよりますが), 取得したすべてのサンプルをプロットしてしまうと, 描画のコストが大きすぎたり,
            メモリ不足によってエラーが発生したりします. したがって, リアルタイム処理の場合とは異なり, 波形のパス (<code>SVGPathElement</code>)
            のプロットもある程度間引くなどプロット数を減らす必要があります. オーディオデータ全体の波形の場合,
            ある程度多いサンプル数で間引いても概形にはあまり影響しません. 以下のコードでは, サンプリング周波数が <code>48000 Hz</code> の場合,
            <code>480</code> サンプルごとにプロットしています (間引き処理では, 剰余を算出するので, 1
            以上の整数になるようにキャストしていることにも着目してください. 変数から算出する場合は整数になっていなかったなど, うっかりがあるので注意が必要です).
            時間軸のテキストを描画する場合も同じく間引きますが, リアルタイムの描画より多くのサンプル数で間引きます. 以下のコードでは,
            <code>60 sec</code> ごとに時間軸のテキストを描画しています.
          </p>
          <p>
            また, Autoplay Policy によって, ユーザーインタラクティブなイベント発火後に Autoplay Policy を解除しないと,
            <code>AudioBuffer</code> インスタンスを取得できない (<code>decodeAudioData</code> メソッドが機能しない) ので, <code>click</code> イベント後に
            Autoplay Policy を解除して, <code>AudioBuffer</code> インスタンスを取得して, 1 度だけ波形描画しています.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let source      = null;
let audioBuffer = null;

let rendered = false;

const width  = 720;
const height = 180;

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 24;
const translateY  = 24;

const render = (svg, data, channelNumber, sampleRate, length) =&gt; {
  const samplingPeriod = 1 / sampleRate;

  const plotInterval = Math.max(Math.trunc(sampleRate / 10), 1);  // Plot path (x, y) every ${sampleRate / 10} sample
  const textInterval = Math.max(Math.trunc(60 * sampleRate), 1);  // Render text every 60 sec

  const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; length; n++) {
    if ((n % plotInterval) !== 0) {
      continue;
    }

    const x = n * (innerWidth / length) + translateX;
    const y = (1 - data[n]) * (innerHeight / 2) + translateY;

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  path.setAttribute(&apos;stroke&apos;, &apos;rgba(0 0 255 / 30%)&apos;);
  path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
  path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
  path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
  path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

  svg.appendChild(path);

  const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

  xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
  xRect.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY - 1).toString(10));
  xRect.setAttribute(&apos;width&apos;, innerWidth.toString(10));
  xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
  xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  xRect.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);

  svg.appendChild(xRect);

  const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

  yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
  yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
  yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
  yRect.setAttribute(&apos;height&apos;, innerHeight.toString(10));
  yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  yRect.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);

  svg.appendChild(yRect);

  const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

  [1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
    const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

    text.textContent = amplitude.toFixed(1);
    text.setAttribute(&apos;x&apos;, (translateX - 4).toString(10));
    text.setAttribute(&apos;y&apos;, ((1 - amplitude) * (innerHeight / 2) + translateY).toString(10));
    text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
    text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
    text.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
    text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

    g.appendChild(text);
  });

  for (let n = 0; n &lt; length; n++) {
    if ((n % textInterval) !== 0) {
      continue;
    }

    const x = n * (innerWidth / length) + translateX;

    const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

    text.textContent = `${n * samplingPeriod} sec`;

    text.setAttribute(&apos;x&apos;, (x + 4).toString(10));
    text.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY + 12).toString(10));
    text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
    text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
    text.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
    text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

    g.appendChild(text);
  }

  svg.appendChild(g);

  const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  xLabel.textContent = &apos;Time&apos;;

  xLabel.setAttribute(&apos;x&apos;, innerWidth.toString(10));
  xLabel.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY - 8).toString(10));
  xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
  xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

  const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  yLabel.textContent = &apos;Amplitude&apos;;

  yLabel.setAttribute(&apos;x&apos;, (translateX / 2).toString(10));
  yLabel.setAttribute(&apos;y&apos;, (translateY / 2).toString(10));
  yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
  yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

  svg.appendChild(xLabel);
  svg.appendChild(yLabel);
};

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const onClick = async () =&gt; {
      buttonElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);

      if (context.state !== &apos;running&apos;) {
        await context.resume();
      }

      if (audioBuffer === null) {
        buttonElement.textContent = &apos;decoding ...&apos;;

        audioBuffer = await context.decodeAudioData(arrayBuffer);
      }

      if (source === null) {
        const sampleRate       = audioBuffer.sampleRate;
        const length           = audioBuffer.length;
        const duration         = audioBuffer.duration;
        const numberOfChannels = audioBuffer.numberOfChannels;

        if (!rendered) {
          buttonElement.textContent = &apos;rendering ...&apos;;

          for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
            render(document.getElementById(`svg-animation-time-overview-channel-${channelNumber}`), audioBuffer.getChannelData(channelNumber), channelNumber, sampleRate, length);
          }

          rendered = true;
        }

        source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

        // AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
        source.connect(context.destination);

        // Start at 0 sec always
        source.start(0);

        buttonElement.textContent = &apos;stop&apos;;
      } else {
        source.stop(0);

        source.disconnect();

        source = null;

        buttonElement.textContent = &apos;start&apos;;
      }

      buttonElement.removeAttribute(&apos;disabled&apos;);
    };

    const buttonElement = document.getElementById(&apos;button-svg-animation-time-overview&apos;);

    buttonElement.addEventListener(&apos;click&apos;, onClick);
  })
  .catch((error) =&gt; {
    // error handling
  })</code></pre>
          <p>
            波形描画, および, グラフィックス API に十分に慣れてきた場合は, 少しチャレンジングな実装として,
            オーディオデータの現在の再生位置までを半透明の矩形でオーバーレイさせる実装を理解してみてください
            (オーディオデータの再生位置の視覚化の実装の一例です).
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

let source      = null;
let audioBuffer = null;

let rendered = false;

let animationId = null;
let startTime   = 0;

const width  = 720;
const height = 180;

const innerWidth  = width - 48;
const innerHeight = height - 48;
const translateX  = 24;
const translateY  = 24;

const currentTimeRects = [];

const render = (svg, data, channelNumber, sampleRate, length) =&gt; {
  const samplingPeriod = 1 / sampleRate;

  const plotInterval = Math.max(Math.trunc(sampleRate / 10), 1);  // Plot path (x, y) every ${sampleRate / 10} sample
  const textInterval = Math.max(Math.trunc(60 * sampleRate), 1);  // Render text every 60 sec

  const path = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;path&apos;);

  let d = &apos;&apos;;

  for (let n = 0; n &lt; length; n++) {
    if ((n % plotInterval) !== 0) {
      continue;
    }

    const x = n * (innerWidth / length) + translateX;
    const y = (1 - data[n]) * (innerHeight / 2) + translateY;

    if (d === &apos;&apos;) {
      d += `M${x} ${y}`;
    } else {
      d += ` L${x} ${y}`;
    }
  }

  path.setAttribute(&apos;d&apos;, d);

  path.setAttribute(&apos;stroke&apos;, &apos;rgba(0 0 255 / 30%)&apos;);
  path.setAttribute(&apos;fill&apos;, &apos;none&apos;);
  path.setAttribute(&apos;stroke-width&apos;, &apos;2&apos;);
  path.setAttribute(&apos;stroke-linecap&apos;, &apos;round&apos;);
  path.setAttribute(&apos;stroke-linejoin&apos;, &apos;miter&apos;);

  svg.appendChild(path);

  const xRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

  xRect.setAttribute(&apos;x&apos;, translateX.toString(10));
  xRect.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY - 1).toString(10));
  xRect.setAttribute(&apos;width&apos;, innerWidth.toString(10));
  xRect.setAttribute(&apos;height&apos;, &apos;2&apos;);
  xRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  xRect.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);

  svg.appendChild(xRect);

  const yRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

  yRect.setAttribute(&apos;x&apos;, translateX.toString(10));
  yRect.setAttribute(&apos;y&apos;, translateY.toString(10));
  yRect.setAttribute(&apos;width&apos;, &apos;2&apos;);
  yRect.setAttribute(&apos;height&apos;, innerHeight.toString(10));
  yRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  yRect.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);

  svg.appendChild(yRect);

  const g = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;g&apos;);

  [1.0, 0.0, -1.0].forEach((amplitude, index) =&gt; {
    const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

    text.textContent = amplitude.toFixed(1);
    text.setAttribute(&apos;x&apos;, (translateX - 4).toString(10));
    text.setAttribute(&apos;y&apos;, ((1 - amplitude) * (innerHeight / 2) + translateY).toString(10));
    text.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
    text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
    text.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
    text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

    g.appendChild(text);
  });

  for (let n = 0; n &lt; length; n++) {
    if ((n % textInterval) !== 0) {
      continue;
    }

    const x = n * (innerWidth / length) + translateX;

    const text = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

    text.textContent = `${n * samplingPeriod} sec`;

    text.setAttribute(&apos;x&apos;, (x + 4).toString(10));
    text.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY + 12).toString(10));
    text.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
    text.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
    text.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
    text.setAttribute(&apos;font-size&apos;, &apos;12px&apos;);

    g.appendChild(text);
  }

  svg.appendChild(g);

  const xLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  xLabel.textContent = &apos;Time&apos;;

  xLabel.setAttribute(&apos;x&apos;, innerWidth.toString(10));
  xLabel.setAttribute(&apos;y&apos;, ((innerHeight / 2) + translateY - 8).toString(10));
  xLabel.setAttribute(&apos;text-anchor&apos;, &apos;end&apos;);
  xLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  xLabel.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
  xLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

  const yLabel = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;text&apos;);

  yLabel.textContent = &apos;Amplitude&apos;;

  yLabel.setAttribute(&apos;x&apos;, (translateX / 2).toString(10));
  yLabel.setAttribute(&apos;y&apos;, (translateY / 2).toString(10));
  yLabel.setAttribute(&apos;text-anchor&apos;, &apos;start&apos;);
  yLabel.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  yLabel.setAttribute(&apos;fill&apos;, &apos;rgb(99 99 99)&apos;);
  yLabel.setAttribute(&apos;font-size&apos;, &apos;14px&apos;);

  svg.appendChild(xLabel);
  svg.appendChild(yLabel);

  const currentTimeRect = document.createElementNS(&apos;http://www.w3.org/2000/svg&apos;, &apos;rect&apos;);

  currentTimeRect.setAttribute(&apos;x&apos;, translateX.toString(10));
  currentTimeRect.setAttribute(&apos;y&apos;, translateY.toString(10));
  currentTimeRect.setAttribute(&apos;width&apos;, &apos;0&apos;);
  currentTimeRect.setAttribute(&apos;height&apos;, innerHeight.toString(10));
  currentTimeRect.setAttribute(&apos;stroke&apos;, &apos;none&apos;);
  currentTimeRect.setAttribute(&apos;fill&apos;, &apos;rgba(255 0 255 / 10%)&apos;);

  svg.appendChild(currentTimeRect);

  currentTimeRects.push(currentTimeRect);
};

fetch(&apos;./assets/medias/Schubert-Symphony-No8-Unfinished-1st-2020-VR.mp3&apos;)
  .then((response) =&gt; {
    return response.arrayBuffer();
  })
  .then(async (arrayBuffer) =&gt; {
    const onClick = async () =&gt; {
      buttonElement.setAttribute(&apos;disabled&apos;, &apos;disabled&apos;);

      if (context.state !== &apos;running&apos;) {
        await context.resume();
      }

      if (audioBuffer === null) {
        buttonElement.textContent = &apos;decoding ...&apos;;

        audioBuffer = await context.decodeAudioData(arrayBuffer);
      }

      if (source === null) {
        const sampleRate       = audioBuffer.sampleRate;
        const length           = audioBuffer.length;
        const duration         = audioBuffer.duration;
        const numberOfChannels = audioBuffer.numberOfChannels;

        if (!rendered) {
          buttonElement.textContent = &apos;rendering ...&apos;;

          for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
            render(document.getElementById(`svg-animation-time-overview-channel-${channelNumber}`), audioBuffer.getChannelData(channelNumber), channelNumber, sampleRate, length);
          }

          rendered = true;
        }

        source = new AudioBufferSourceNode(context, { buffer: audioBuffer });

        // AudioBufferSourceNode (Input) -&gt; AudioDestinationNode (Output)
        source.connect(context.destination);

        startTime = context.currentTime;

        // Start at 0 sec always
        source.start(0);

        const animate = () =&gt; {
          const currentTime = context.currentTime - startTime;

          // Ended ?
          if (currentTime &gt; duration) {
            for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
              currentTimeRects[channelNumber].setAttribute(&apos;width&apos;, &apos;0&apos;);
            }

            window.cancelAnimationFrame(animationId);
            animationId = null;

            buttonElement.textContent = &apos;start&apos;;

            return;
          }

          const width = (currentTime * sampleRate) * (innerWidth / length);

          for (let channelNumber = 0; channelNumber &lt; numberOfChannels; channelNumber++) {
            currentTimeRects[channelNumber].setAttribute(&apos;width&apos;, width.toString(10));
          }

          animationId = window.requestAnimationFrame(animate);
        };

        animationId = animate();

        buttonElement.textContent = &apos;stop&apos;;
      } else {
        source.stop(0);

        source.disconnect();

        source = null;

        window.cancelAnimationFrame(animationId);
        animationId = null;

        buttonElement.textContent = &apos;start&apos;;
      }

      buttonElement.removeAttribute(&apos;disabled&apos;);
    };

    const buttonElement = document.getElementById(&apos;button-svg-animation-time-overview&apos;);

    buttonElement.addEventListener(&apos;click&apos;, onClick);
  })
  .catch((error) =&gt; {
    // error handling
  })</code></pre>
          <p>
            理想的には, 再生・停止時にも現在の再生位置をキープしたまま, 再生とオーバーレイの矩形の描画ができるとよいのですが, メインスレッドのみの実装では,
            基準となる <code>AudioContext</code> インスタンスの <code>currentTime</code> プロパティと現在の
            <code>currentTime</code> プロパティの減算から再生位置を算出するためのコールバック関数を, <code>requestAnimationFrame</code> メソッドや
            <code>setInterval</code> メソッドで呼び出すほど, 再生位置が少しずつずれてしまいます. この問題を解決するには, タイマー処理 (更新処理) を
            <code>Web Workers</code> でバックグラウンドのスレッドで実行するか,
            <code>AudioWorkletProcessor</code> で再生されたサンプル数などから現在の再生位置をメインスレッドと同期する実装が考えられます
            (これと本質的に同様のことが,
            <a href="https://web.dev/articles/audio-scheduling" target="_blank" rel="noopener noreferrer">A tale of two clocks - 二つの時計についての物語 -</a>
            においても言及されています. ちなみに, 記事の内容は Web Audio API の初期から公開されており, 比較的有名な記事です). あるいは, オーディオデータの再生は
            <code>MediaElementAudioSourceNode</code> を利用して, <code>AudioBuffer</code> を波形描画のみに利用するのも 1 つの解決方法です.
            技術的にはどれが正解というのはありません. 実装するアプリケーションによって最適解が変わる判断の問題と言えそうです (例えば, エフェクターのために,
            AudioWorklet を利用することが必須なら, 前者の実装のほうが API がまとまってよいかもしれません).
          </p>
          <figure>
            <dl>
              <dt>Left Channel (Channel Number is <code>0</code>)</dt>
              <dd><svg id="svg-animation-time-overview-channel-0" width="720" height="180"></svg></dd>
              <dt>Right Channel (Channel Number is <code>1</code>)</dt>
              <dd><svg id="svg-animation-time-overview-channel-1" width="720" height="180"></svg></dd>
            </dl>
            <figcaption>
              <span>オーディオデータ全体の視覚化 (現在の再生位置をオーバーレイで視覚化)</span>
              <button type="button" id="button-svg-animation-time-overview">start</button>
            </figcaption>
          </figure>
        </section>
        <section id="section-sound-visualization-periodic-wave">
          <h3>PeriodicWave</h3>
          <p>
            コンピュータにおける音合成の 1 つとして, <b>波形テーブル合成</b>があります. 波形データ 1 周期分をコンピュータに内臓しておき, それを繰り返す, さらに,
            複数の波形データを合成するといった手法で, コンピュータで音を生成する手段です.
            <abbr title="Pulse Code Modulation">PCM</abbr> のようにレコードされた音を保存しないので, 少ないメモリで可能な限り, 様々な音 (波形)
            を生成する手法です.
          </p>
          <p>
            <b><code>PeriodicWave</code></b> クラスは, 波形テーブルを定義することによって任意の波形の生成を可能にします. ただし,
            現実世界の波形テーブル合成とは異なり, 1 周期分の波形データではなく, スペクトルのもとになる複素数の実部の値と虚部の値を設定する, すなわち,
            <b>周波数領域で波形テーブルを定義しておくという Web Audio API 特有の仕様となっています</b> (したがって,
            スペクトルについて一定以上の理解をしておくのがよいので, <a href="#section-oscillator-node">OscillatorNode</a> のセクションではなく,
            こちらのセクションに解説を記載しました).
          </p>
          <p>
            <code>PeriodicWave</code> コンストラクタ呼び出しの第 2 引数 (<b><code>PeriodicWaveOptions</code></b> 型の
            <b><code>real</code></b> プロパティで実部の値を, <b><code>imag</code></b> プロパティで虚部の値を設定します) でこれらを設定します
            (ファクトリメソッドを利用する場合, 実部の値を第 1 引数に, 虚部の値を第 2 引数に指定します).
          </p>
          <p>
            実部・虚部の値はどちらも <code>Float32Array</code> で設定します. ただし, 任意の <code>Float32Array</code> を設定することはできず, 以下 2
            つの制約があります.
          </p>
          <ul>
            <li>実部・虚部の <code>Float32Array</code> のサイズは, 同じである必要があります</li>
            <li>虚部の <code>Float32Array</code> のインデックス <code>0</code> 番目は, (使用禁止の意味で) 必ず <code>0</code> にしておきます</li>
            <li>
              実部・虚部の <code>Float32Array</code> のサイズは, <code>8192</code> 要素以下にしておく (ブラウザによっては,
              それより多いサイズをサポートしているかもしれませんが, 仕様で保証されるのは <code>8192</code> までです
            </li>
          </ul>
          <p>
            上記の 2 つの制約は, フーリエ級数の制約から設定されています (最後の 1 つは実装上の制約でしかありません).
            級数の部分から同じサイズである必要があること, また, <span class="math-inline">$\frac{a_{0}}{2}$</span> の項が存在することから, 虚部の
            <code>0</code> 項目は使用されないので 2 つ目の制約があります (オイラーの公式からも導出できますが, 正弦波の項は虚部に対応します).
          </p>
          <div class="math-block">
            <!-- prettier-ignore -->
            $
              \begin{flalign}
                &f\left(t\right) = \frac{a_{0}}{2} + \sum_{n=1}^{\infty}\left(a_{n}\cos\left(n\frac{2 \pi}{T}t\right) + b_{n}\sin\left(n\frac{2 \pi}{T}t\right)\right) \\
                &a_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\cos\left(n\frac{2 \pi}{T}t\right)dt \\
                &b_{n} = \frac{2}{T}\int_{-\frac{T}{2}}^{\frac{T}{2}}f\left(t\right)\sin\left(n\frac{2 \pi}{T}t\right)dt \\
              \end{flalign}
            $
          </div>
          <p>
            実部の <code>Float32Array</code> のインデックス <code>0</code> 番目は, (数学的には) フーリエ級数の
            <span class="math-inline">$\frac{a_{0}}{2}$</span> の項に対応します (仕様では, DC-offset (DC オフセット) と呼ばれています) が,
            実装上は使われないので, この要素も <code>0</code> にするのが定石です.
          </p>
          <p>以上の制約を考慮して, <code>PeriodicWave</code> インスタンスの生成コードは以下のようになります.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const NUMBER_OF_PERIODIC_WAVES = 10;

const reals = new Float32Array(NUMBER_OF_PERIODIC_WAVES);
const imags = new Float32Array(NUMBER_OF_PERIODIC_WAVES);

reals[0] = 0;
imags[0] = 0;

const real = 1;
const imag = 1;

for (let n = 1; n &lt; NUMBER_OF_PERIODIC_WAVES; n++) {
  reals[n] = real / n;
  imags[n] = imag / n;
}

const periodicWave = new PeriodicWave(context, { real: reals, imag: imags });

// If use `createPeriodicWave`
// periodicWave = context.createPeriodicWave(reals, image);</code></pre>
          <p>
            実部・虚部の値は設定の 1 例です. また, Typed Array (ここでは, <code>Float32Array</code>) は, インスタンス生成時に要素はすべて
            <code>0</code> で初期化されるので, 実部・虚部の <code>Float32Array</code> のインデックス <code>0</code> 番目の設定は不要ですが,
            制約を強調するためにあえて設定しています.
          </p>
          <p>
            <code>PeriodicWave</code> インスタンスを生成したら, 最後に <code>OscillatorNode</code> インスタンスの
            <b><code>setPeriodicWave</code></b> の引数に設定します. このとき,
            <b><code>OscillatorType</code> は, <code>&apos;sine&apos;</code> である必要があります</b>.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const NUMBER_OF_PERIODIC_WAVES = 10;

const reals = new Float32Array(NUMBER_OF_PERIODIC_WAVES);
const imags = new Float32Array(NUMBER_OF_PERIODIC_WAVES);

reals[0] = 0;
imags[0] = 0;

const real = 1;
const imag = 1;

for (let n = 1; n &lt; NUMBER_OF_PERIODIC_WAVES; n++) {
  reals[n] = real / n;
  imags[n] = imag / n;
}

const periodicWave = new PeriodicWave(context, { real: reals, imag: imags });

// If use `createPeriodicWave`
// periodicWave = context.createPeriodicWave(reals, image);

// `OscillatorType` must be &apos;sine&apos;
const oscillator = new OscillatorNode(context);

oscillator.setPeriodicWave(periodicWave);</code></pre>
          <p>
            <code>OscillatorType</code> 列挙型のとりうる値として, <b><code>&apos;custom&apos;</code></b> がありますが, これは,
            <code>PeriodicWave</code> インスタンスを <code>OscillatorNode</code> インスタンスの
            <code>setPeriodicWave</code> メソッドで設定することでオートマティックに <code>OscillatorType</code> が <code>&apos;custom&apos;</code> となります.
          </p>
          <p>
            1 つの実用的な利用例として, 電気オルガンのような音色を生成することができます. 具体的には, (オイラーの公式より) 虚部の項は, 正弦波の項であり,
            インデックス <code>1</code> 番目が基本周波数となります. インデックス <code>2</code> 番目以降は, それぞれ, 2 倍音, 3 倍音, 4 倍音 ...
            と設定することになります. 同様に, 実部の項は, 余弦波の項であり, インデックス <code>1</code> 番目から順に, 基本周波数, 2 倍音, 3 倍音, 4 倍音 ...
            と設定することが可能です. 正弦波と余弦波は位相の違いでしかなく, 音色は (知覚できるようなレベルで) 変わりませんが,
            倍音が多くなるほど時間領域の波形はかなり違った形になってきます). また, デフォルトでは, これらの値は正規化されるので,
            倍音成分の比率のみを考慮して設定すればよいです. 基本周波数は, <code>OscillatorNode</code> インスタンスの
            <code>frequency</code> プロパティで設定できます.
          </p>
          <p>以下のコードは, 虚部 (正弦波) の項のみを利用して, 4 倍音を生成して, 電気オルガンのような音色を生成する実装例です.</p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const numberOfOvertones = 4;

const reals = new Float32Array(numberOfOvertones);
const imags = new Float32Array(numberOfOvertones);

reals[0] = 0;
imags[0] = 0;

for (let n = 1; n &lt;= numberOfOvertones; n++) {
  reals[n] = 0;
  imags[n] = n === 1 ? 1 : 0.5;
}

const periodicWave = new PeriodicWave(context, { real: reals, imag: imags });

// If use `createPeriodicWave`
// periodicWave = context.createPeriodicWave(reals, image);

// `OscillatorType` must be &apos;sine&apos;
const oscillator = new OscillatorNode(context, { frequency: 220 });

oscillator.setPeriodicWave(periodicWave);

// OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
oscillator.connect(context.destination);

// Start immediately
oscillator.start(0);

// Stop after 10 sec
oscillator.stop(context.currentTime + 10);</code></pre>
          <p>
            以下のビジュアライザーは, ユーザーインタラクティブに, <code>PeriodicWave</code> の実部 (余弦波) と虚部 (正弦波) の基本周波数と 8
            倍音までの比率を設定できるようにして, 時間領域の波形と振幅スペクトルを視覚化しています. 倍音成分を同じにしても, 実部 (余弦波) の項のみと, 虚部
            (正弦波) の項のみでは, 音色は知覚できるほどの差異がなくても, 時間領域の波形がかなり違った波形になることも確認できます.
          </p>
          <div class="app-container app-periodic-wave">
            <div class="app-headline">
              <button type="button" id="button-periodic-wave">start</button>
              <label>
                <span>OscillatorNode frequency (Fundamental frequency)</span>
                <input type="range" id="range-periodic-wave-oscillator-frequency" value="440" min="220" max="880" step="1" />
                <span id="print-periodic-wave-oscillator-frequency-value">440 Hz</span>
              </label>
            </div>
            <figure>
              <svg id="svg-animation-periodic-wave" width="1080" height="240"></svg>
              <figcaption><code>PeriodicWave</code> の波形と振幅スペクトル</figcaption>
            </figure>
            <div class="grid-2x2-layout">
              <dl>
                <dt>real (cosine) terms</dt>
                <dt><label for="range-periodic-wave-real-1">1 (Fundamental Frequency)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-1" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-1">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-2">2 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-2" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-2">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-3">3 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-3" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-3">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-4">4 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-4" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-4">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-5">5 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-5" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-5">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-6">6 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-6" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-6">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-7">7 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-7" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-7">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-real-8">8 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-real-8" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-real-8">0.00</span>
                </dd>
              </dl>
              <dl>
                <dt>imag (sine) terms</dt>
                <dt><label for="range-periodic-wave-imag-1">1 (Fundamental Frequency)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-1" value="1" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-1">1.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-2">2 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-2" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-2">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-3">3 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-3" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-3">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-4">4 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-4" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-4">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-5">5 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-5" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-5">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-6">6 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-6" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-6">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-7">7 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-7" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-7">0.00</span>
                </dd>
                <dt><label for="range-periodic-wave-imag-8">8 (Overtone)</label></dt>
                <dd>
                  <input type="range" id="range-periodic-wave-imag-8" value="0" min="0" max="1" step="0.05" />
                  <span id="print-periodic-wave-imag-8">0.00</span>
                </dd>
              </dl>
            </div>
          </div>
          <section id="section-sound-visualization-periodic-wave-oscillator-types">
            <h4>OscillatorType と PeriodicWave</h4>
            <p>
              実は, <code>OscillatorNode</code> で生成できる基本波形, すなわち, 正弦波, 矩形波, ノコギリ波, 三角波も
              <code>PeriodicWave</code> によって生成することが仕様で定義されています.
            </p>
            <table class="nowrap-table">
              <caption>
                <code>OscillatorType</code>
                の
                <code>PeriodicWave</code>
                の係数定義
              </caption>
              <thead>
                <tr>
                  <th scope="col">OscillatorType</th>
                  <th scope="col">Formula</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td><code>&apos;sine&apos;</code></td>
                  <td>
                    <span class="math-inline">$a\left[n\right] = 0 \quad \left(n \ge 0\right)$</span><br /><span class="math-inline">$b\left[1\right] = 1$</span><br /><span class="math-inline">$b\left[n\right] = 0 \quad \left(n \ge 2\right)$</span>
                  </td>
                </tr>
                <tr>
                  <td><code>&apos;square&apos;</code></td>
                  <td>
                    <span class="math-inline">$a\left[n\right] = 0 \quad \left(n \ge 0\right)$</span><br /><span class="math-inline"
                      >$b\left[n\right] = \frac{2}{n \pi}\left(1 - \left(-1\right)^{n}\right) \quad \left(n \ge 1\right)$</span>
                  </td>
                </tr>
                <tr>
                  <td><code>&apos;sawtooth&apos;</code></td>
                  <td>
                    <span class="math-inline">$a\left[n\right] = 0 \quad \left(n \ge 0\right)$</span><br /><span class="math-inline"
                      >$b\left[n\right] = \left(-1\right)^{n + 1}\frac{2}{n \pi} \quad \left(n \ge 1\right)$</span>
                  </td>
                </tr>
                <tr>
                  <td><code>&apos;triangle&apos;</code></td>
                  <td>
                    <span class="math-inline">$a\left[n\right] = 0 \quad \left(n \ge 0\right)$</span><br /><span class="math-inline"
                      >$b\left[n\right] = \frac{8 \sin\left(\frac{n \pi}{2}\right)}{\left(n \pi\right)^{2}} \quad \left(n \ge 1\right)$</span>
                  </td>
                </tr>
              </tbody>
            </table>
            <p>
              上記の表の数式は, 仕様から引用した数式を記載していますが, <span class="math-inline">$a\left[n\right]$</span> は, 実部 (余弦波) の項であり,
              これはすべての <code>OscillatorType</code> で <code>0</code> です. すなわち, 虚部 (正弦波) の項のみを利用して, (ブラウザレンダリングエンジンが)
              それぞれの <code>OscillatorType</code> を実装することが仕様で決められています. 正弦波は,
              <span class="math-inline">$b\left[1\right]$</span> 以外の要素はすべて <code>0</code> であり, まさにこれは倍音成分のない,
              基本周波数のみで生成される<b>純音</b>であることを意味しています.
            </p>
            <p>
              また, <code>OscillatorNode</code> で直接生成される基本波形は振幅が正規化されていますが, <code>PeriodicWaveOptions</code> の
              <b><code>PeriodicWaveConstraints</code></b> 型の <b><code>disableNormalization</code></b> プロパティを <code>true</code> に設定して
              (デフォルト値は, <code>false</code>), <code>PeriodicWave</code> インスタンス生成時の, 虚部 (正弦波) の項の
              <code>Float32Array</code> を上記の定義にしたがってアプリケーション側で設定することで, 正規化しない基本波形を生成することも可能です
              (ユースケースはわからないですが, 正規化しない基本波形の生成として, 仕様では記載があります).
            </p>
          </section>
          <article id="section-sound-visualization-periodic-sound-synthesis-types">
            <h4>音合成の種類</h4>
            <p>音合成, つまり, シンセサイザーにはいくつかの種類があります. 以下に, よくピックアップされる合成を記載します.</p>
            <dl>
              <dt>加算合成</dt>
              <dd>フーリエ級数をまさにそのまま実装した方式で, 周波数や振幅の異なる正弦波を十分に加算する音合成</dd>
              <dt>減算合成</dt>
              <dd>倍音の多い原音 (矩形波, ノコギリ波, 三角波をフィルタリングして倍音成分を減算する音合成</dd>
              <dt>FM (Frequency Modulation) 合成</dt>
              <dd>
                <a href="#section-effectors-tremolo-and-ringmodulator">トレモロ・リングモジュレーター</a>のセクションで少し解説していますが, キャリア (搬送波)
                とモジュレーター (変調波) によって, 金属的な音色を生成する音合成
              </dd>
              <dt>グラニュラー (Granular) 合成</dt>
              <dd>原音をグレイン (音の粒) と呼ばれる, <code>10 msec</code> - <code>50 msec</code> に分解して, グレインを加工して, 再度合成する音合成</dd>
            </dl>
            <p>
              波形テーブル合成は, グラニュラー合成に比較的近い音合成と言えます. 大きく異なるのは, 分解の処理がなく, コンピュータにごく短時間の (厳密には, 1
              周期分の) 波形データが内蔵されていることです (さらに, この波形の種類もリソースが許容する限り, 十分な種類の波形を内蔵しています).
            </p>
            <p>
              上記の記載はあくまでごく簡潔に説明しただけなので, 詳細は,
              <a href="https://blog.landr.com/ja/types-of-synthesis-wavetable-fm-synthesis-and-others-explained/" target="_blank" rel="noopener noreferrer"
                >合成の種類</a>などによくまとめられているので参考にしてください.
            </p>
          </article>
        </section>
      </section>
      <section id="section-device">
        <h2>Web Audio API とオーディオデバイス</h2>
        <section id="section-device-media-stream-audio-destination-node">
          <h3>MediaStreamAudioDestinationNode</h3>
          <p>
            <b><code>MediaStreamAudioDestinationNode</code></b> クラスが実質的に有用なのは, WebRTC と併用する場合になります. もう少し具体的には,
            <b>オーディオグラフの出力先を <code>MediaStream</code> インスタンスとして利用したい場合です</b>. ちなみに, 音源として対象のデバイスの
            <code>MediaStream</code> インスタンスを利用する場合は, <code>MediaStreamAudioSourceNode</code> クラスを利用するのはすでに解説しましたが,
            それと対になるクラスという理解だと, 出力先を対象のデバイスに設定したいユースケースと考えてしまうかもしれませんが, それは, 次のセクションで解説する
            <code>sinkId</code> と <code>setSinkId</code> メソッドで可能になります.
          </p>
          <p>
            WebRTC で利用する場合, <code>RTCPeerConnetion</code> (<abbr title="Peer to Peer">P2P</abbr> 通信の役割を担うクラス) インスタンスを生成して,
            <code>addTrack</code> メソッドの第 2 引数に指定します (ちなみに, 第 1 引数のトラックは, <code>MediaStream</code> インスタンスから取得できます).
            以下は, イメージとしてのコード片です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const oscillator  = new OscillatorNode(context);
const destination = new MediaStreamAudioDestinationNode(context);

// If use `createMediaStreamDestination`
// const destination = context.createMediaStreamDestination();

// Audio Graph (OscillatorNode (Input) -&gt; MediaStreamAudioDestinationNode (Output))
oscillator.connect(destination);

// ...

const config = { iceServers: [/* ... */] };
const peerConnection = new RTCPeerConnection(configuration);

destination.stream.getTracks().forEach((track) =&gt; {
  peerConnection.addTrack(track, destination.stream);
});

// If specific track
// const tracks      = destination.stream.getTracks();
// const transceiver = peerConnection.addTransceiver(tracks[0]);</code></pre>
          <p>
            WebRTC を利用しない場合のよくあるユースケースとしては,
            <a href="https://www.w3.org/TR/mediastream-recording/" target="_blank" rel="noopener noreferrer"
              ><b><code>MediaRecorder</code></b></a>
            クラスのコンストラクタの引数に, <code>MediaStream</code> インスタンスである <code>MediaStreamAudioDestinationNode</code> の
            <b><code>stream</code></b> プロパティを指定することで, <code>MediaStreamAudioDestinationNode</code> に出力される音を録音する機能の実装です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;button type=&quot;button&quot; id=&quot;button-media-stream-audio-destination-node-and-media-recorder&quot;&gt;start&lt;/button&gt;
&lt;a href=&quot;&quot; id=&quot;anchor-media-stream-audio-destination-node-and-media-recorder&quot;&gt;download&lt;/a&gt;
&lt;div&gt;
  &lt;audio id=&quot;audio-media-stream-audio-destination-node-and-media-recorder&quot; controls&gt;&lt;/audio&gt;
&lt;/div&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const destination = new MediaStreamAudioDestinationNode(context);

// If use `createMediaStreamDestination`
// const destination = context.createMediaStreamDestination();

const recorder = new MediaRecorder(destination.stream);

const chunks = [];

const buttonElement = document.getElementById(&apos;button-media-stream-audio-destination-node-and-media-recorder&apos;);
const audioElement  = document.getElementById(&apos;audio-media-stream-audio-destination-node-and-media-recorder&apos;);
const anchorElement = document.getElementById(&apos;anchor-media-stream-audio-destination-node-and-media-recorder&apos;);

let oscillator = null;

buttonElement.addEventListener(&apos;click&apos;, async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator === null) {
    oscillator = new OscillatorNode(context);

    // OscillatorNode (Input) -&gt; MediaStreamAudioDestinationNode (Output / Recorder)
    oscillator.connect(destination);

    // OscillatorNode (Input) -&gt; AudioDestinationNode (Output / Speaker)
    oscillator.connect(audiocontext.destination);

    recorder.start();
    oscillator.start(0);

    buttonElement.textContent = &apos;stop&apos;;
  } else {
    recorder.stop();
    oscillator.stop(0);

    oscillator = null;

    buttonElement.textContent = &apos;start&apos;;
  }
});

recorder.ondataavailable = (event) =&gt; {
  chunks.push(event.data);
};

recorder.onstop = async () =&gt; {
  const blob = new Blob(chunks, { type: &apos;audio/ogg; codecs=opus&apos; });
  const url  = window.URL.createObjectURL(blob);

  audioElement.setAttribute(&apos;src&apos;, url);

  anchorElement.setAttribute(&apos;href&apos;, url);
  anchorElement.setAttribute(&apos;download&apos;, &apos;sine.ogg&apos;);
};</code></pre>
          <div class="app-container app-media-stream-audio-destination-node-and-media-recorder">
            <div class="flexbox">
              <button type="button" id="button-media-stream-audio-destination-node-and-media-recorder">start</button>
              <a href="" id="anchor-media-stream-audio-destination-node-and-media-recorder">download</a>
            </div>
            <div>
              <audio id="audio-media-stream-audio-destination-node-and-media-recorder" controls></audio>
            </div>
          </div>
        </section>
        <section id="section-device-sink-id">
          <h3>sinkId と setSinkId メソッド</h3>
          <p>
            Web Audio API 1.1 では, <b><code>sinkId</code></b> と <b><code>setSinkId</code></b> メソッドが仕様に追加されて,
            明示的に出力先のオーディオをデバイスを指定できるようになりました. もっとも, <code>sinkId</code> は, <code>MediaDeviceInfo</code> に定義されている
            <code>deviceId</code> プロパティです. 実装としては, 対象のデバイスの <code>sinkId</code> を <code>AudioContext</code> インスタンスの
            <code>setSinkId</code> メソッドの引数に指定するだけです.
          </p>
          <p>
            <code>setSinkId</code> メソッド実行は, <b>非同期処理</b>です (したがって, 返り値は <code>Promise</code> です. 非同期処理なので,
            イベントインターフェース (<code>sinkId</code> 変更完了時に発火する, <b><code>onsinkchange</code></b> イベントインターフェースと, 失敗時に発火する
            <b><code>onerror</code></b> イベントインターフェース) も <code>AudioContext</code> に定義されています).
          </p>
          <p>
            <code>sinkId</code> に関するプロパティは, Web Audio API 1.1 で仕様策定されたものなので, 対応するブラウザが多い場合は, ガードしておくのが無難です.
          </p>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-html line-numbers">&lt;select id=&quot;select-output-device-by-sink-id&quot;&gt;&lt;/select&gt;
&lt;button type=&quot;button&quot; id=&quot;button-output-device-by-sink-id&quot;&gt;start&lt;/button&gt;</code></pre>
          <pre
            data-prismjs-copy="クリップボードにコピー"
            data-prismjs-copy-success="コピーしました"
          ><code class="language-js line-numbers">const context = new AudioContext();

const constraints = {
  audio: true
};

const buttonPermissionElement = document.getElementById(&apos;button-output-device-by-sink-id-permission&apos;);
const buttonElement           = document.getElementById(&apos;button-output-device-by-sink-id&apos;);

buttonPermissionElement.addEventListener(&apos;click&apos;, async () =&gt; {
  buttonPermissionElement.setAttribute(&apos;disabled&apos;, 'disabled&apos;);

  await navigator.mediaDevices.getUserMedia(constraints);

  const deviceInfos = await navigator.mediaDevices.enumerateDevices();

  const outputDeviceInfos = deviceInfos.filter((deviceInfo) =&gt; {
    return deviceInfo.kind === &apos;audiooutput&apos;;
  });

  const fragmentOutputDevices = document.createDocumentFragment();

  outputDeviceInfos.forEach((deviceInfo) =&gt; {
    const optionElement = document.createElement(&apos;option&apos;);

    optionElement.setAttribute(&apos;value&apos;, deviceInfo.deviceId);

    const textNode = document.createTextNode(deviceInfo.label);

    optionElement.appendChild(textNode);

    fragmentOutputDevices.appendChild(optionElement);
  });

  const selectOutputDevicesElement = document.getElementById(&apos;select-output-device-by-sink-id&apos;);

  selectOutputDevicesElement.appendChild(fragmentOutputDevices);

  selectOutputDevicesElement.addEventListener(&apos;change&apos;, async (event) =&gt; {
    const deviceId = event.target.value;

    if (typeof context.setSinkId === &apos;function&apos;) {
      await context.setSinkId(deviceId);
    }

    if (&apos;onsinkchange&apos; in context) {
      context.onsinkchange = (event) =&gt; {
        console.dir(event);
      };
    }

    if (&apos;onerror&apos; in context) {
      context.onerror = (error) =&gt; {
        console.dir(error);
      };
    }
  });
});

let oscillator = null;

const onDown = async () =&gt; {
  if (context.state !== &apos;running&apos;) {
    await context.resume();
  }

  if (oscillator !== null) {
    return;
  }

  oscillator = new OscillatorNode(context);

  // OscillatorNode (Input) -&gt; AudioDestinationNode (Output)
  oscillator.connect(context.destination);

  // Start immediately
  oscillator.start(0);

  buttonElement.textContent = &apos;stop&apos;;
};

const onUp = () =&gt; {
  if (oscillator === null) {
    return;
  }

  oscillator.stop(0);
  oscillator.disconnect(0);

  oscillator = null;

  buttonElement.textContent = &apos;start&apos;;
};

buttonElement.addEventListener(&apos;mousedown&apos;,  onDown);
buttonElement.addEventListener(&apos;touchstart&apos;, onDown);
buttonElement.addEventListener(&apos;mouseup&apos;,    onUp);
buttonElement.addEventListener(&apos;touchend&apos;,   onUp);</code></pre>
          <div class="app-container app-output-device-by-sink-id">
            <dl>
              <dt><label for="select-output-device-by-sink-id">Output Devices</label></dt>
              <dd>
                <select id="select-output-device-by-sink-id">
                  <option value="">-- Select Output Device --</option>
                </select>
              </dd>
            </dl>
            <div class="flexbox">
              <button type="button" id="button-output-device-by-sink-id-permission">permission</button>
              <button type="button" id="button-output-device-by-sink-id">start</button>
            </div>
          </div>
        </section>
        <p>
          <code>MediaStreamAudioSourceNode</code> と <code>sinkId</code>, そして, <code>MediaStream</code> インスタンスとして, WebRTC など, 別のメディア系 API
          を出力先とするための <code>MediaStreamAudioDestinationNode</code> によって, オーディオインターフェース, つまり,
          外部デバイスからの入出力が可能となります. 特に, <abbr title="Digital Audio Workstation">DAW</abbr> の場合,
          デバイスからの入出力は必須機能となることが多いので, 重要な API と言えます.
        </p>
        <p>
          DAW やソフトウェアシンセサイザーでは, もう 1 つ多用されるデバイスがあります. それは,
          <abbr title="Musical Instrument Digital Interface">MIDI</abbr> デバイスです. これに関しては, Web Audio API がカバーする API ではなく, Web MIDI API
          によって仕様策定されている API なので, あとのセクションで解説します.
        </p>
      </section>
      <footer>
        <span>Copyright (c) 2013 - 2014, 2023 - <span id="copyright-end-year"></span></span>
        <span><a href="https://github.com/Korilakkuma/Web-Music-Documentation" target="_blank" rel="noopener noreferrer">Korilakkuma</a> (Tomohiro IKEDA)</span>
      </footer>
    </main>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/prism.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/line-numbers/prism-line-numbers.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/toolbar/prism-toolbar.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs@latest/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
    <script>
      const date = new Date();

      document.getElementById("copyright-end-year").textContent = date.getFullYear().toString(10);
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [
            ["$", "$"],
            ["\\(", "\\)"]
          ]
        }
      };
    </script>
    <script defer src="docs.js"></script>
  </body>
</html>
